<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="true" />














  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Microsoft YaHei:300,300italic,400,400italic,700,700italic|Microsoft YaHei:300,300italic,400,400italic,700,700italic|Microsoft YaHei:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="机器学习," />










<meta name="description" content="这篇文章我们将讲解一下机器学习与数据挖掘领域最常用的一些算法，包括有监督学习的分类与回归算法以及无监督学习的聚类与降维算法。好了，闲话不多说，让我们一起开始吧！">
<meta name="keywords" content="机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习算法概览">
<meta property="og:url" content="https://zhouzhuming.tech/2018/11/27/机器学习算法概览/index.html">
<meta property="og:site_name" content="Zhou Zhuming&#39;s blogs">
<meta property="og:description" content="这篇文章我们将讲解一下机器学习与数据挖掘领域最常用的一些算法，包括有监督学习的分类与回归算法以及无监督学习的聚类与降维算法。好了，闲话不多说，让我们一起开始吧！">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/1.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/2.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/3.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/4.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/5.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/6.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/7.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/8.jpg">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/9.jpg">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/10.jpg">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/11.jpg">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/12.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/13.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/14.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/15.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/16.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/17.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/18.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/19.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/22.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/23.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/20.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/21.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/24.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/25.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/26.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/27.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/28.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/29.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/30.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/31.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/32.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/33.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/34.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/31.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/35.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/36.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/37.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/38.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/39.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/40.png">
<meta property="og:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/41.png">
<meta property="og:updated_time" content="2018-11-28T09:53:07.429Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习算法概览">
<meta name="twitter:description" content="这篇文章我们将讲解一下机器学习与数据挖掘领域最常用的一些算法，包括有监督学习的分类与回归算法以及无监督学习的聚类与降维算法。好了，闲话不多说，让我们一起开始吧！">
<meta name="twitter:image" content="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://zhouzhuming.tech/2018/11/27/机器学习算法概览/"/>





  <title>机器学习算法概览 | Zhou Zhuming's blogs</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?4aab8459c9ccb9136d4791cd2e7c6294";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
    <a href="https://github.com/zmzhouXJTU"><img style="position: absolute; top: 0; left: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_left_darkblue_121621.png" alt="Fork me on GitHub"></a>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Zhou Zhuming's blogs</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">你必须非常努力，才能看起来毫不费力。</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404.html" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            公益404
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zhouzhuming.tech/2018/11/27/机器学习算法概览/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhou Zhuming">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/head-image.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhou Zhuming's blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">机器学习算法概览</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-27T13:04:27+08:00">
                2018-11-27
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/11/27/机器学习算法概览/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2018/11/27/机器学习算法概览/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2018/11/27/机器学习算法概览/" class="leancloud_visitors" data-flag-title="机器学习算法概览">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  9,798字
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  36分钟
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>这篇文章我们将讲解一下<strong>机器学习与数据挖掘领域</strong>最常用的一些算法，包括<strong>有监督学习</strong>的<strong>分类与回归</strong>算法以及<strong>无监督学习</strong>的<strong>聚类与降维</strong>算法。好了，闲话不多说，让我们一起开始吧！<br><a id="more"></a></p>
<hr>
<h3 id="K-Nearest-Neighbor-K近邻"><a href="#K-Nearest-Neighbor-K近邻" class="headerlink" title="K-Nearest Neighbor(K近邻)"></a>K-Nearest Neighbor(K近邻)</h3><h4 id="前置假设"><a href="#前置假设" class="headerlink" title="前置假设"></a>前置假设</h4><ul>
<li>输入空间: $\chi \subseteq \mathbb R^n$</li>
<li>训练集: $T = [(x_1,y_1),(x_2,y_2),…,(x_N,y_N)]$</li>
<li>输出空间: $\gamma = [c_1,c_2,…,c_k]$</li>
</ul>
<h4 id="K近邻算法基本思想"><a href="#K近邻算法基本思想" class="headerlink" title="K近邻算法基本思想"></a>K近邻算法基本思想</h4><p>K近邻算法并<strong>不具有显式的学习过程</strong>，K近邻算法实际上是<strong>利用训练集数据对特征空间进行划分</strong>，并作为其分类的模型。给定一个训练集，对于新的输入实例，首先在训练集中找到与该实例最近的K个实例，然后统计这K个实例的多数属于哪个类，就把该实例划分为哪个类。</p>
<h4 id="K近邻算法的三个要素"><a href="#K近邻算法的三个要素" class="headerlink" title="K近邻算法的三个要素"></a>K近邻算法的三个要素</h4><p>经过上面的讨论，我们可以总结出K近邻算法的三个要素：</p>
<ul>
<li>K值的选择</li>
<li>距离的度量</li>
<li>分类决策的准则</li>
</ul>
<h5 id="K值的选择"><a href="#K值的选择" class="headerlink" title="K值的选择"></a>K值的选择</h5><p>首先考虑一个极端的情况，当K值为1时，此时的K近邻算法又称为最近邻算法，这种情况下，很容易发生<strong>过拟合</strong>，很容易将一些噪声学习到模型中(很容易将实例判定为噪声类别)</p>
<p>我们再考虑另外一种极端情况，当K值为N时，此时不管你输入的实例是什么类别，最终的模型都会将该实例判定为模型中实例最多的类别。也就是在这种情况下，很容易发生<strong>欠拟合</strong>。</p>
<h5 id="距离的度量"><a href="#距离的度量" class="headerlink" title="距离的度量"></a>距离的度量</h5><ul>
<li>闵可夫斯基距离</li>
<li>曼哈顿距离</li>
<li>欧氏距离</li>
<li>切比雪夫距离</li>
</ul>
<p>设有两个向量：</p>
<p>$$x_i = (x_{i}^{(1)},x_{i}^{(2)},x_{i}^{(3)},…,x_{i}^{(n)})$$</p>
<p>$$x_j = (x_{j}^{(1)},x_{j}^{(2)},x_{j}^{(3)},…,x_{j}^{(n)})$$</p>
<p>闵可夫斯基距离的定义如下：</p>
<p>$$d_{(x_i,x_j)} = (\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p)^{1 \over p}$$</p>
<p>当 p=1 时就是曼哈顿距离，当 p=2 时就是欧氏距离，当 p=$\infty$时就是切比雪夫距离。</p>
<h5 id="分类决策的准则"><a href="#分类决策的准则" class="headerlink" title="分类决策的准则"></a>分类决策的准则</h5><p>这利用的是<strong>多数表决</strong>的决策准则，关于对多数表决规则的解释可以参考《统计学习方法》这本书的3.2.4小节(<strong>多数表决规则等价于经验风险最小化</strong>)</p>
<h4 id="特征归一化"><a href="#特征归一化" class="headerlink" title="特征归一化"></a>特征归一化</h4><p>为了让各个特征在分类的时候同等重要，我们需要将各个特征进行归一化。</p>
<h4 id="对异常值敏感"><a href="#对异常值敏感" class="headerlink" title="对异常值敏感"></a>对异常值敏感</h4><p>由于KNN是<strong>基于距离</strong>的算法，所以KNN对异常值是比较敏感的。</p>
<hr>
<h3 id="Linear-Regression-线性回归"><a href="#Linear-Regression-线性回归" class="headerlink" title="Linear Regression(线性回归)"></a>Linear Regression(线性回归)</h3><h4 id="定义符号"><a href="#定义符号" class="headerlink" title="定义符号"></a>定义符号</h4><p>假设数据集为：{$(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})$}，参数为$\theta$，其中：</p>
<ul>
<li>训练样本数目为 m</li>
<li>第 i 个样本为$(x^{(i)},y^{(i)})$</li>
<li>$x^{(i)} = [x_1^{(i)},x_2^{(i)},…,x_n^{(i)}]^T, x^{(i)} \in \mathbb R^n$</li>
<li>$y^{(i)} \in \mathbb R$</li>
<li>$\theta = [\theta_1,\theta_2,…,\theta_n]^T, \theta \in \mathbb R^n$</li>
</ul>
<h4 id="假设函数"><a href="#假设函数" class="headerlink" title="假设函数"></a>假设函数</h4><p>$$h(x) = \sum_{i=1}^n \theta_ix_i = \theta^Tx$$</p>
<h4 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h4><h5 id="目标函数的形式"><a href="#目标函数的形式" class="headerlink" title="目标函数的形式"></a>目标函数的形式</h5><p>$$J(\theta) = {1 \over 2m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2$$</p>
<h5 id="为什么要选择这样的目标函数？"><a href="#为什么要选择这样的目标函数？" class="headerlink" title="为什么要选择这样的目标函数？"></a>为什么要选择这样的目标函数？</h5><p>(1) 对于每一个样例$(x^{(i)},y^{(i)})$, 假设预测值和真实值存在以下关系：</p>
<p>$$y^{(i)} = {\theta}^Tx^{(i)} + {\epsilon}^{(i)}$$</p>
<p>其中${\epsilon}^{(i)}$表示预测值和真实值之间的差值。</p>
<p>(2) 影响误差的因素有很多，这些因素又都是随机分布的。根据中心极限定理，许多独立随机变量的和趋于正态分布。所以进一步假设：</p>
<div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/1.png"><br></div>

<p>当给定参数$\theta$和变量$x$时，有以下公式成立：</p>
<div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/2.png"><br></div>

<p>(3) 再进一步假设各个样例的误差是独立同分布的，可以得到似然函数：</p>
<div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/3.png"><br></div>

<p>因为似然函数表示的是在参数$\theta$下数据集出现的概率，所以需要做的工作就是极大化似然函数。</p>
<p>(4) 将似然函数转化为对数似然：</p>
<div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/4.png"><br></div>

<p>转换为对数似然函数后，需要做的工作也转变为极大化对数似然函数，要极大化对数似然函数，从式子中可以得出需要使得$\sum_{i=1}^m(\theta^Tx^{(i)} - y^{(i)})^2$最小。到这一步也基本可以对选择这样的目标函数做出一个比较合理的解释了。</p>
<h4 id="优化目标函数的方法"><a href="#优化目标函数的方法" class="headerlink" title="优化目标函数的方法"></a>优化目标函数的方法</h4><h5 id="批量梯度下降"><a href="#批量梯度下降" class="headerlink" title="批量梯度下降"></a>批量梯度下降</h5><div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/5.png"><br></div>

<h5 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h5><p>当每次只用一个样本训练时，$J(\theta)$退化成以下形式：</p>
<p>$$J(\theta) = (h_\theta(x^{(i)})-y^{(i)})^2$$</p>
<p>此时参数更新公式变为以下形式：</p>
<p>$$\theta_j:= \theta_j - \alpha(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$$</p>
<p>随机梯度下降(Stochastic Gradient Descent)可能永远不能收敛到最小值，参数$\theta$将会一直在使得$J(\theta)$取最小值的附近振荡。</p>
<hr>
<h3 id="K-means-K-均值聚类"><a href="#K-means-K-均值聚类" class="headerlink" title="K-means(K-均值聚类)"></a>K-means(K-均值聚类)</h3><h4 id="假设"><a href="#假设" class="headerlink" title="假设"></a>假设</h4><p>K-means算法是一种聚类算法。为了更好的解释这个算法，首先我们假设给定的数据集为$(x^{(1)},x^{(2)},…,x^{(m)}),x^{(i)} \in \mathbb R^n,$, 注意<strong>数据是没有标签的</strong>。</p>
<h4 id="K-means算法的一般流程"><a href="#K-means算法的一般流程" class="headerlink" title="K-means算法的一般流程"></a>K-means算法的一般流程</h4><ul>
<li>选择初始的K个聚类中心$\mu_1,\mu_2,…,\mu_k \in \mathbb R^n$</li>
<li>重复以下两步直到收敛(聚类中心不再变化或者变化低于阈值)：</li>
</ul>
<p>&ensp;&ensp;&ensp;&ensp;(1). 计算每个样本到各个聚类中心的距离，并将其类别标号设为距其最近的聚类中心的标号，即：</p>
<p>$$c^{(i)}:=arg\min_{j}||x^{(i)}-\mu^{(j)}||^2, j = 1,2,…,k$$</p>
<p>&ensp;&ensp;&ensp;&ensp;其中$c^{(i)}$为第i个样例的类别标号</p>
<p>&ensp;&ensp;&ensp;&ensp;(2). 更新聚类中心的值为相同类别的样本的平均值：</p>
<div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/6.png"><br></div>

<p>&ensp;&ensp;&ensp;&ensp;其中，当$c^{(i)} = j$时，$I{c^{(i)}=j} = 1$,当$c^{(i)} \ne j$时，$I{c^{(i)}=j} = 0$</p>
<h4 id="对K-means算法的进一步解释"><a href="#对K-means算法的进一步解释" class="headerlink" title="对K-means算法的进一步解释"></a>对K-means算法的进一步解释</h4><ol>
<li>K-means算法要优化的目标函数如下：</li>
</ol>
<div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/7.png"><br></div>


<p>优化目标可以看成让所有点到其对应的聚类中心点的距离和最小。K-means算法可以看成对目标函数J的<strong>坐标下降</strong>过程，对应的解释如下：</p>
<ul>
<li>执行上述2(1)这一步的时候，相当于固定$\mu$,改变$c$ (每个样本所对应的类别)，改变的规则是样本到哪个聚类中心的距离最小就将对应的样本对应的$c$改为哪类，所以$J(c,\mu)$一定会减小。</li>
<li>执行上述2(2)步的时候，相当于固定所有样本的$c$,重新计算各个类别的中心，进一步使得$J(c,\mu)$减小。</li>
</ul>
<ol start="2">
<li>目标函数$J$不是一个凸函数，因此K-means算法不能保证收敛到全局最优解，一个简单的方法就是随机初始化多次，以最优的聚类结果作为最终的结果。</li>
<li>聚类结束后，如果一个中心没有任何相关的样本，那么这个中心就应该去掉，或者重新聚类。</li>
</ol>
<h4 id="对K-means算法的改进"><a href="#对K-means算法的改进" class="headerlink" title="对K-means算法的改进"></a>对K-means算法的改进</h4><ol>
<li>K-means++算法对K个初始聚类中心的选取做了改进，各个聚类中心之间的距离越远越好。</li>
</ol>
<p>&ensp;&ensp;&ensp;&ensp;(1) 随机选取一个聚类中心<br>&ensp;&ensp;&ensp;&ensp;(2) 计算每个样本到所有已知聚类中心的最短距离$D(x)$<br>&ensp;&ensp;&ensp;&ensp;(3) 计算每个样本被选为下一个聚类中心的概率${D^2(x)}\over{\sum_{x \in X}D^2(x)}$<br>&ensp;&ensp;&ensp;&ensp;(4) 确定每个样本被选为下一个聚类中心的概率区间<br>&ensp;&ensp;&ensp;&ensp;(5) 生成一个0~1的随机数，选取随机数对应区间的样本作为下一个聚类中心<br>&ensp;&ensp;&ensp;&ensp;(6) 重复以上过程，直到选取了K个聚类中心</p>
<ol start="2">
<li>二分K-means算法，为了克服K-means聚类算法容易陷入局部最小值的问题和提高聚类的性能，提出了二分K-means聚类算法。该算法的基本思想是首先将所有的样本点划分到一个簇，然后将该簇一分为二，之后选择其中的一个簇继续进行划分，直到得到用户指定的簇数目为止。选择哪个簇进行划分取决于对其划分是否可以最大程度的降低SSE(误差平方和)</li>
</ol>
<h4 id="K值的选取"><a href="#K值的选取" class="headerlink" title="K值的选取"></a>K值的选取</h4><p>对于如何选取K值，本文只简要提及以下两种方法：</p>
<ul>
<li>经验法</li>
<li>手肘法：横坐标为K值，纵坐标为所有样本点到它所对应的聚类中心的误差平方和，在图中找到最佳拐点。</li>
</ul>
<hr>
<h3 id="Principal-Components-Analysis-主成分分析"><a href="#Principal-Components-Analysis-主成分分析" class="headerlink" title="Principal Components Analysis(主成分分析)"></a>Principal Components Analysis(主成分分析)</h3><h4 id="定义符号-1"><a href="#定义符号-1" class="headerlink" title="定义符号"></a>定义符号</h4><p>主成分分析(Principal Components Analysis,PCA)是一种降维方法。为了更好地解释该算法，首先假设数据集为$(x^{(i)};i = 1,2,…,m)$, 其中$x^{(i)} \in \mathbb R^N$, 也就是说数据集一共包含m条数据，每条数据的特征向量维度为n.</p>
<h4 id="中心化和标准化"><a href="#中心化和标准化" class="headerlink" title="中心化和标准化"></a>中心化和标准化</h4><p>中心化又叫零均值化，中心化(零均值化)后的数据均值为零。下面两幅图是数据做中心化前后的对比，可以看到其实就是一个平移的过程，平移后所有数据的中心是(0,0)。</p>
<p><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/8.jpg" alt=""></p>
<p>数据标准化的目的就是使各个特征都在同一尺度下被衡量。</p>
<h4 id="Z-score标准化"><a href="#Z-score标准化" class="headerlink" title="Z-score标准化"></a>Z-score标准化</h4><p>Z-score标准化(也叫0-1标准化)，这种方法给予原始数据的均值(mean)和标准差(standard deviation)进行数据的标准化。进过处理的数据符合正态分布，即均值为0, 标准差为1。Z-score标准化的公式如下：</p>
<p>$$x^* = {x-\mu \over \sigma}$$</p>
<p>我们可以发现Z-score标准化的过程中是包含中心化的。以下图片展示了一组数据进行Z-score标准化的过程。左图表示的是原始数据，中间的是中心化后的数据，右图是将中心化后的数据除以标准差，得到的标准化后的数据，可以看出每个维度上的尺度是一致的(红色线段的长度表示尺度)。</p>
<p><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/9.jpg" alt=""></p>
<p>想要使用PCA算法，需要先对数据做以下处理：</p>
<ol>
<li>$\mu = {1 \over m}\sum_{i=1}^mx^{(i)}$</li>
<li>$x^{(i)} = x^{(i)} - \mu$</li>
<li>$\sigma_j^2 = {1 \over m}\sum_i(x_j^{(i)})^2$</li>
<li>$x_j^{(i)} = {x_j^{(i)}\over \sigma_j}$</li>
</ol>
<p>整个过程其实就是Z-score标准化的过程</p>
<h4 id="PCA算法的基本思想"><a href="#PCA算法的基本思想" class="headerlink" title="PCA算法的基本思想"></a>PCA算法的基本思想</h4><p>PCA算法的基本思想就是寻找到数据的主轴方向，我们希望数据在主轴方向上能够更好地被区分开，直观的说就是我们希望数据在主轴方向上尽量分散，更具体的就是指所有的点在主轴方向的投影点的方差最大。比如在以下两个图中，在方向一上，数据更分散，投影点的方差最大，所以如果从这两个方向上选一个主轴的话，应该选择方向一。</p>
<p><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/10.jpg" alt=""></p>
<p><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/11.jpg" alt=""></p>
<p>在数据已经做了Z-score标准化的前提下，数据的均值为0，其投影点的均值也为0.</p>
<div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/12.png"><br></div>

<p>(1) 上述第一个式子里面的$x^{(i)^T}\mu$就是$x^{(i)}$这个向量在投影方向$\mu$上的长度。这里的$\mu$是单位向量，即$||\mu|| = 1$。<br>(2) 上述第二个式子是把向量内积的平方换了一个写法。<br>(3) 上述第三个式子又对式子做了一个变形，不难看出${1 \over m}\sum_{i=1}^mx^{(i)}x^{(i)^T}$是一个矩阵，并且这个矩阵是对称矩阵(实际上是一个协方差矩阵)。<br>(4) 纵观整个式子，最终的目标则是找到使得整个式子取到最大值的向量$\mu$，所以这是一个最优化问题。</p>
<h4 id="求解-mu-与降维"><a href="#求解-mu-与降维" class="headerlink" title="求解$\mu$与降维"></a>求解$\mu$与降维</h4><p>$\mu$是一个单位向量，这其实是这个最优化问题的约束条件($||\mu|| = 1$), 可以使用拉格朗日方程来求解最优化问题：</p>
<div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/13.png"><br></div>

<p>对$\mu$求导：</p>
<div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/14.png"><br></div>

<p>令导数为0，可以发现$\mu$是$\Sigma$的特征向量。再因为矩阵$\Sigma$是对称矩阵，所以一定能找到n个相互正交的特征向量，如果要选出k个主轴方向，我们从n个特征向量里面选出k个最大的特征值对应的特征向量即可。假设选出的k个特征向量为：</p>
<p>$${\mu_1,\mu_2,…,\mu_k}$$</p>
<p>则可以通过以下公式将原来的n维的$x^{(i)}$降为k维的$y^{(i)}$:</p>
<p>$$[y_1^{(i)},y_2^{(i)},…,y_k^{(i)}] = [\mu_1^Tx^{(i)},\mu_2^Tx^{(i)},…,\mu_k^Tx^{(i)}]$$</p>
<hr>
<h3 id="Logistic-Regression-LR—逻辑回归"><a href="#Logistic-Regression-LR—逻辑回归" class="headerlink" title="Logistic Regression(LR—逻辑回归)"></a>Logistic Regression(LR—逻辑回归)</h3><h4 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h4><p>假设要解决的问题是一个二分类问题，目标值为{0,1},以线性回归为基础，将模型输出映射到[0,1]之间。我们选择这样一个函数：</p>
<div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/15.png"><br></div>

<p>其中$g(z)$被称为sigmoid函数。为什么要选择sigmoid函数其实是可以通过指数分布族加上广义线性模型进行推导分析的。通过sigmoid函数我们可以计算单个样本属于正类还是负类的概率：</p>
<div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/16.png"><br></div>

<p>我们将上面两个式子合并成一个：</p>
<p>$$p(y=x|;\theta) = (h_\theta(x))^y(1 - (h_\theta(x))^{(1-y)}$$</p>
<p>有了上面这个式子，我们就能很容易的得到函数$h$在在整个数据集上的<strong>似然函数</strong>：</p>
<div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/17.png"><br></div>

<p>将其转为<strong>对数似然函数</strong>：</p>
<div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/18.png"><br></div>

<p>假设我们用<strong>随机梯度下降法</strong>更新参数，每次只用一个样例，则上面的对数似然函数退化成：</p>
<p>$$L(\theta) = y^{(i)}logh_\theta(x^{(i)}) + (1 - y^{(i)})log(1 - h_\theta(x^{(i)}))$$</p>
<p>更新参数的公式为：</p>
<p>$$\theta_j:= \theta_j + \alpha * {\partial\over\partial\theta_j}L(\theta)$$</p>
<p>这里的$\alpha$就是学习率。其次注意式子里面的“+”，因为我们要极大化对数似然函数，所以我们需要沿着梯度方向更新参数。接下来我们要做的就是求出$L(\theta)$对各个参数的偏导。</p>
<ul>
<li>首先我们知道sigmoid函数的求导结果为：</li>
</ul>
<p>$$g’(z) = g(z)(1 - g(z))$$</p>
<ul>
<li>我们可以推导出$L(\theta)$对各个参数的偏导为：</li>
</ul>
<p>$${\partial\over\partial\theta_j}L(\theta) = x_j(y - h_\theta(x))$$</p>
<ul>
<li>所以，参数更新公式为：</li>
</ul>
<p>$$\theta_j:= \theta_j + \alpha(y^{(i)} - h_{\theta_j}(x^{(i)}))x_j^{(i)}$$</p>
<p>如果我们用<strong>梯度下降法</strong>,每次更新参数用所有样例，则参数更新公式为：</p>
<p>$$\theta_j:= \theta_j + \sum_{i=1}^m\alpha(y^{(i)} - h_{\theta_j}(x^{(i)}))x_j^{(i)}$$</p>
<hr>
<h3 id="Naive-Bayes-朴素贝叶斯"><a href="#Naive-Bayes-朴素贝叶斯" class="headerlink" title="Naive Bayes(朴素贝叶斯)"></a>Naive Bayes(朴素贝叶斯)</h3><h4 id="判别式学习算法和生成式学习算法"><a href="#判别式学习算法和生成式学习算法" class="headerlink" title="判别式学习算法和生成式学习算法"></a>判别式学习算法和生成式学习算法</h4><p>对于一个分类问题来说(这里以二分类问题为例)，逻辑回归算法是在解空间中寻找一条直线从而把两种类别的样例分开，对于新的样例只要判断其在直线的哪一侧即可，这种<strong>直接对问题求解的方法可以称为判别式学习方法</strong>。<strong>生成式学习算法则是对两个类别的数据分别进行建模，用新的样例去匹配两个模型，匹配度较高的作为新样例的类别。</strong></p>
<h4 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h4><p>朴素贝叶斯算法的核心自然是贝叶斯公式：</p>
<p>$$P(B|A) = {P(A|B)P(B) \over P(A)}$$</p>
<p>在机器学习分类算法中，用以下形式可能会更加清晰明了：</p>
<p>$$P(类别|特征) = {P(特征|类别)P(类别) \over P(特征)}$$</p>
<h4 id="朴素贝叶斯算法的基本思想"><a href="#朴素贝叶斯算法的基本思想" class="headerlink" title="朴素贝叶斯算法的基本思想"></a>朴素贝叶斯算法的基本思想</h4><ul>
<li>如果要解决的是一个分类问题，那么我们的任务就是根据样本的特征来判断样本属于哪个类别。首先我们要对训练集中的样本进行统计，并计算各个类别数据所占的比例(<strong>先验概率</strong>)：</li>
</ul>
<p>$$P(类别y)$$</p>
<ul>
<li>接着计算各个类别下各个特征取到某值的概率(<strong>条件概率</strong>)：</li>
</ul>
<p>$$P(第i个特征的第K个可取值|类别y)$$</p>
<ul>
<li>朴素贝叶斯算法假设各个特征相互独立，这样的话，对于测试集上的一个新样本来说，有以下等式成立：</li>
</ul>
<p>$$P(特征1,特征2,…,特征n|类别y) = P(特征1|类别y)P(特征2|类别y),…,P(特征n|类别y)$$</p>
<ul>
<li>给定测试集上的一个样本(也就是告知样本的各个特征的取值)，我们可以计算出：</li>
</ul>
<p>$$P(特征|类别y)P(类别y)$$</p>
<ul>
<li>想要计算出后验概率<strong>P(类别y|特征)</strong>，我们还需要计算出<strong>P(特征)</strong>，但是任一样本的<b>P(特征)</b>在各个类别下的值是完全相同的，又因为我们的目的是找出样本属于哪个类别的概率最大，为了简化计算，分母部分的的<b>P(特征)</b>可以去掉。</li>
</ul>
<h4 id="拉普拉斯平滑"><a href="#拉普拉斯平滑" class="headerlink" title="拉普拉斯平滑"></a>拉普拉斯平滑</h4><p>$P(特征1|类别y)P(特征2|类别y),…,P(特征n|类别y)$中有任何一部分的值为0，则整个式子的值为0。在对条件概率P(特征i的第k个可取值|类别y)进行建模时，发现它们很有可能为0，为了避免出现这种情况，可以引入拉普拉斯平滑，在建模过程中，假定每个特征的每个取值至少出现1次，这样：</p>
<div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/19.png"><br></div>


<h4 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h4><p>某个医院早上收了6个门诊病人，如下表：</p>
<table>
<thead>
<tr>
<th style="text-align:center">症状</th>
<th style="text-align:center">职业</th>
<th style="text-align:center">疾病</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">打喷嚏</td>
<td style="text-align:center">护士</td>
<td style="text-align:center">感冒</td>
</tr>
<tr>
<td style="text-align:center">打喷嚏</td>
<td style="text-align:center">农夫</td>
<td style="text-align:center">没患感冒</td>
</tr>
<tr>
<td style="text-align:center">头痛</td>
<td style="text-align:center">建筑工人</td>
<td style="text-align:center">没患感冒</td>
</tr>
<tr>
<td style="text-align:center">头痛</td>
<td style="text-align:center">建筑工人</td>
<td style="text-align:center">感冒</td>
</tr>
<tr>
<td style="text-align:center">打喷嚏</td>
<td style="text-align:center">教师</td>
<td style="text-align:center">感冒</td>
</tr>
<tr>
<td style="text-align:center">头痛</td>
<td style="text-align:center">教师</td>
<td style="text-align:center">没患感冒</td>
</tr>
</tbody>
</table>
<p>现在又来了第七个病人，是一个打喷嚏的建筑工人。请问他患上感冒的概率有多大？</p>
<ul>
<li>根据贝叶斯公式可得：</li>
</ul>
<div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/22.png"><br></div>

<ul>
<li>假定“打喷嚏”和“建筑工人”这两个特征是相互独立的，因此，上面的等式就变成了：</li>
</ul>
<div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/23.png"><br></div>

<ul>
<li>按照以上公式进行统计、计算：</li>
</ul>
<div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/20.png"><br></div>

<ul>
<li>那他没患上感冒的概率为0.33，我们可以通过以下式子进行验证：</li>
</ul>
<div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/21.png"><br></div>

<hr>
<h3 id="Desicion-Tree-决策树-—Part-1"><a href="#Desicion-Tree-决策树-—Part-1" class="headerlink" title="Desicion Tree(决策树)—Part 1"></a>Desicion Tree(决策树)—Part 1</h3><h4 id="决策树算法简介"><a href="#决策树算法简介" class="headerlink" title="决策树算法简介"></a>决策树算法简介</h4><p>决策树算法既可以应用于<strong>分类</strong>问题，也可以应用于<strong>回归</strong>问题。决策树算法有<strong>可解释性好、分类速度快</strong>的特点。决策树算法主要包含<strong>特征选择、决策树的生成、决策树的修剪</strong>这几个主要步骤。决策树相关的算法原理主要包含<strong>ID3、C4.5、CART</strong>，其中<code>scikit-learn</code>使用了优化版的<code>CART</code>算法作为其决策树算法的实现。</p>
<h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><p>给出以下数据集，考虑如何利用这个数据集建立一棵决策树，当给定一个新的西瓜，这棵决策树可以利用这个西瓜的特征（色泽、根蒂、敲声、纹理、脐部、触感）来尽量准确的判断这是不是一颗好瓜？</p>
<p><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/24.png" alt="数据集"></p>
<h4 id="决策树模型"><a href="#决策树模型" class="headerlink" title="决策树模型"></a>决策树模型</h4><p>决策树模型由<strong>结点</strong>和<strong>有向边</strong>组成，结点又包括<strong>内部结点</strong>和<strong>叶子结点</strong>。内部结点表示一个特征（也可以叫属性），叶子结点表示一个类别。</p>
<h4 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h4><p><strong>决策树学习的关键就是特征选择的问题</strong>，即如何确定每个内部结点对应哪个特征（每次划分应该按照哪个特征来进行）。</p>
<h4 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h4><p>在信息论中，<strong>熵</strong>是对随机变量不确定性的度量。随机变量的不确定性越高，它的熵就越大。设随机变量$X$是一个取有限个值的离散的随机变量，其概率分布为：</p>
<p>$$p(X = x_i) = p_i, i=1,2,…,n$$</p>
<p>随机变量的熵定义为：</p>
<p>$$H(X) = -\sum_{i=1}^np_ilogp_i$$</p>
<p>若$p_i = 0$则定义：</p>
<p>$$0log(0) = 0$$</p>
<p>当$log$函数以2为底时，这时的熵被称为<strong>比特熵</strong>，当$log$函数以$e$为底时，这时的熵被称为<strong>纳熵</strong>。我们可以用熵来度量一个数据集的混乱程度。</p>
<h4 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h4><p>假设离散特征(属性)$a$有$V$个可能的取值${a^1,a^2,…,a^V}$,若用特征$a$对样本集$D$进行划分,则会产生$V$个分支节点，其中第$V$个分支结点包含了样本集$D$中的所有在特征(属性)$a$上取值为$a^V$的样本，记为$D^v$.</p>
<p>首先可以计算出所有$D^v$的熵。然后我们假设  $|D^v|$表示的是$D^v$中的样本数量，$|D|$表示的是$D$中的样本数量，这样我们可以通过计算给每个分支结点赋予一个权重:</p>
<p>$$|D^v|\over|D|$$</p>
<p>接着我们就可以计算出样本集$D$按照特征$a$划分的信息增益：</p>
<p>$$Gain(D,a) = G(D,a) = H(D) - \sum_{v=1}^V{|D^v|\over|D|}H(D^v)$$</p>
<p>一般来说信息增益越大，说明按对应特征将数据集进行划分后，数据集的混乱程度下降的越多。</p>
<h4 id="ID3算法实例介绍"><a href="#ID3算法实例介绍" class="headerlink" title="ID3算法实例介绍"></a>ID3算法实例介绍</h4><p>ID3算法就是根据<strong>信息增益</strong>来选择特征进行划分的。按照一个特征进行划分后，产生的信息增益最大，ID3算法就选择该特征进行划分。</p>
<p>假设上文给出的数据集为$D$，观察数据集只有两类，正例个数为$8$，负例个数为$9$。我们设$p_1$为样本集中正例的概率，$p_2$为样本集中负例的概率，熵采用比特熵的计算方式，则:</p>
<div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/25.png"><br></div>

<p>接下来考虑按照<strong>色泽</strong>进行划分，色泽这个特征对应三个取值：{青绿、乌黑、浅白}，如果使用这个特征进行划分，那么会得到三个分支结点。也就是会得到三个子样本集合，分别记为: ${D^1,D^2,D^3}$, $D^1$对应色泽为青绿的子集，$D^2$对应色泽为乌黑的子集，$D^3$对应色泽为浅白的子集。</p>
<p>$D^1$包含 6 个样本，3 个正例，3 个反例。<br>$D^2$包含 6 个样本，4 个正例，2 个反例。<br>$D^3$包含 5 个样本，1 个正例，4 个反例。</p>
<p>计算$D^1,D^2,D^3$三个划分后的样本集的<strong>权重</strong>和<strong>熵</strong>，设$D^1,D^2,D^3$对应的权重分别为$p_1,p_2,p_3$.</p>
<p><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/26.png" alt=""></p>
<p>然后我们用<strong>同样的方法</strong>，依次计算按照其他特征(根蒂、敲声、纹理、脐部、触感)进行划分对应的信息增益：</p>
<div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/27.png"><br></div>

<p>通过比较我们可以确定通过<strong>纹理</strong>这个特征进行划分，得到的信息增益最大，所以应该选择纹理这个特征进行划分。划分结果如下：</p>
<div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/28.png"><br></div>

<p>接着对得到的三个子数据集$D^1,D^2,D^3$进行划分。子数据集$D^1$中有 9 个样例，有色泽、根蒂、敲声、脐部、触感五个特征可供选择($D^1,D^2,D^3$这三个子数据集中每个数据集的纹理特征经过第一次划分已经是唯一的了)。划分$D^1$数据集，计算按各个特征进行划分对应的信息增益：</p>
<div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/29.png"><br></div>

<p>经过比较我们发现按<strong>根蒂、脐部、触感</strong>三个特征中的任意一个划分，信息增益都能取到最大值0.458，我们可以从这三个特征里任选一个进行划分。这里选择<strong>根蒂</strong>这个特征进行划分。如此按照<strong>递归的方式</strong>依次对各个子数据集进行划分最后不难得到一棵如下的决策树：</p>
<div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/30.png"><br></div>


<h4 id="ID3算法整体过程"><a href="#ID3算法整体过程" class="headerlink" title="ID3算法整体过程"></a>ID3算法整体过程</h4><p>算法的输入的是包含$m$个样本的数据集$D$，每个样本有$n$个离散特征，特征集为$A = {A_1,A_2,…,A_n}$, 最终输出为决策树$T$。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(1) 初始化信息增益的阈值ϵ</span><br><span class="line">(2) 判断数据集 D 中的样本是否是同一类别的，如果是，则返回单节点树 T，并标记类别为对应类别</span><br><span class="line">(3) 判断特征集 A 是否为空，如果为空，则返回单节点树 T，并标记类别为数据集 D 中实例数最多的类别</span><br><span class="line">(4) 计算特征集 A 中的各个特征对应的信息增益，选择信息增益最大的特征 </span><br><span class="line">(5) 如果  的信息增益小于阈值 ϵ，则返回单节点树 T，并标记类别为数据集 D 中实例数最多的类别</span><br><span class="line">(6) 否则，按特征  的不同取值对数据集 D 进行划分，每个取值对应一个子节点，每个子节点对应一个子数据集，返回增加了若干子节点的树 T</span><br><span class="line">(7) 对于所有的子节点，更新数据集 D 和特征集合 ，递归调用 (2) - (6) 步，得到子树并返回</span><br></pre></td></tr></table></figure>
<h4 id="ID3算法的不足之处"><a href="#ID3算法的不足之处" class="headerlink" title="ID3算法的不足之处"></a>ID3算法的不足之处</h4><ul>
<li>ID3算法没有考虑取值为连续值的特征</li>
<li>ID3算法对可取值较多的特征有偏好</li>
<li>ID3算法没有处理缺失值</li>
<li>ID3算法没有考虑过拟合的问题</li>
</ul>
<hr>
<h3 id="Desicion-Tree-决策树-—Part-2"><a href="#Desicion-Tree-决策树-—Part-2" class="headerlink" title="Desicion Tree(决策树)—Part 2"></a>Desicion Tree(决策树)—Part 2</h3><p>上一部分提到<strong>ID3算法</strong>有四个不足之处，这 4 个不足之处在<strong>C4.5</strong>算法中得到了改进。</p>
<h4 id="处理连续特征"><a href="#处理连续特征" class="headerlink" title="处理连续特征"></a>处理连续特征</h4><p>ID3算法没有考虑连续特征，比如质量，密度等，C4.5算法的思路是将<strong>连续的特征离散化</strong>。假设在样本集$D$中连续特征$a$的取值有$n$个，记为 ${a_1,a_2,a_3,…,a_n}$。那么对于特征$a$一共有$n-1$个候选划分点，首先将这$n$个点排序，然后通过下式计算出这$n-1$个候选点：</p>
<div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/31.png"><br></div>

<p>对于这$n-1$个点，分别计算以该点作为二元分类点时的<strong>信息增益</strong>。选择信息增益最大的点作为该连续特征的二元离散分类点。比如取到的增益最大的点为$a_i$，则小于$a_i$的值为类别1，大于$a_i$的值为类别2，这样我们就做到了<strong>连续特征的离散化</strong>。<strong>要注意的是，与离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程</strong>。</p>
<h4 id="解决偏好问题"><a href="#解决偏好问题" class="headerlink" title="解决偏好问题"></a>解决偏好问题</h4><p>ID3算法对可取值较多的特征有偏好，C4.5算法使用<strong>信息增益比</strong>来选择特征，解决偏好问题。信息增益比的定义如下：</p>
<div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/32.png"><br></div>

<p>以上定义的符号，大部分在上一部分中已经出现过了，$GainRatio(D,a)$就是将数据集$D$按照特征 $a$进行划分后的<strong>信息增益比</strong>。通常来说，特征$a$的可取值数目越大，$IV(a)$的值也会越大。将 $IV(a)$放在分母的位置上，在一定程度上减轻了使用信息增益选择特征时，会更加偏好那些可取值数目多的特征带来的不利影响。但是直接使用信息增益比来选择特征则会更加偏好那些可取值数目较少的特征。因此C4.5算法并不是直接选取信息增益比最大的特征，而是采用了以下方式:</p>
<ul>
<li>首先C4.5算法会从可选特征中找出那些信息增益高于平均水平的特征</li>
<li>然后再从这些信息增益高于平均水平的特征中选出信息增益比最高的特征</li>
</ul>
<h4 id="处理缺失值"><a href="#处理缺失值" class="headerlink" title="处理缺失值"></a>处理缺失值</h4><p>现实生活中的数据集中的样本通常在某些属性上是有缺失值存在的，如果属性值缺失的样本数量比较少，我们可以直接简单粗暴的把不完备的样本删除掉，但是如果有大量的样本都有属性值的缺失，那么就不能简单地删除，因为这样删除了大量的样本，对于机器学习模型而言损失了大量有用的信息，训练出来的模型性能会受到影响。在决策树中处理含有缺失值的样本的时候，需要解决两个问题：</p>
<ul>
<li>如何在属性值缺失的情况下进行划分属性的选择？(比如“色泽”这个属性有的样本在该属性上的值是缺失的，那么该如何计算“色泽”的信息增益？)</li>
<li>给定划分属性，若样本在该属性上的值是缺失的，那么该如何对这个样本进行划分？(即到底把这个样本划分到哪个结点里？)</li>
</ul>
<p>由于计算过程比较繁琐，不适宜在此类文章中展开叙述，想要了解详细计算过程的朋友可以阅读一下文章：决策树(decision tree)(四)——缺失值处理。</p>
<h4 id="解决过拟合问题"><a href="#解决过拟合问题" class="headerlink" title="解决过拟合问题"></a>解决过拟合问题</h4><p>关于过拟合问题是如何解决的，在最后一部分介绍 <strong>CART算法</strong>会详细的介绍一下<strong>剪枝策略</strong>。</p>
<h4 id="C4-5算法总结"><a href="#C4-5算法总结" class="headerlink" title="C4.5算法总结"></a>C4.5算法总结</h4><ul>
<li>C4.5算法解决了ID3算法的偏好问题</li>
<li>C4.5算法可以处理连续特征</li>
<li>C4.5算法引入了剪枝策略</li>
<li>C4.5算法处理了缺失值</li>
<li>C4.5算法只能用来分类(ID3算法也只能用来分类)</li>
<li>C4.5算法生成的决策树是多叉树</li>
<li>C4.5算法<strong>处理连续特征时需要排序(排序可能会带来比较大的性能开销)</strong></li>
<li>C4.5算法执行过程中存在大量的熵运算(ID3算法也存在大量的熵运算)</li>
</ul>
<hr>
<h3 id="Desicion-Tree-决策树-—Part-3"><a href="#Desicion-Tree-决策树-—Part-3" class="headerlink" title="Desicion Tree(决策树)—Part 3"></a>Desicion Tree(决策树)—Part 3</h3><p>这部分主要介绍<strong>CART</strong>算法。</p>
<h4 id="CART算法简介"><a href="#CART算法简介" class="headerlink" title="CART算法简介"></a>CART算法简介</h4><p>在<strong>ID3算法</strong>中我们使用了<strong>信息增益</strong>来选择特征。在<strong>C4.5算法</strong>中，使用<strong>信息增益比</strong>来选择特征。但是ID3算法和C4.5算法都是<strong>基于信息论的熵模型</strong>的，这里面会涉及大量的对数运算，<strong>计算比较复杂</strong>。另外ID3算法和C4.5算法生成的决策树都是多叉树，模型比较复杂。<strong>CART算法在简化计算的同时也简化了决策树模型</strong>：</p>
<ul>
<li>对于<strong>回归问题</strong>，使用<strong>平方误差最小化准则</strong>来进行特征选择。对于<strong>分类问题</strong>，使用<strong>基尼指数最小化准则</strong>来进行特征选择(CART算法既可以用来解决分类问题，也可以用来解决回归问题)</li>
<li>CART算法生成的决策树是一棵<strong>二叉树</strong></li>
</ul>
<h4 id="CART算法的特征选择"><a href="#CART算法的特征选择" class="headerlink" title="CART算法的特征选择"></a>CART算法的特征选择</h4><h5 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h5><p>假设数据集$D$中共有$|\gamma|$个类别，第 k 个类别的概率为$p_k$，则数据集$D$的<strong>基尼值</strong>的定义如下：</p>
<div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/33.png"><br></div>

<p>基尼值反映的是从样本集中随机抽两个样本，这两个样本不属于同一类的概率，因此基尼值越小，数据集的纯度越高。</p>
<p>如果根据特征$a$的取值对数据集$D$进行划分，由于CART算法生成的决策树是一棵二叉树，所以就是将数据集$D$根据特征$a$的取值划分为两部分：$D_1,D_2$。则特征 a 对应的<strong>基尼指数</strong>为：</p>
<div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/34.png"><br></div>

<p>基尼指数代表了模型的不纯度，基尼指数越小，则不纯度越低，特征越好。这和信息增益(比)是相反的.</p>
<h5 id="回归问题"><a href="#回归问题" class="headerlink" title="回归问题"></a>回归问题</h5><p>对于回归问题，由于要预测的是<strong>连续值</strong>，所以需要一个指标对连续值的混乱程度进行度量，这里用<strong>均方差</strong>来度量样本集上连续值的混乱程度。假设共有$m$个特征，并假设每个特征有若干个候选划分点，遍历每个特征的所有候选划分点将数据集$D$ 划分成$D_1,D_2$两部分，求出使$D_1,D_2$均方差之和最小的特征和其对应的划分点。</p>
<h4 id="CART算法如何划分数据集"><a href="#CART算法如何划分数据集" class="headerlink" title="CART算法如何划分数据集"></a>CART算法如何划分数据集</h4><h5 id="连续特征"><a href="#连续特征" class="headerlink" title="连续特征"></a>连续特征</h5><p>对于连续特征来说，假设在样本集$D$中连续特征$a$的取值有$n$个，记为：</p>
<p>$${a_1,a_2,a_3,…,a_n}$$</p>
<p>那么对于特征$a$共有$n-1$个候选划分点，首先我们要将这$n$个取值排序。然后通过以下公式计算出这$n-1$个划分点：</p>
<div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/31.png"><br></div>

<p>然后按每个候选划分点进行划分，将样本集划分为两部分，一部分中的每个实例对应的特征的值都大于或者等于候选划分点，另一部分中的每个实例对应特征的值则都小于候选划分点。</p>
<h5 id="非连续特征"><a href="#非连续特征" class="headerlink" title="非连续特征"></a>非连续特征</h5><p>对于非连续特征来说，就是遍历特征上的每个取值，按照每个取值进行划分，同样将样本集划分为两部分，一部分样本集中的每个实例对应的特征的值等于该取值，另一部分实例对应的特征的值则不等于该取值。</p>
<h4 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h4><p>剪枝策略主要有两种，分别为<strong>预剪枝</strong>和<strong>后剪枝</strong>，<strong>剪枝是为了解决过拟合问题</strong>。</p>
<p>预剪枝是在决策树的生成过程中，对每个结点在划分前后进行评估。如果当前结点划分后并不能带来泛化能力的提升，就停止划分当前结点，并将当前结点标记为叶结点。</p>
<p>后剪枝是先训练一棵完整的决策树，然后自底向上对每个非叶结点进行考察，若将该结点对应的子树替换为叶子结点能带来决策树泛化能力的提升，则将该子树替换为叶结点。</p>
<p>那如何判断决策树的泛化能力是否提升呢？可以用“留出法”，也就是预留一部分数据用作“验证集”进行评估。</p>
<p><strong>通常来说，后剪枝比预剪枝保留了更多的分支，所以一般情形下，后剪枝更不容易欠拟合</strong>。并且通过后剪枝得到的决策树的泛化能力也往往要比预剪枝好。但是后剪枝要在决策树完全生成以后进行，与预剪枝相比时间开销更大。</p>
<hr>
<h3 id="Expectation-Maximization-Algorithm-EM算法"><a href="#Expectation-Maximization-Algorithm-EM算法" class="headerlink" title="Expectation Maximization Algorithm(EM算法)"></a>Expectation Maximization Algorithm(EM算法)</h3><h4 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h4><p>EM算法也称<strong>期望最大化</strong>(Expectation Maximum)算法，它是很多机器学习算法的基础。EM算法只需要有一些训练数据，定义一个最大化函数，经过若干次迭代，我们需要的模型就训练好了。</p>
<p>概率模型有时既含有观测变量，又含有隐变量。对于只含有观测变量的情况，给定观测数据，我们可以直接采用<strong>极大似然估计法</strong>或<strong>贝叶斯估计法</strong>估计模型参数。但是，当模型含有隐变量的时候，就无法直接使用这些估计方法估计模型参数了。<strong>EM算法就是含有隐变量的模型参数的极大似然估计法</strong></p>
<p>假设我们采集了50个男生、50个女生的身高数据。并且我们知道每条数据对应的是男生还是女生。假设男生的身高服从高斯分布$N(\mu_1,\sigma_1)$，女生的身高服从高斯分布$N(\mu_2,\sigma_2)$，这时候我们可以用极大似然估计法，通过这50个男生和50个女生的身高数据来估计这两个高斯分布的参数。大致步骤如下：</p>
<ul>
<li>写出概率密度函数</li>
<li>写出联合概率密度函数</li>
<li>得到极大似然函数</li>
<li>转化为对数似然函数</li>
<li>对各个参数求偏导并令导数为0</li>
<li>得到参数估计值</li>
</ul>
<p>假设我们采集了50个男生、50个女生的身高数据。但是我们不知道每条数据对应的是男生还是女生。假设男生的身高服从高斯分布$N(\mu_1,\sigma_1)$，女生的身高服从高斯分布$N(\mu_2,\sigma_2)$，这时候我们就不可以直接用极大似然估计法来估计这两个高斯分布的参数了。</p>
<p>因为通常来说，我们只有知道了这两个高斯分布的参数我们才能知道每条数据对应的更有可能是男生还是女生。但从另一方面去考量，我们只有知道了每条数据对应的是男生还是女生才能更准确地估计这两个高斯分布的参数。</p>
<p>这里每条数据对应的性别就是隐变量。EM算法就是解决这类问题的。EM算法解决上面这个问题的一般步骤：</p>
<ul>
<li>初始化两个高斯分布的参数值</li>
<li>E步：基于这两个高斯分布来猜测隐含数据(每条数据对应的性别是男生还是女生)</li>
<li>M步：基于观测数据和隐含数据一起来极大化对数似然函数，求解并更新两个高斯分布的参数值</li>
<li>迭代 2、3 步，直到参数基本无变化，算法收敛</li>
</ul>
<p>从上面的描述可以看出，EM算法是迭代求解最大值的算法，同时算法在每一次迭代时分为两步：E步和M步。一轮轮迭代更新隐含数据和模型分布参数，直到收敛，即得到我们需要的模型参数。如果只想了解算法的思想，看到这里就基本可以了。</p>
<h4 id="推导过程-1"><a href="#推导过程-1" class="headerlink" title="推导过程"></a>推导过程</h4><h5 id="极大化含有隐含变量的对数似然函数"><a href="#极大化含有隐含变量的对数似然函数" class="headerlink" title="极大化含有隐含变量的对数似然函数"></a>极大化含有隐含变量的对数似然函数</h5><p>对于$m$个样本的观测数据$x = (x^{(1)},x^{(2)},…,x^{(m)},)$，找出模型的参数$\theta$，极大化以下对数似然函数：</p>
<p>$$\theta = \arg \max_\theta\sum_{i=1}^mlogP(x^{(i)}|\theta)$$</p>
<p>如果我们得到的观测数据对应的隐含数据为$z = (z^{(1)},z^{(2)},…,z^{(m)},)$，此时我们要极大化的对数似然函数如下：</p>
<div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/35.png"><br></div>

<p>直接通过上式求解$\theta$，主要有以下两个难点：</p>
<ul>
<li>有隐藏变量$z$存在</li>
<li>包含和的对数，求偏导比较困难</li>
</ul>
<h5 id="引入-Q-分布"><a href="#引入-Q-分布" class="headerlink" title="引入 Q 分布"></a>引入 Q 分布</h5><p><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/36.png" alt=""></p>
<h5 id="Jensen不等式"><a href="#Jensen不等式" class="headerlink" title="Jensen不等式"></a>Jensen不等式</h5><p><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/37.png" alt=""></p>
<h4 id="EM算法的一般形式"><a href="#EM算法的一般形式" class="headerlink" title="EM算法的一般形式"></a>EM算法的一般形式</h4><ul>
<li>首先我们要初始化参数$\theta$</li>
<li>重复以下步骤直到收敛:<ul>
<li>E-Step：对于当前参数$\theta$，找到使<br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/38.png" alt=""><br>成立的Q 分布，即让$Q_i(z^{(i)}) = p(z^{(i)}|x^{(i)}|\theta)$ </li>
<li>M-step：在M-step中对对数似然函数进行极大似然估计，得到新的参数$\theta$，形式化表述为:<br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/39.png" alt=""></li>
</ul>
</li>
</ul>
<h4 id="EM算法中的坐标上升过程"><a href="#EM算法中的坐标上升过程" class="headerlink" title="EM算法中的坐标上升过程"></a>EM算法中的坐标上升过程</h4><p>如果我们将EM算法的优化目标看成：</p>
<div align="center"><br><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/40.png"><br></div>

<p>那么EM算法就可以看做是对<strong>目标函数的坐标上升过程</strong>，在E-step中， $\theta$不变，调整Q使函数变大，在M-step中，Q不变，调整$\theta$使目标函数变大。</p>
<h4 id="EM算法的直观图示"><a href="#EM算法的直观图示" class="headerlink" title="EM算法的直观图示"></a>EM算法的直观图示</h4><p><img src="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/41.png" alt=""></p>
<hr>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul>
<li>李航 —《统计学习方法》</li>
<li>周志华 —《机器学习》</li>
<li>吴恩达 — 斯坦福机器学习课程</li>
<li>张雨石 — 斯坦福ML公开课笔记</li>
<li>阮一峰 — 朴素贝叶斯分类器的应用</li>
<li>决策树(decision tree)(四)——缺失值处理</li>
<li>刘建平 — EM算法原理总结</li>
</ul>

      
    </div>
    
    
    

    

      <div>
        
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:18px;">----------------本文结束<i class="fa fa-paw"></i>感谢您的阅读----------------</div>
    
</div>

        
      </div>

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>坚持原创技术分享，您的支持将鼓励我继续创作！</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.png" alt="Zhou Zhuming 微信支付"/>
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.png" alt="Zhou Zhuming 支付宝"/>
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者：</strong>
    Zhou Zhuming
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://zhouzhuming.tech/2018/11/27/机器学习算法概览/" title="机器学习算法概览">https://zhouzhuming.tech/2018/11/27/机器学习算法概览/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>
    本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" <i class="fa fa-tag"></i> 机器学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/11/17/华为机试题之提取不重复的整数/" rel="next" title="华为机试题之提取不重复的整数">
                <i class="fa fa-chevron-left"></i> 华为机试题之提取不重复的整数
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/11/30/Python数据分析与挖掘实战读书笔记之数据挖掘基础/" rel="prev" title="Python数据分析与挖掘实战读书笔记之数据挖掘基础">
                Python数据分析与挖掘实战读书笔记之数据挖掘基础 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/head-image.jpg"
                alt="Zhou Zhuming" />
            
              <p class="site-author-name" itemprop="name">Zhou Zhuming</p>
              <p class="site-description motion-element" itemprop="description">用来记录自己学习旅程的个人小站</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">34</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">19</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/zmzhouXJTU" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.jianshu.com/users/baea3822be48/timeline" target="_blank" title="简书">
                      
                        <i class="fa fa-fw fa-book"></i>简书</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://weibo.com/zzm961020" target="_blank" title="微博">
                      
                        <i class="fa fa-fw fa-weibo"></i>微博</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.zhihu.com/people/zhou-zhu-ming-24/activities" target="_blank" title="知乎">
                      
                        <i class="fa fa-fw fa-heartbeat"></i>知乎</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                友情链接
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://cyc2018.github.io/" title="CyC2018" target="_blank">CyC2018</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://blog.csdn.net/u014380165/" title="AI之路" target="_blank">AI之路</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://blog.csdn.net/zchang81/" title="zchang81的博客" target="_blank">zchang81的博客</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://cuijiahua.com/blog/ml/" title="Jack Cui的博客" target="_blank">Jack Cui的博客</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://cherryblog.site/" title="Cheey's Blog做更好的自己" target="_blank">Cheey's Blog做更好的自己</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://zhangwenli.com/blog/" title="羡辙杂俎" target="_blank">羡辙杂俎</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#K-Nearest-Neighbor-K近邻"><span class="nav-number">1.</span> <span class="nav-text">K-Nearest Neighbor(K近邻)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#前置假设"><span class="nav-number">1.1.</span> <span class="nav-text">前置假设</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#K近邻算法基本思想"><span class="nav-number">1.2.</span> <span class="nav-text">K近邻算法基本思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#K近邻算法的三个要素"><span class="nav-number">1.3.</span> <span class="nav-text">K近邻算法的三个要素</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#K值的选择"><span class="nav-number">1.3.1.</span> <span class="nav-text">K值的选择</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#距离的度量"><span class="nav-number">1.3.2.</span> <span class="nav-text">距离的度量</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#分类决策的准则"><span class="nav-number">1.3.3.</span> <span class="nav-text">分类决策的准则</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#特征归一化"><span class="nav-number">1.4.</span> <span class="nav-text">特征归一化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#对异常值敏感"><span class="nav-number">1.5.</span> <span class="nav-text">对异常值敏感</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Linear-Regression-线性回归"><span class="nav-number">2.</span> <span class="nav-text">Linear Regression(线性回归)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#定义符号"><span class="nav-number">2.1.</span> <span class="nav-text">定义符号</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#假设函数"><span class="nav-number">2.2.</span> <span class="nav-text">假设函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#目标函数"><span class="nav-number">2.3.</span> <span class="nav-text">目标函数</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#目标函数的形式"><span class="nav-number">2.3.1.</span> <span class="nav-text">目标函数的形式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#为什么要选择这样的目标函数？"><span class="nav-number">2.3.2.</span> <span class="nav-text">为什么要选择这样的目标函数？</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#优化目标函数的方法"><span class="nav-number">2.4.</span> <span class="nav-text">优化目标函数的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#批量梯度下降"><span class="nav-number">2.4.1.</span> <span class="nav-text">批量梯度下降</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#随机梯度下降"><span class="nav-number">2.4.2.</span> <span class="nav-text">随机梯度下降</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K-means-K-均值聚类"><span class="nav-number">3.</span> <span class="nav-text">K-means(K-均值聚类)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#假设"><span class="nav-number">3.1.</span> <span class="nav-text">假设</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#K-means算法的一般流程"><span class="nav-number">3.2.</span> <span class="nav-text">K-means算法的一般流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#对K-means算法的进一步解释"><span class="nav-number">3.3.</span> <span class="nav-text">对K-means算法的进一步解释</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#对K-means算法的改进"><span class="nav-number">3.4.</span> <span class="nav-text">对K-means算法的改进</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#K值的选取"><span class="nav-number">3.5.</span> <span class="nav-text">K值的选取</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Principal-Components-Analysis-主成分分析"><span class="nav-number">4.</span> <span class="nav-text">Principal Components Analysis(主成分分析)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#定义符号-1"><span class="nav-number">4.1.</span> <span class="nav-text">定义符号</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#中心化和标准化"><span class="nav-number">4.2.</span> <span class="nav-text">中心化和标准化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Z-score标准化"><span class="nav-number">4.3.</span> <span class="nav-text">Z-score标准化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#PCA算法的基本思想"><span class="nav-number">4.4.</span> <span class="nav-text">PCA算法的基本思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#求解-mu-与降维"><span class="nav-number">4.5.</span> <span class="nav-text">求解$\mu$与降维</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Logistic-Regression-LR—逻辑回归"><span class="nav-number">5.</span> <span class="nav-text">Logistic Regression(LR—逻辑回归)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#推导过程"><span class="nav-number">5.1.</span> <span class="nav-text">推导过程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Naive-Bayes-朴素贝叶斯"><span class="nav-number">6.</span> <span class="nav-text">Naive Bayes(朴素贝叶斯)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#判别式学习算法和生成式学习算法"><span class="nav-number">6.1.</span> <span class="nav-text">判别式学习算法和生成式学习算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#贝叶斯公式"><span class="nav-number">6.2.</span> <span class="nav-text">贝叶斯公式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#朴素贝叶斯算法的基本思想"><span class="nav-number">6.3.</span> <span class="nav-text">朴素贝叶斯算法的基本思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#拉普拉斯平滑"><span class="nav-number">6.4.</span> <span class="nav-text">拉普拉斯平滑</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#实例"><span class="nav-number">6.5.</span> <span class="nav-text">实例</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Desicion-Tree-决策树-—Part-1"><span class="nav-number">7.</span> <span class="nav-text">Desicion Tree(决策树)—Part 1</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#决策树算法简介"><span class="nav-number">7.1.</span> <span class="nav-text">决策树算法简介</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#数据集"><span class="nav-number">7.2.</span> <span class="nav-text">数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#决策树模型"><span class="nav-number">7.3.</span> <span class="nav-text">决策树模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#特征选择"><span class="nav-number">7.4.</span> <span class="nav-text">特征选择</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#信息熵"><span class="nav-number">7.5.</span> <span class="nav-text">信息熵</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#信息增益"><span class="nav-number">7.6.</span> <span class="nav-text">信息增益</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ID3算法实例介绍"><span class="nav-number">7.7.</span> <span class="nav-text">ID3算法实例介绍</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ID3算法整体过程"><span class="nav-number">7.8.</span> <span class="nav-text">ID3算法整体过程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ID3算法的不足之处"><span class="nav-number">7.9.</span> <span class="nav-text">ID3算法的不足之处</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Desicion-Tree-决策树-—Part-2"><span class="nav-number">8.</span> <span class="nav-text">Desicion Tree(决策树)—Part 2</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#处理连续特征"><span class="nav-number">8.1.</span> <span class="nav-text">处理连续特征</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#解决偏好问题"><span class="nav-number">8.2.</span> <span class="nav-text">解决偏好问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#处理缺失值"><span class="nav-number">8.3.</span> <span class="nav-text">处理缺失值</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#解决过拟合问题"><span class="nav-number">8.4.</span> <span class="nav-text">解决过拟合问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#C4-5算法总结"><span class="nav-number">8.5.</span> <span class="nav-text">C4.5算法总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Desicion-Tree-决策树-—Part-3"><span class="nav-number">9.</span> <span class="nav-text">Desicion Tree(决策树)—Part 3</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#CART算法简介"><span class="nav-number">9.1.</span> <span class="nav-text">CART算法简介</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CART算法的特征选择"><span class="nav-number">9.2.</span> <span class="nav-text">CART算法的特征选择</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#分类问题"><span class="nav-number">9.2.1.</span> <span class="nav-text">分类问题</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#回归问题"><span class="nav-number">9.2.2.</span> <span class="nav-text">回归问题</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CART算法如何划分数据集"><span class="nav-number">9.3.</span> <span class="nav-text">CART算法如何划分数据集</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#连续特征"><span class="nav-number">9.3.1.</span> <span class="nav-text">连续特征</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#非连续特征"><span class="nav-number">9.3.2.</span> <span class="nav-text">非连续特征</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#剪枝"><span class="nav-number">9.4.</span> <span class="nav-text">剪枝</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Expectation-Maximization-Algorithm-EM算法"><span class="nav-number">10.</span> <span class="nav-text">Expectation Maximization Algorithm(EM算法)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#算法原理"><span class="nav-number">10.1.</span> <span class="nav-text">算法原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#推导过程-1"><span class="nav-number">10.2.</span> <span class="nav-text">推导过程</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#极大化含有隐含变量的对数似然函数"><span class="nav-number">10.2.1.</span> <span class="nav-text">极大化含有隐含变量的对数似然函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#引入-Q-分布"><span class="nav-number">10.2.2.</span> <span class="nav-text">引入 Q 分布</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Jensen不等式"><span class="nav-number">10.2.3.</span> <span class="nav-text">Jensen不等式</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#EM算法的一般形式"><span class="nav-number">10.3.</span> <span class="nav-text">EM算法的一般形式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#EM算法中的坐标上升过程"><span class="nav-number">10.4.</span> <span class="nav-text">EM算法中的坐标上升过程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#EM算法的直观图示"><span class="nav-number">10.5.</span> <span class="nav-text">EM算法的直观图示</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#参考"><span class="nav-number">11.</span> <span class="nav-text">参考</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<div class="copyright">&copy; 2017 &mdash; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhou Zhuming</span>

  
</div>


  <div class="powered-by">
  <i class="fa fa-user"></i><span id="busuanzi_container_site_uv">
  本站访客数:<span id="busuanzi_value_site_uv"></span>
  </span>
  </div>

<!-- <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>
-->



  <span class="post-meta-divider">|</span>



  <div class="powered-by">
  <i class="fa fa-eye"></i><span id="busuanzi_container_site_pv">
  本站总访问量:<span id="busuanzi_value_site_pv"></span>
  </span>
  </div>

  <!-- <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a></div>
-->




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  






  
  







  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/canvas_lines.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'l8meKq8eW9oObYAjvhwFV1OX-gzGzoHsz',
        appKey: 'VyMAFxmDWYooxCn6iP1pRJ0M',
        placeholder: 'ヾﾉ≧∀≦)o 据说看到这个之后发表评论的程序猿(媛)最后都找到了对象！',
        avatar:'wavatar',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("l8meKq8eW9oObYAjvhwFV1OX-gzGzoHsz", "VyMAFxmDWYooxCn6iP1pRJ0M");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/love.js"></script>
