<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[机器学习算法概览]]></title>
    <url>%2F2018%2F11%2F27%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88%2F</url>
    <content type="text"><![CDATA[这篇文章我们将讲解一下机器学习与数据挖掘领域最常用的一些算法，包括有监督学习的分类与回归算法以及无监督学习的聚类与降维算法。好了，闲话不多说，让我们一起开始吧！ K-Nearest Neighbor(K近邻)前置假设 输入空间: $\chi \subseteq \mathbb R^n$ 训练集: $T = [(x_1,y_1),(x_2,y_2),…,(x_N,y_N)]$ 输出空间: $\gamma = [c_1,c_2,…,c_k]$ K近邻算法基本思想K近邻算法并不具有显式的学习过程，K近邻算法实际上是利用训练集数据对特征空间进行划分，并作为其分类的模型。给定一个训练集，对于新的输入实例，首先在训练集中找到与该实例最近的K个实例，然后统计这K个实例的多数属于哪个类，就把该实例划分为哪个类。 K近邻算法的三个要素经过上面的讨论，我们可以总结出K近邻算法的三个要素： K值的选择 距离的度量 分类决策的准则 K值的选择首先考虑一个极端的情况，当K值为1时，此时的K近邻算法又称为最近邻算法，这种情况下，很容易发生过拟合，很容易将一些噪声学习到模型中(很容易将实例判定为噪声类别) 我们再考虑另外一种极端情况，当K值为N时，此时不管你输入的实例是什么类别，最终的模型都会将该实例判定为模型中实例最多的类别。也就是在这种情况下，很容易发生欠拟合。 距离的度量 闵可夫斯基距离 曼哈顿距离 欧氏距离 切比雪夫距离 设有两个向量： $$x_i = (x_{i}^{(1)},x_{i}^{(2)},x_{i}^{(3)},…,x_{i}^{(n)})$$ $$x_j = (x_{j}^{(1)},x_{j}^{(2)},x_{j}^{(3)},…,x_{j}^{(n)})$$ 闵可夫斯基距离的定义如下： $$d_{(x_i,x_j)} = (\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p)^{1 \over p}$$ 当 p=1 时就是曼哈顿距离，当 p=2 时就是欧氏距离，当 p=$\infty$时就是切比雪夫距离。 分类决策的准则这利用的是多数表决的决策准则，关于对多数表决规则的解释可以参考《统计学习方法》这本书的3.2.4小节(多数表决规则等价于经验风险最小化) 特征归一化为了让各个特征在分类的时候同等重要，我们需要将各个特征进行归一化。 对异常值敏感由于KNN是基于距离的算法，所以KNN对异常值是比较敏感的。 Linear Regression(线性回归)定义符号假设数据集为：{$(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})$}，参数为$\theta$，其中： 训练样本数目为 m 第 i 个样本为$(x^{(i)},y^{(i)})$ $x^{(i)} = [x_1^{(i)},x_2^{(i)},…,x_n^{(i)}]^T, x^{(i)} \in \mathbb R^n$ $y^{(i)} \in \mathbb R$ $\theta = [\theta_1,\theta_2,…,\theta_n]^T, \theta \in \mathbb R^n$ 假设函数$$h(x) = \sum_{i=1}^n \theta_ix_i = \theta^Tx$$ 目标函数目标函数的形式$$J(\theta) = {1 \over 2m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2$$ 为什么要选择这样的目标函数？(1) 对于每一个样例$(x^{(i)},y^{(i)})$, 假设预测值和真实值存在以下关系： $$y^{(i)} = {\theta}^Tx^{(i)} + {\epsilon}^{(i)}$$ 其中${\epsilon}^{(i)}$表示预测值和真实值之间的差值。 (2) 影响误差的因素有很多，这些因素又都是随机分布的。根据中心极限定理，许多独立随机变量的和趋于正态分布。所以进一步假设： 当给定参数$\theta$和变量$x$时，有以下公式成立： (3) 再进一步假设各个样例的误差是独立同分布的，可以得到似然函数： 因为似然函数表示的是在参数$\theta$下数据集出现的概率，所以需要做的工作就是极大化似然函数。 (4) 将似然函数转化为对数似然： 转换为对数似然函数后，需要做的工作也转变为极大化对数似然函数，要极大化对数似然函数，从式子中可以得出需要使得$\sum_{i=1}^m(\theta^Tx^{(i)} - y^{(i)})^2$最小。到这一步也基本可以对选择这样的目标函数做出一个比较合理的解释了。 优化目标函数的方法批量梯度下降 随机梯度下降当每次只用一个样本训练时，$J(\theta)$退化成以下形式： $$J(\theta) = (h_\theta(x^{(i)})-y^{(i)})^2$$ 此时参数更新公式变为以下形式： $$\theta_j:= \theta_j - \alpha(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$$ 随机梯度下降(Stochastic Gradient Descent)可能永远不能收敛到最小值，参数$\theta$将会一直在使得$J(\theta)$取最小值的附近振荡。 K-means(K-均值聚类)假设K-means算法是一种聚类算法。为了更好的解释这个算法，首先我们假设给定的数据集为$(x^{(1)},x^{(2)},…,x^{(m)}),x^{(i)} \in \mathbb R^n,$, 注意数据是没有标签的。 K-means算法的一般流程 选择初始的K个聚类中心$\mu_1,\mu_2,…,\mu_k \in \mathbb R^n$ 重复以下两步直到收敛(聚类中心不再变化或者变化低于阈值)： &ensp;&ensp;&ensp;&ensp;(1). 计算每个样本到各个聚类中心的距离，并将其类别标号设为距其最近的聚类中心的标号，即： $$c^{(i)}:=arg\min_{j}||x^{(i)}-\mu^{(j)}||^2, j = 1,2,…,k$$ &ensp;&ensp;&ensp;&ensp;其中$c^{(i)}$为第i个样例的类别标号 &ensp;&ensp;&ensp;&ensp;(2). 更新聚类中心的值为相同类别的样本的平均值： &ensp;&ensp;&ensp;&ensp;其中，当$c^{(i)} = j$时，$I{c^{(i)}=j} = 1$,当$c^{(i)} \ne j$时，$I{c^{(i)}=j} = 0$ 对K-means算法的进一步解释 K-means算法要优化的目标函数如下： 优化目标可以看成让所有点到其对应的聚类中心点的距离和最小。K-means算法可以看成对目标函数J的坐标下降过程，对应的解释如下： 执行上述2(1)这一步的时候，相当于固定$\mu$,改变$c$ (每个样本所对应的类别)，改变的规则是样本到哪个聚类中心的距离最小就将对应的样本对应的$c$改为哪类，所以$J(c,\mu)$一定会减小。 执行上述2(2)步的时候，相当于固定所有样本的$c$,重新计算各个类别的中心，进一步使得$J(c,\mu)$减小。 目标函数$J$不是一个凸函数，因此K-means算法不能保证收敛到全局最优解，一个简单的方法就是随机初始化多次，以最优的聚类结果作为最终的结果。 聚类结束后，如果一个中心没有任何相关的样本，那么这个中心就应该去掉，或者重新聚类。 对K-means算法的改进 K-means++算法对K个初始聚类中心的选取做了改进，各个聚类中心之间的距离越远越好。 &ensp;&ensp;&ensp;&ensp;(1) 随机选取一个聚类中心&ensp;&ensp;&ensp;&ensp;(2) 计算每个样本到所有已知聚类中心的最短距离$D(x)$&ensp;&ensp;&ensp;&ensp;(3) 计算每个样本被选为下一个聚类中心的概率${D^2(x)}\over{\sum_{x \in X}D^2(x)}$&ensp;&ensp;&ensp;&ensp;(4) 确定每个样本被选为下一个聚类中心的概率区间&ensp;&ensp;&ensp;&ensp;(5) 生成一个0~1的随机数，选取随机数对应区间的样本作为下一个聚类中心&ensp;&ensp;&ensp;&ensp;(6) 重复以上过程，直到选取了K个聚类中心 二分K-means算法，为了克服K-means聚类算法容易陷入局部最小值的问题和提高聚类的性能，提出了二分K-means聚类算法。该算法的基本思想是首先将所有的样本点划分到一个簇，然后将该簇一分为二，之后选择其中的一个簇继续进行划分，直到得到用户指定的簇数目为止。选择哪个簇进行划分取决于对其划分是否可以最大程度的降低SSE(误差平方和) K值的选取对于如何选取K值，本文只简要提及以下两种方法： 经验法 手肘法：横坐标为K值，纵坐标为所有样本点到它所对应的聚类中心的误差平方和，在图中找到最佳拐点。 Principal Components Analysis(主成分分析)定义符号主成分分析(Principal Components Analysis,PCA)是一种降维方法。为了更好地解释该算法，首先假设数据集为$(x^{(i)};i = 1,2,…,m)$, 其中$x^{(i)} \in \mathbb R^N$, 也就是说数据集一共包含m条数据，每条数据的特征向量维度为n. 中心化和标准化中心化又叫零均值化，中心化(零均值化)后的数据均值为零。下面两幅图是数据做中心化前后的对比，可以看到其实就是一个平移的过程，平移后所有数据的中心是(0,0)。 数据标准化的目的就是使各个特征都在同一尺度下被衡量。 Z-score标准化Z-score标准化(也叫0-1标准化)，这种方法给予原始数据的均值(mean)和标准差(standard deviation)进行数据的标准化。进过处理的数据符合正态分布，即均值为0, 标准差为1。Z-score标准化的公式如下： $$x^* = {x-\mu \over \sigma}$$ 我们可以发现Z-score标准化的过程中是包含中心化的。以下图片展示了一组数据进行Z-score标准化的过程。左图表示的是原始数据，中间的是中心化后的数据，右图是将中心化后的数据除以标准差，得到的标准化后的数据，可以看出每个维度上的尺度是一致的(红色线段的长度表示尺度)。 想要使用PCA算法，需要先对数据做以下处理： $\mu = {1 \over m}\sum_{i=1}^mx^{(i)}$ $x^{(i)} = x^{(i)} - \mu$ $\sigma_j^2 = {1 \over m}\sum_i(x_j^{(i)})^2$ $x_j^{(i)} = {x_j^{(i)}\over \sigma_j}$ 整个过程其实就是Z-score标准化的过程 PCA算法的基本思想PCA算法的基本思想就是寻找到数据的主轴方向，我们希望数据在主轴方向上能够更好地被区分开，直观的说就是我们希望数据在主轴方向上尽量分散，更具体的就是指所有的点在主轴方向的投影点的方差最大。比如在以下两个图中，在方向一上，数据更分散，投影点的方差最大，所以如果从这两个方向上选一个主轴的话，应该选择方向一。 在数据已经做了Z-score标准化的前提下，数据的均值为0，其投影点的均值也为0. (1) 上述第一个式子里面的$x^{(i)^T}\mu$就是$x^{(i)}$这个向量在投影方向$\mu$上的长度。这里的$\mu$是单位向量，即$||\mu|| = 1$。(2) 上述第二个式子是把向量内积的平方换了一个写法。(3) 上述第三个式子又对式子做了一个变形，不难看出${1 \over m}\sum_{i=1}^mx^{(i)}x^{(i)^T}$是一个矩阵，并且这个矩阵是对称矩阵(实际上是一个协方差矩阵)。(4) 纵观整个式子，最终的目标则是找到使得整个式子取到最大值的向量$\mu$，所以这是一个最优化问题。 求解$\mu$与降维$\mu$是一个单位向量，这其实是这个最优化问题的约束条件($||\mu|| = 1$), 可以使用拉格朗日方程来求解最优化问题： 对$\mu$求导： 令导数为0，可以发现$\mu$是$\Sigma$的特征向量。再因为矩阵$\Sigma$是对称矩阵，所以一定能找到n个相互正交的特征向量，如果要选出k个主轴方向，我们从n个特征向量里面选出k个最大的特征值对应的特征向量即可。假设选出的k个特征向量为： $${\mu_1,\mu_2,…,\mu_k}$$ 则可以通过以下公式将原来的n维的$x^{(i)}$降为k维的$y^{(i)}$: $$[y_1^{(i)},y_2^{(i)},…,y_k^{(i)}] = [\mu_1^Tx^{(i)},\mu_2^Tx^{(i)},…,\mu_k^Tx^{(i)}]$$ Logistic Regression(LR—逻辑回归)推导过程假设要解决的问题是一个二分类问题，目标值为{0,1},以线性回归为基础，将模型输出映射到[0,1]之间。我们选择这样一个函数： 其中$g(z)$被称为sigmoid函数。为什么要选择sigmoid函数其实是可以通过指数分布族加上广义线性模型进行推导分析的。通过sigmoid函数我们可以计算单个样本属于正类还是负类的概率： 我们将上面两个式子合并成一个： $$p(y=x|;\theta) = (h_\theta(x))^y(1 - (h_\theta(x))^{(1-y)}$$ 有了上面这个式子，我们就能很容易的得到函数$h$在在整个数据集上的似然函数： 将其转为对数似然函数： 假设我们用随机梯度下降法更新参数，每次只用一个样例，则上面的对数似然函数退化成： $$L(\theta) = y^{(i)}logh_\theta(x^{(i)}) + (1 - y^{(i)})log(1 - h_\theta(x^{(i)}))$$ 更新参数的公式为： $$\theta_j:= \theta_j + \alpha * {\partial\over\partial\theta_j}L(\theta)$$ 这里的$\alpha$就是学习率。其次注意式子里面的“+”，因为我们要极大化对数似然函数，所以我们需要沿着梯度方向更新参数。接下来我们要做的就是求出$L(\theta)$对各个参数的偏导。 首先我们知道sigmoid函数的求导结果为： $$g’(z) = g(z)(1 - g(z))$$ 我们可以推导出$L(\theta)$对各个参数的偏导为： $${\partial\over\partial\theta_j}L(\theta) = x_j(y - h_\theta(x))$$ 所以，参数更新公式为： $$\theta_j:= \theta_j + \alpha(y^{(i)} - h_{\theta_j}(x^{(i)}))x_j^{(i)}$$ 如果我们用梯度下降法,每次更新参数用所有样例，则参数更新公式为： $$\theta_j:= \theta_j + \sum_{i=1}^m\alpha(y^{(i)} - h_{\theta_j}(x^{(i)}))x_j^{(i)}$$ Naive Bayes(朴素贝叶斯)判别式学习算法和生成式学习算法对于一个分类问题来说(这里以二分类问题为例)，逻辑回归算法是在解空间中寻找一条直线从而把两种类别的样例分开，对于新的样例只要判断其在直线的哪一侧即可，这种直接对问题求解的方法可以称为判别式学习方法。生成式学习算法则是对两个类别的数据分别进行建模，用新的样例去匹配两个模型，匹配度较高的作为新样例的类别。 贝叶斯公式朴素贝叶斯算法的核心自然是贝叶斯公式： $$P(B|A) = {P(A|B)P(B) \over P(A)}$$ 在机器学习分类算法中，用以下形式可能会更加清晰明了： $$P(类别|特征) = {P(特征|类别)P(类别) \over P(特征)}$$ 朴素贝叶斯算法的基本思想 如果要解决的是一个分类问题，那么我们的任务就是根据样本的特征来判断样本属于哪个类别。首先我们要对训练集中的样本进行统计，并计算各个类别数据所占的比例(先验概率)： $$P(类别y)$$ 接着计算各个类别下各个特征取到某值的概率(条件概率)： $$P(第i个特征的第K个可取值|类别y)$$ 朴素贝叶斯算法假设各个特征相互独立，这样的话，对于测试集上的一个新样本来说，有以下等式成立： $$P(特征1,特征2,…,特征n|类别y) = P(特征1|类别y)P(特征2|类别y),…,P(特征n|类别y)$$ 给定测试集上的一个样本(也就是告知样本的各个特征的取值)，我们可以计算出： $$P(特征|类别y)P(类别y)$$ 想要计算出后验概率P(类别y|特征)，我们还需要计算出P(特征)，但是任一样本的P(特征)在各个类别下的值是完全相同的，又因为我们的目的是找出样本属于哪个类别的概率最大，为了简化计算，分母部分的的P(特征)可以去掉。 拉普拉斯平滑$P(特征1|类别y)P(特征2|类别y),…,P(特征n|类别y)$中有任何一部分的值为0，则整个式子的值为0。在对条件概率P(特征i的第k个可取值|类别y)进行建模时，发现它们很有可能为0，为了避免出现这种情况，可以引入拉普拉斯平滑，在建模过程中，假定每个特征的每个取值至少出现1次，这样： 实例某个医院早上收了6个门诊病人，如下表： 症状 职业 疾病 打喷嚏 护士 感冒 打喷嚏 农夫 没患感冒 头痛 建筑工人 没患感冒 头痛 建筑工人 感冒 打喷嚏 教师 感冒 头痛 教师 没患感冒 现在又来了第七个病人，是一个打喷嚏的建筑工人。请问他患上感冒的概率有多大？ 根据贝叶斯公式可得： 假定“打喷嚏”和“建筑工人”这两个特征是相互独立的，因此，上面的等式就变成了： 按照以上公式进行统计、计算： 那他没患上感冒的概率为0.33，我们可以通过以下式子进行验证： Desicion Tree(决策树)—Part 1决策树算法简介决策树算法既可以应用于分类问题，也可以应用于回归问题。决策树算法有可解释性好、分类速度快的特点。决策树算法主要包含特征选择、决策树的生成、决策树的修剪这几个主要步骤。决策树相关的算法原理主要包含ID3、C4.5、CART，其中scikit-learn使用了优化版的CART算法作为其决策树算法的实现。 数据集给出以下数据集，考虑如何利用这个数据集建立一棵决策树，当给定一个新的西瓜，这棵决策树可以利用这个西瓜的特征（色泽、根蒂、敲声、纹理、脐部、触感）来尽量准确的判断这是不是一颗好瓜？ 决策树模型决策树模型由结点和有向边组成，结点又包括内部结点和叶子结点。内部结点表示一个特征（也可以叫属性），叶子结点表示一个类别。 特征选择决策树学习的关键就是特征选择的问题，即如何确定每个内部结点对应哪个特征（每次划分应该按照哪个特征来进行）。 信息熵在信息论中，熵是对随机变量不确定性的度量。随机变量的不确定性越高，它的熵就越大。设随机变量$X$是一个取有限个值的离散的随机变量，其概率分布为： $$p(X = x_i) = p_i, i=1,2,…,n$$ 随机变量的熵定义为： $$H(X) = -\sum_{i=1}^np_ilogp_i$$ 若$p_i = 0$则定义： $$0log(0) = 0$$ 当$log$函数以2为底时，这时的熵被称为比特熵，当$log$函数以$e$为底时，这时的熵被称为纳熵。我们可以用熵来度量一个数据集的混乱程度。 信息增益假设离散特征(属性)$a$有$V$个可能的取值${a^1,a^2,…,a^V}$,若用特征$a$对样本集$D$进行划分,则会产生$V$个分支节点，其中第$V$个分支结点包含了样本集$D$中的所有在特征(属性)$a$上取值为$a^V$的样本，记为$D^v$. 首先可以计算出所有$D^v$的熵。然后我们假设 $|D^v|$表示的是$D^v$中的样本数量，$|D|$表示的是$D$中的样本数量，这样我们可以通过计算给每个分支结点赋予一个权重: $$|D^v|\over|D|$$ 接着我们就可以计算出样本集$D$按照特征$a$划分的信息增益： $$Gain(D,a) = G(D,a) = H(D) - \sum_{v=1}^V{|D^v|\over|D|}H(D^v)$$ 一般来说信息增益越大，说明按对应特征将数据集进行划分后，数据集的混乱程度下降的越多。 ID3算法实例介绍ID3算法就是根据信息增益来选择特征进行划分的。按照一个特征进行划分后，产生的信息增益最大，ID3算法就选择该特征进行划分。 假设上文给出的数据集为$D$，观察数据集只有两类，正例个数为$8$，负例个数为$9$。我们设$p_1$为样本集中正例的概率，$p_2$为样本集中负例的概率，熵采用比特熵的计算方式，则: 接下来考虑按照色泽进行划分，色泽这个特征对应三个取值：{青绿、乌黑、浅白}，如果使用这个特征进行划分，那么会得到三个分支结点。也就是会得到三个子样本集合，分别记为: ${D^1,D^2,D^3}$, $D^1$对应色泽为青绿的子集，$D^2$对应色泽为乌黑的子集，$D^3$对应色泽为浅白的子集。 $D^1$包含 6 个样本，3 个正例，3 个反例。$D^2$包含 6 个样本，4 个正例，2 个反例。$D^3$包含 5 个样本，1 个正例，4 个反例。 计算$D^1,D^2,D^3$三个划分后的样本集的权重和熵，设$D^1,D^2,D^3$对应的权重分别为$p_1,p_2,p_3$. 然后我们用同样的方法，依次计算按照其他特征(根蒂、敲声、纹理、脐部、触感)进行划分对应的信息增益： 通过比较我们可以确定通过纹理这个特征进行划分，得到的信息增益最大，所以应该选择纹理这个特征进行划分。划分结果如下： 接着对得到的三个子数据集$D^1,D^2,D^3$进行划分。子数据集$D^1$中有 9 个样例，有色泽、根蒂、敲声、脐部、触感五个特征可供选择($D^1,D^2,D^3$这三个子数据集中每个数据集的纹理特征经过第一次划分已经是唯一的了)。划分$D^1$数据集，计算按各个特征进行划分对应的信息增益： 经过比较我们发现按根蒂、脐部、触感三个特征中的任意一个划分，信息增益都能取到最大值0.458，我们可以从这三个特征里任选一个进行划分。这里选择根蒂这个特征进行划分。如此按照递归的方式依次对各个子数据集进行划分最后不难得到一棵如下的决策树： ID3算法整体过程算法的输入的是包含$m$个样本的数据集$D$，每个样本有$n$个离散特征，特征集为$A = {A_1,A_2,…,A_n}$, 最终输出为决策树$T$。 1234567(1) 初始化信息增益的阈值ϵ(2) 判断数据集 D 中的样本是否是同一类别的，如果是，则返回单节点树 T，并标记类别为对应类别(3) 判断特征集 A 是否为空，如果为空，则返回单节点树 T，并标记类别为数据集 D 中实例数最多的类别(4) 计算特征集 A 中的各个特征对应的信息增益，选择信息增益最大的特征 (5) 如果 的信息增益小于阈值 ϵ，则返回单节点树 T，并标记类别为数据集 D 中实例数最多的类别(6) 否则，按特征 的不同取值对数据集 D 进行划分，每个取值对应一个子节点，每个子节点对应一个子数据集，返回增加了若干子节点的树 T(7) 对于所有的子节点，更新数据集 D 和特征集合 ，递归调用 (2) - (6) 步，得到子树并返回 ID3算法的不足之处 ID3算法没有考虑取值为连续值的特征 ID3算法对可取值较多的特征有偏好 ID3算法没有处理缺失值 ID3算法没有考虑过拟合的问题 Desicion Tree(决策树)—Part 2上一部分提到ID3算法有四个不足之处，这 4 个不足之处在C4.5算法中得到了改进。 处理连续特征ID3算法没有考虑连续特征，比如质量，密度等，C4.5算法的思路是将连续的特征离散化。假设在样本集$D$中连续特征$a$的取值有$n$个，记为 ${a_1,a_2,a_3,…,a_n}$。那么对于特征$a$一共有$n-1$个候选划分点，首先将这$n$个点排序，然后通过下式计算出这$n-1$个候选点： 对于这$n-1$个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的二元离散分类点。比如取到的增益最大的点为$a_i$，则小于$a_i$的值为类别1，大于$a_i$的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。 解决偏好问题ID3算法对可取值较多的特征有偏好，C4.5算法使用信息增益比来选择特征，解决偏好问题。信息增益比的定义如下： 以上定义的符号，大部分在上一部分中已经出现过了，$GainRatio(D,a)$就是将数据集$D$按照特征 $a$进行划分后的信息增益比。通常来说，特征$a$的可取值数目越大，$IV(a)$的值也会越大。将 $IV(a)$放在分母的位置上，在一定程度上减轻了使用信息增益选择特征时，会更加偏好那些可取值数目多的特征带来的不利影响。但是直接使用信息增益比来选择特征则会更加偏好那些可取值数目较少的特征。因此C4.5算法并不是直接选取信息增益比最大的特征，而是采用了以下方式: 首先C4.5算法会从可选特征中找出那些信息增益高于平均水平的特征 然后再从这些信息增益高于平均水平的特征中选出信息增益比最高的特征 处理缺失值现实生活中的数据集中的样本通常在某些属性上是有缺失值存在的，如果属性值缺失的样本数量比较少，我们可以直接简单粗暴的把不完备的样本删除掉，但是如果有大量的样本都有属性值的缺失，那么就不能简单地删除，因为这样删除了大量的样本，对于机器学习模型而言损失了大量有用的信息，训练出来的模型性能会受到影响。在决策树中处理含有缺失值的样本的时候，需要解决两个问题： 如何在属性值缺失的情况下进行划分属性的选择？(比如“色泽”这个属性有的样本在该属性上的值是缺失的，那么该如何计算“色泽”的信息增益？) 给定划分属性，若样本在该属性上的值是缺失的，那么该如何对这个样本进行划分？(即到底把这个样本划分到哪个结点里？) 由于计算过程比较繁琐，不适宜在此类文章中展开叙述，想要了解详细计算过程的朋友可以阅读一下文章：决策树(decision tree)(四)——缺失值处理。 解决过拟合问题关于过拟合问题是如何解决的，在最后一部分介绍 CART算法会详细的介绍一下剪枝策略。 C4.5算法总结 C4.5算法解决了ID3算法的偏好问题 C4.5算法可以处理连续特征 C4.5算法引入了剪枝策略 C4.5算法处理了缺失值 C4.5算法只能用来分类(ID3算法也只能用来分类) C4.5算法生成的决策树是多叉树 C4.5算法处理连续特征时需要排序(排序可能会带来比较大的性能开销) C4.5算法执行过程中存在大量的熵运算(ID3算法也存在大量的熵运算) Desicion Tree(决策树)—Part 3这部分主要介绍CART算法。 CART算法简介在ID3算法中我们使用了信息增益来选择特征。在C4.5算法中，使用信息增益比来选择特征。但是ID3算法和C4.5算法都是基于信息论的熵模型的，这里面会涉及大量的对数运算，计算比较复杂。另外ID3算法和C4.5算法生成的决策树都是多叉树，模型比较复杂。CART算法在简化计算的同时也简化了决策树模型： 对于回归问题，使用平方误差最小化准则来进行特征选择。对于分类问题，使用基尼指数最小化准则来进行特征选择(CART算法既可以用来解决分类问题，也可以用来解决回归问题) CART算法生成的决策树是一棵二叉树 CART算法的特征选择 参考 李航 —《统计学习方法》 周志华 —《机器学习》 吴恩达 — 斯坦福机器学习课程 张雨石 — 斯坦福ML公开课笔记 阮一峰 — 朴素贝叶斯分类器的应用 决策树(decision tree)(四)——缺失值处理]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[华为机试题之提取不重复的整数]]></title>
    <url>%2F2018%2F11%2F17%2F%E5%8D%8E%E4%B8%BA%E6%9C%BA%E8%AF%95%E9%A2%98%E4%B9%8B%E6%8F%90%E5%8F%96%E4%B8%8D%E9%87%8D%E5%A4%8D%E7%9A%84%E6%95%B4%E6%95%B0%2F</url>
    <content type="text"><![CDATA[今天是开始刷题打怪的第五天啊，今天在牛客网上面做到了一道比较有趣的华为机试题—提取不重复的整数。我觉得这题目的解法还是比较经典的，这种解题思想值得我们掌握。好了，话不多说，让我们正式开始吧！ 提取不重复的整数题目描述输入一个int型整数，按照从右向左的阅读顺序，返回一个不含重复数字的新的整数。 输入描述 输入一个int型整数 输出描述: 按照从右向左的阅读顺序，返回一个不含重复数字的新的整数 示例1：输入 9876673 输出 37689 Python解法12345result=""for i in input()[::-1]: if i not in result: result+=iprint(result)]]></content>
      <categories>
        <category>华为机试题</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>算法</tag>
        <tag>华为机试题</tag>
        <tag>整数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[华为机试题之删除字符串中出现次数最少的字符]]></title>
    <url>%2F2018%2F11%2F16%2F%E5%8D%8E%E4%B8%BA%E6%9C%BA%E8%AF%95%E9%A2%98%E4%B9%8B%E5%88%A0%E9%99%A4%E5%AD%97%E7%AC%A6%E4%B8%B2%E4%B8%AD%E5%87%BA%E7%8E%B0%E6%AC%A1%E6%95%B0%E6%9C%80%E5%B0%91%E7%9A%84%E5%AD%97%E7%AC%A6%2F</url>
    <content type="text"><![CDATA[今天是开始刷题打怪的第四天啊，晚上闲来无事，Leetcode本菜鸡刷不动，于是乎心血来潮又刷了几道华为的机试题来增长信心哈哈哈。这里来给大家分享一道比较经典的机试题—删除字符串中出现次数最少的字符。好了，话不多说，让我们正式开始吧！ 删除字符串中出现次数最少的字符题目描述：实现删除字符串中出现次数最少的字符，若多个字符出现次数一样，则都删除。输出删除这些单词后的字符串，字符串中其它字符保持原来的顺序。 输入描述: 字符串只包含小写英文字母, 不考虑非法输入，输入的字符串长度小于等于20个字节。 输出描述: 删除字符串中出现次数最少的字符后的字符串。 示例1:输入 abcdd 输出 dd Python解法12345678910111213while True: try: a=input() b=[] c='' for i in range(len(a)): b.append(a.count(a[i])) for i in range(len(a)): if min(b)!=b[i]: c=c+a[i] print(c) except: break]]></content>
      <categories>
        <category>华为机试题</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>算法</tag>
        <tag>华为机试题</tag>
        <tag>字符串</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode刷题之二叉树的最大深度]]></title>
    <url>%2F2018%2F11%2F16%2FLeetcode%E5%88%B7%E9%A2%98%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E6%9C%80%E5%A4%A7%E6%B7%B1%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[今天是刷题的第四天呀！接上昨天的那一篇博客，昨天在逛牛客网的时候偶然发现牛客网上也有Leetcode专题，一共选了148道题，而且还有相应的题目热度排名，于是决定就在牛客网刷Leetcode系列的题了，昨天刷的是牛客网上Leetcode热度指数最高的一题—Minimum Depth of Binary Tree(二叉树的最小深度)，用的是递归的思想，刷完自信心爆棚。于是今天准备刷第二道题—逆波兰表达式求值。直接打脸，不会做。于是本狗子灵机一动，按照通过率对题目排序，于是就说今天的这道题啦—maximum-depth-of-binary-tree(二叉树的最大深度)，也是递归的思想啦！ 二叉树的最大深度(难度：Easy)题目描述给定一个二叉树，找出其最大深度。 二叉树的深度为根节点到最远叶子节点的最长路径上的节点数。 说明: 叶子节点是指没有子节点的节点。 示例： 给定二叉树 [3,9,20,null,null,15,7]， 123456 3 / \ 9 20 / \ 15 7返回它的最大深度 3 。 解题方案 思路一：- 时间复杂度: O(N) - 空间复杂度: O(1) 简单题, 但是这道题跟昨天刷的Leetcode 111不一样，这道题没有特殊情况，所以一行就够了。以下给出代码： 1234567class Solution(object): def maxDepth(self, root): """ :type root: TreeNode :rtype: int """ return 1 + max(map(self.maxDepth, (root.left, root.right))) if root else 0 后记觉得博客这个东西应该是用来总结的，不应该当做刷题的日记。所以啊，这是最后一篇刷题的博文啦！我在Github上面专门建了一个仓库来记录我的Leetcode刷题之路，下面给出传送门：https://github.com/zmzhouXJTU/Leetcode-Sword_to_Offer，欢迎各位看官来踩！]]></content>
      <categories>
        <category>Leetcode刷题</category>
      </categories>
      <tags>
        <tag>Leetcode</tag>
        <tag>Python</tag>
        <tag>算法</tag>
        <tag>树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用sklearn优雅地进行数据挖掘]]></title>
    <url>%2F2018%2F11%2F15%2F%E4%BD%BF%E7%94%A8sklearn%E4%BC%98%E9%9B%85%E5%9C%B0%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%2F</url>
    <content type="text"><![CDATA[之前的一篇博文的最后我们提到了一个问题，我们可以使用sklearn完成几乎所有特征处理的工作，而且不管是数据预处理，还是特征选择，抑或降维，它们都是通过某个类的方法fit_transform完成的，fit_transform要不只带一个参数：特征矩阵，要不带两个参数：特征矩阵加目标向量。这些难道都是巧合吗？还是故意设计成这样？方法fit_transform中有fit这一单词，它和训练模型的fit方法有关联吗？下面就让我们来一起揭晓答案！ 使用sklearn进行数据挖掘数据挖掘的步骤数据挖掘通常包括数据采集，数据分析，特征工程，训练模型，模型评估等步骤。使用sklearn工具可以方便地进行特征工程和模型训练工作，在《使用sklearn做单机特征工程》中，我们最后留下了一些疑问：特征处理类都有三个方法fit, transform和fit_transform, fit方法居然和模型训练方法fit同名(不光同名，参数列表都一样)，这难道都是巧合？ 显然，这不是巧合，这正是sklearn的设计风格。我们能够更加优雅地使用sklearn进行特征工程和模型训练工作。此时，不妨从一个基本的数据挖掘场景入手： 我们使用sklearn进行虚线框内的工作(sklearn也可以进行文本特征提取)。通过分析sklearn源码，我们可以看到除训练，预测和评估以外，处理其他工作的类都实现了3个方法：fit、transform和fit_transform。从命名中可以看到，fit_transform方法是先调用fit然后调用transform，我们只需要关注fit方法和transform方法即可。 transform方法主要用来对特征进行转换。从可利用信息的角度来说，转换分为无信息转换和有信息转换。无信息转换是指不利用任何其他信息进行转换，比如指数、对数函数转换等。有信息转换从是否利用目标值向量又可分为无监督转换和有监督转换。无监督转换指只利用特征的统计信息的转换，统计信息包括均值、标准差、边界等等，比如标准化、PCA法降维等。有监督转换指既利用了特征信息又利用了目标值信息的转换，比如通过模型选择特征、LDA法降维等。通过总结常用的转换类，我们得到下表： 不难看到，只有有信息的转换类的fit方法才实际有用，显然fit方法的主要工作是获取特征信息和目标值信息，在这点上，fit方法和模型训练时的fit方法就能够联系在一起了：都是通过分析特征和目标值，提取有价值的信息，对于转换类来说是某些统计量，对于模型来说可能是特征的权值系数等。另外，只有有监督的转换类的fit和transform方法才需要特征和目标值两个参数。fit方法无用不代表其没实现，而是除合法性校验以外，其并没有对特征和目标值进行任何处理，Normalizer的fit方法实现如下： 1234567def fit(self, X, y=None): """Do nothing and return the estimator unchanged This method is just there to implement the usual API and hence work in pipelines. """ X = check_array(X, accept_sparse='csr') return self 基于这些特征处理工作都有共同的方法，那么试想可不可以将他们组合在一起？在本文假设的场景中，我们可以看到这些工作的组合形式有两种：流水线式和并行式。基于流水线组合的工作需要依次进行，前一个工作的输出是后一个工作的输入；基于并行式的工作可以同时进行，其使用同样的输入，所有工作完成后将各自的输出合并之后输出。sklearn提供了包pipeline来完成流水线式和并行式的工作。 数据初貌在此，我们仍然使用IRIS数据集来进行说明。为了适应提出的场景，对原数据集需要稍微加工： 1234567891011from numpy import hstack, vstack, array, median, nanfrom numpy.random import choicefrom sklearn.datasets import load_iris#特征矩阵加工#使用vstack增加一行含缺失值的样本(nan, nan, nan, nan)#使用hstack增加一列表示花的颜色（0-白、1-黄、2-红），花的颜色是随机的，意味着颜色并不影响花的分类iris.data = hstack((choice([0, 1, 2], size=iris.data.shape[0]+1).reshape(-1,1), vstack((iris.data, array([nan, nan, nan, nan]).reshape(1,-1)))))#目标值向量加工#增加一个目标值，对应含缺失值的样本，值为众数iris.target = hstack((iris.target, array([median(iris.target)]))) 关键技术并行处理，流水线处理，自动化调参，持久化是使用sklearn优雅地进行数据挖掘的核心。并行处理和流水线处理将多个特征处理工作，甚至包括模型训练工作组合成一个工作(从代码的角度来说，即将多个对象组合成了一个对象)。在组合的前提下，自动化调参技术帮我们省去了人工调参的繁琐。训练好的模型是贮存在内存中的数据，持久化能够将这些数据保存在文件系统中，之后使用时无需再进行训练，直接从文件系统中加载即可。 并行处理并行处理使得多个特征处理工作能够并行地进行。根据对特征矩阵的读取方式不同，可分为整体并行处理和部分并行处理。整体并行处理，即并行处理的每个工作的输入都是特征矩阵的整体；部分并行处理，即可定义每个工作需要输入的特征矩阵的列。 整体并行处理pipeline包提供了FeatureUnion类来进行整体并行处理： 12345678910111213from numpy import log1pfrom sklearn.preprocessing import FunctionTransformerfrom sklearn.preprocessing import Binarizerfrom sklearn.pipeline import FeatureUnion#新建将整体特征矩阵进行对数函数转换的对象step2_1 = ('ToLog', FunctionTransformer(log1p))#新建将整体特征矩阵进行二值化类的对象step2_2 = ('ToBinary', Binarizer())#新建整体并行处理对象#该对象也有fit和transform方法，fit和transform方法均是并行地调用需要并行处理的对象的fit和transform方法#参数transformer_list为需要并行处理的对象列表，该列表为二元组列表，第一元为对象的名称，第二元为对象step2 = ('FeatureUnion', FeatureUnion(transformer_list=[step2_1, step2_2, step2_3])) 部分并行处理整体并行处理有其缺陷，在一些场景下，我们只需要对特征矩阵的某些列进行转换，而不是所有列。pipeline并没有提供相应的类（仅OneHotEncoder类实现了该功能），需要我们在FeatureUnion的基础上进行优化： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051from sklearn.pipeline import FeatureUnion, _fit_one_transformer, _fit_transform_one, _transform_one from sklearn.externals.joblib import Parallel, delayedfrom scipy import sparseimport numpy as np#部分并行处理，继承FeatureUnionclass FeatureUnionExt(FeatureUnion): #相比FeatureUnion，多了idx_list参数，其表示每个并行工作需要读取的特征矩阵的列 def __init__(self, transformer_list, idx_list, n_jobs=1, transformer_weights=None): self.idx_list = idx_list FeatureUnion.__init__(self, transformer_list=map(lambda trans:(trans[0], trans[1]), transformer_list), n_jobs=n_jobs, transformer_weights=transformer_weights) #由于只部分读取特征矩阵，方法fit需要重构 def fit(self, X, y=None): transformer_idx_list = map(lambda trans, idx:(trans[0], trans[1], idx), self.transformer_list, self.idx_list) transformers = Parallel(n_jobs=self.n_jobs)( #从特征矩阵中提取部分输入fit方法 delayed(_fit_one_transformer)(trans, X[:,idx], y) for name, trans, idx in transformer_idx_list) self._update_transformer_list(transformers) return self #由于只部分读取特征矩阵，方法fit_transform需要重构 def fit_transform(self, X, y=None, **fit_params): transformer_idx_list = map(lambda trans, idx:(trans[0], trans[1], idx), self.transformer_list, self.idx_list) result = Parallel(n_jobs=self.n_jobs)( #从特征矩阵中提取部分输入fit_transform方法 delayed(_fit_transform_one)(trans, name, X[:,idx], y, self.transformer_weights, **fit_params) for name, trans, idx in transformer_idx_list) Xs, transformers = zip(*result) self._update_transformer_list(transformers) if any(sparse.issparse(f) for f in Xs): Xs = sparse.hstack(Xs).tocsr() else: Xs = np.hstack(Xs) return Xs #由于只部分读取特征矩阵，方法transform需要重构 def transform(self, X): transformer_idx_list = map(lambda trans, idx:(trans[0], trans[1], idx), self.transformer_list, self.idx_list) Xs = Parallel(n_jobs=self.n_jobs)( #从特征矩阵中提取部分输入transform方法 delayed(_transform_one)(trans, name, X[:,idx], self.transformer_weights) for name, trans, idx in transformer_idx_list) if any(sparse.issparse(f) for f in Xs): Xs = sparse.hstack(Xs).tocsr() else: Xs = np.hstack(Xs) return Xs 在本文提出的场景中，我们对特征矩阵的第1列(花的颜色)进行定性特征编码，对第2、3、4列进行对数函数转换，对第5列进行定量特征二值化处理。使用FeatureUnionExt类进行部分并行处理的代码如下： 123456789101112131415from numpy import log1pfrom sklearn.preprocessing import OneHotEncoderfrom sklearn.preprocessing import FunctionTransformerfrom sklearn.preprocessing import Binarizer#新建将部分特征矩阵进行定性特征编码的对象step2_1 = ('OneHotEncoder', OneHotEncoder(sparse=False))#新建将部分特征矩阵进行对数函数转换的对象step2_2 = ('ToLog', FunctionTransformer(log1p))#新建将部分特征矩阵进行二值化类的对象step2_3 = ('ToBinary', Binarizer())#新建部分并行处理对象#参数transformer_list为需要并行处理的对象列表，该列表为二元组列表，第一元为对象的名称，第二元为对象#参数idx_list为相应的需要读取的特征矩阵的列step2 = ('FeatureUnionExt', FeatureUnionExt(transformer_list=[step2_1, step2_2, step2_3], idx_list=[[0], [1, 2, 3], [4]])) 流水线处理pipeline包提供了Pipeline类来进行流水线处理。流水线上除最后一个工作以外，其他都要执行fit_transform方法，且上一个工作输出作为下一个工作的输入。最后一个工作必须实现fit方法，输入为上一个工作的输出；但是不限定一定有transform方法，因为流水线的最后一个工作可能是训练！ 根据本文提出的场景，结合并行处理，构建完整的流水线的代码如下： 123456789101112131415161718192021222324252627282930313233from numpy import log1pfrom sklearn.preprocessing import Imputerfrom sklearn.preprocessing import OneHotEncoderfrom sklearn.preprocessing import FunctionTransformerfrom sklearn.preprocessing import Binarizerfrom sklearn.preprocessing import MinMaxScalerfrom sklearn.feature_selection import SelectKBestfrom sklearn.feature_selection import chi2from sklearn.decomposition import PCAfrom sklearn.linear_model import LogisticRegressionfrom sklearn.pipeline import Pipeline#新建计算缺失值的对象step1 = ('Imputer', Imputer())#新建将部分特征矩阵进行定性特征编码的对象step2_1 = ('OneHotEncoder', OneHotEncoder(sparse=False))#新建将部分特征矩阵进行对数函数转换的对象step2_2 = ('ToLog', FunctionTransformer(log1p))#新建将部分特征矩阵进行二值化类的对象step2_3 = ('ToBinary', Binarizer())#新建部分并行处理对象，返回值为每个并行工作的输出的合并step2 = ('FeatureUnionExt', FeatureUnionExt(transformer_list=[step2_1, step2_2, step2_3], idx_list=[[0], [1, 2, 3], [4]]))#新建无量纲化对象step3 = ('MinMaxScaler', MinMaxScaler())#新建卡方校验选择特征的对象step4 = ('SelectKBest', SelectKBest(chi2, k=3))#新建PCA降维的对象step5 = ('PCA', PCA(n_components=2))#新建逻辑回归的对象，其为待训练的模型作为流水线的最后一步step6 = ('LogisticRegression', LogisticRegression(penalty='l2'))#新建流水线处理对象#参数steps为需要流水线处理的对象列表，该列表为二元组列表，第一元为对象的名称，第二元为对象pipeline = Pipeline(steps=[step1, step2, step3, step4, step5, step6]) 自动化调参网格搜索为自动化调参的常见技术之一，grid_search包提供了自动化调参的工具，包括GridSearchCV类。对组合好的对象进行训练以及调参的代码如下： 12345678from sklearn.grid_search import GridSearchCV#新建网格搜索对象#第一参数为待训练的模型#param_grid为待调参数组成的网格，字典格式，键为参数名称（格式“对象名称__子对象名称__参数名称”），值为可取的参数值列表grid_search = GridSearchCV(pipeline, param_grid=&#123;'FeatureUnionExt__ToBinary__threshold':[1.0, 2.0, 3.0, 4.0], 'LogisticRegression__C':[0.1, 0.2, 0.4, 0.8]&#125;)#训练以及调参grid_search.fit(iris.data, iris.target) 持久化externals.joblib包提供了dump和load方法来持久化和加载内存数据： 1234567#持久化数据#第一个参数为内存中的对象#第二个参数为保存在文件系统中的名称#第三个参数为压缩级别，0为不压缩，3为合适的压缩级别dump(grid_search, 'grid_search.dmp', compress=3)#从文件系统中加载数据到内存中grid_search = load('grid_search.dmp') 回顾 包 类或方法 说明 sklearn.pipeline Pipeline 流水线处理 sklearn.pipeline FeatureUnion 并行处理 sklearn.grid_search GridSearchCV 网格搜索调参 externals.joblib dump 数据持久化 externals.joblib load 从文件系统中加载数据至内存 注意：组合和持久化都会涉及pickle技术，在sklearn的技术文档中有说明，将lambda定义的函数作为FunctionTransformer的自定义转换函数将不能pickle化。]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>数据挖掘</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode刷题之二叉树的最小深度]]></title>
    <url>%2F2018%2F11%2F15%2FLeetcode%E5%88%B7%E9%A2%98%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E6%9C%80%E5%B0%8F%E6%B7%B1%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[今天是刷题的第三天呀！今天在逛牛客网的时候偶然发现牛客网上也有Leetcode专题，而且还有相关的题目的热度，瞬间觉得牛客网真的是程序员求职的良心网站(哈哈哈，牛客网没有给我广告费，手动滑稽)。以后决定就在牛客网刷Leetcode系列的题了，牛客网上面一共148道题，刷完了我觉得应该也就差不多了。好了，话不多说，让我们开始吧！今天刷的是牛客网上Leetcode热度指数最高的一题—Minimum Depth of Binary Tree(二叉树的最小深度) 二叉树的最小深度(难度：Easy)题目描述给定一个二叉树，找出其最小深度。 最小深度是从根节点到最近叶子节点的最短路径上的节点数量。 说明: 叶子节点是指没有子节点的节点。 示例:12345678给定二叉树 [3,9,20,null,null,15,7], 3 / \ 9 20 / \ 15 7返回它的最小深度 2. 解题方案思路一：- 时间复杂度: O(N) - 空间复杂度: O(1) 思路，看完题目我想当然的认为就是直接递归取最小的值，代码如下： 123456789class Solution(object): def minDepth(self, root): """ :type root: TreeNode :rtype: int """ if not root: return 0 return 1 + min(map(self.minDepth, (root.left, root.right))) 但是没过，有一种特殊情况就是 注意leaf node: 反正就是没有left和right的 比如下图 1231 \ 2 2是一个孩子节点 这种情况应该输出2而不是1 唯一的特殊情况就是上面这种了，因为root下只有一个左节点或者是右节点，这样另外一边的空节点并不算是leaf node leaf node: itself is not null but it has both children null 所以还是要养成多写edge case的好习惯，也许就帮你避免了general写法的特例,代码如下: 12345678910111213141516class Solution(object): def minDepth(self, root): """ :type root: TreeNode :rtype: int """ if not root: return 0 elif root.left and root.right: return 1 + min(self.minDepth(root.left), self.minDepth(root.right)) elif root.left: return 1 + self.minDepth(root.left) elif root.right: return 1 + self.minDepth(root.right) else: return 1 思路二：- 时间复杂度: O(N) - 空间复杂度: O(1) 或许写的更巧妙一些 12345678910class Solution(object): def minDepth(self, root): """ :type root: TreeNode :rtype: int """ if not root: return 0 depth_under_root = map(self.minDepth, (root.left, root.right)) return 1 + (min(depth_under_root) or max(depth_under_root))]]></content>
      <categories>
        <category>Leetcode刷题</category>
      </categories>
      <tags>
        <tag>Leetcode</tag>
        <tag>Python</tag>
        <tag>算法</tag>
        <tag>树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用sklearn做单机特征工程]]></title>
    <url>%2F2018%2F11%2F15%2F%E4%BD%BF%E7%94%A8sklearn%E5%81%9A%E5%8D%95%E6%9C%BA%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[有这么一句话在业界广泛流传：数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。那特征工程到底是什么呢？顾名思义，其本质是一项工程活动，目的是最大限度地从原始数据中提取特征以供算法和模型使用。 下面就让我们一起来看看吧！ 特征工程是什么？让我们来看一张图： 通过总结和归纳，人们认为特征工程包括上图所示的方面。 特征处理是特征工程的核心部分，sklearn提供了较为完整的特征处理方法，包括数据预处理，特征选择，降维等。首次接触到sklearn，通常会被其丰富且方便的算法模型库吸引，但是这里介绍的特征处理库也十分强大！ 本文中使用sklearn中的IRIS(鸢尾花)数据集来对特征处理功能进行说明。IRIS数据集由Fisher在1936年整理，包含4个特征(Sepal.Length(花萼长度)、Sepal.Width(花萼宽度)、Petal.Length(花瓣长度)、Petal.Width(花瓣宽度))，特征值都为正浮点数，单位为厘米。目标值为鸢尾花的分类(Iris Setosa(山鸢尾）、Iris Versicolour(杂色鸢尾)，Iris Virginica(维吉尼亚鸢尾))。导入IRIS数据集的代码如下： 12345678910from sklearn.datasets import load_iris #导入IRIS数据集iris = load_iris()#特征矩阵iris.data #目标向量iris.target 数据预处理通过特征提取，我们能得到未经处理的特征，这时的特征可能有以下问题： 不属于同一量纲：即特征的规格不一样，不能够放在一起比较。无量纲化可以解决这一问题 信息冗余：对于某些定量特征，其包含的有效信息为区间划分，例如学习成绩，假若只关心“及格”或不“及格”，那么需要将定量的考分，转换成“1”和“0”表示及格和未及格。二值化可以解决这一问题。 定性特征不能直接使用：某些机器学习算法和模型只能接受定量特征的输入，那么需要将定性特征转换为定量特征。最简单的方式是为每一种定性值指定一个定量值，但是这种方式过于灵活，增加了调参的工作。通常使用哑编码的方式将定性特征转换为定量特征：假设有N种定性值，则将这一个特征扩展为N种特征，当原始特征值为第i种定性值时，第i个扩展特征赋值为1，其他扩展特征赋值为0。哑编码的方式相比直接指定的方式，不用增加调参的工作，对于线性模型来说，使用哑编码后的特征可达到非线性的效果。 存在缺失值：缺失值需要补充 信息利用率低：不同的机器学习算法和模型对数据中信息的利用是不同的，之前提到在线性模型中，使用对定性特征哑编码可以达到非线性的效果。类似地，对定量变量多项式化，或者进行其他的转换，都能达到非线性的效果 我们使用sklearn中的preproccessing库来进行数据预处理，可以覆盖以上问题的解决方案。 无量纲化无量纲化使不同规格的数据转换到同一规格。常见的无量纲化方法有标准化和区间缩放法。标准化的前提是特征值服从正态分布，标准化后，其转换成标准正态分布。区间缩放法利用了边界值信息，将特征的取值区间缩放到某个特点的范围，例如[0, 1]等。 标准化标准化需要计算特征的均值和标准差，公式表达为： $$x’ = {x- \overline X \over S}$$ 使用preproccessing库的StandardScaler类对数据进行标准化的代码如下: 1234from sklearn.preprocessing import StandardScaler#标准化，返回值为标准化后的数据StandardScaler().fit_transform(iris.data) 区间缩放法区间缩放法的思路有多种，常见的一种为利用两个最值进行缩放，公式表达为： $$x’ = {x-Min \over Max-Min} $$ 使用preproccessing库的MinMaxScaler类对数据进行区间缩放的代码如下： 1234from sklearn.preprocessing import MinMaxScaler #区间缩放，返回值为缩放到[0, 1]区间的数据MinMaxScaler().fit_transform(iris.data) 标准化与归一化的区别简单来说，标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。规则为l2的归一化公式如下: $$x’ = {x \over \sqrt{\sum_{j}^mx[j]^2}}$$ 使用preproccessing库的Normalizer类对数据进行归一化的代码如下： 1234from sklearn.preprocessing import Normalizer#归一化，返回值为归一化后的数据Normalizer().fit_transform(iris.data) 对定量特征二值化定量特征二值化的核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0，公式表达如下： 使用preproccessing库的Binarizer类对数据进行二值化的代码如下： 1234from sklearn.preprocessing import Binarizer #二值化，阈值设置为3，返回值为二值化后的数据Binarizer(threshold=3).fit_transform(iris.data) 对定性特征哑编码由于IRIS数据集的特征皆为定量特征，故使用其目标值进行哑编码(实际上是不需要的)。使用preproccessing库的OneHotEncoder类对数据进行哑编码的代码如下： 1234from sklearn.preprocessing import OneHotEncoder#哑编码，对IRIS数据集的目标值，返回值为哑编码后的数据OneHotEncoder().fit_transform(iris.target.reshape((-1,1))) 缺失值计算由于IRIS数据集没有缺失值，故对数据集新增一个样本，4个特征均赋值为NaN，表示数据缺失。使用preproccessing库的Imputer类对数据进行缺失值计算的代码如下： 1234567from numpy import vstack, array, nanfrom sklearn.preprocessing import Imputer#缺失值计算，返回值为计算缺失值后的数据#参数missing_value为缺失值的表示形式，默认为NaN#参数strategy为缺失值填充方式，默认为mean（均值）Imputer().fit_transform(vstack((array([nan, nan, nan, nan]), iris.data))) 数据变换常见的数据变换有基于多项式的、基于指数函数的、基于对数函数的。4个特征，度为2的多项式转换公式如下： 使用preproccessing库的PolynomialFeatures类对数据进行多项式转换的代码如下： 12345from sklearn.preprocessing import PolynomialFeatures#多项式转换#参数degree为度，默认值为2PolynomialFeatures().fit_transform(iris.data) 基于单变元函数的数据变换可以使用一个统一的方式完成，使用preproccessing库的FunctionTransformer对数据进行对数函数转换的代码如下： 123456from numpy import log1pfrom sklearn.preprocessing import FunctionTransformer #自定义转换函数为对数函数的数据变换#第一个参数是单变元函数FunctionTransformer(log1p).fit_transform(iris.data) 回顾 类 功能 说明 StandardScaler 无量纲化 标准化，基于特征矩阵的列，将特征值转换至服从标准正态分布 MinMaxScaler 无量纲化 区间缩放，基于最大最小值，将特征值转换到[0, 1]区间上 Normalizer 归一化 基于特征矩阵的行，将样本向量转换为“单位向量” Binarizer 二值化 基于给定阈值，将定量特征按阈值划分 OneHotEncoder 哑编码 将定性数据编码为定量数据 Imputer 缺失值计算 计算缺失值，缺失值可填充为均值等 PolynomialFeatures 多项式数据转换 多项式数据转换 FunctionTransformer 自定义单元数据转换 使用单变元的函数来转换数据 特征选择当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征： 特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。 特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。 根据特征选择的形式又可以将特征选择方法分为3种： Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。 Wrapper：包装法，根据目标函数(通常是预测效果评分)，每次选择若干特征，或者排除若干特征。 Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。 我们使用sklearn中的feature_selection库来进行特征选择。 Filter方差选择法使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。使用feature_selection库的VarianceThreshold类来选择特征的代码如下： 12345from sklearn.feature_selection import VarianceThreshold #方差选择法，返回值为特征选择后的数据#参数threshold为方差的阈值VarianceThreshold(threshold=3).fit_transform(iris.data) 相关系数法使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的P值。用feature_selection库的SelectKBest类结合相关系数来选择特征的代码如下： 1234567from sklearn.feature_selection import SelectKBestfrom scipy.stats import pearsonr#选择K个最好的特征，返回选择特征后的数据#第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。在此定义为计算相关系数#参数k为选择的特征个数SelectKBest(lambda X, Y: array(map(lambda x:pearsonr(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target) 卡方检验经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量： $$\chi^2 = \sum{(A-E)^2 \over E}$$ 这个统计量的含义简而言之就是自变量对因变量的相关性。用feature_selection库的SelectKBest类结合卡方检验来选择特征的代码如下： 12345from sklearn.feature_selection import SelectKBestfrom sklearn.feature_selection import chi2#选择K个最好的特征，返回选择特征后的数据SelectKBest(chi2, k=2).fit_transform(iris.data, iris.target) 互信息法经典的互信息也是评价定性自变量对定性因变量的相关性的，互信息计算公式如下： $$I(X;Y) = \sum_{x \in X}\sum_{y \in Y}p(x,y)log{p(x,y) \over p(x)p(y)}$$ 为了处理定量数据，最大信息系数法被提出，使用feature_selection库的SelectKBest类结合最大信息系数法来选择特征的代码如下: 1234567891011from sklearn.feature_selection import SelectKBestfrom minepy import MINE#由于MINE的设计不是函数式的，定义mic方法将其为函数式的，返回一个二元组，二元组的第2项设置成固定的P值0.5def mic(x, y): m = MINE() m.compute_score(x, y) return (m.mic(), 0.5)#选择K个最好的特征，返回特征选择后的数据SelectKBest(lambda X, Y: array(map(lambda x:mic(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target) Wrapper递归特征消除法递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。使用feature_selection库的RFE类来选择特征的代码如下： 1234567from sklearn.feature_selection import RFEfrom sklearn.linear_model import LogisticRegression#递归特征消除法，返回特征选择后的数据#参数estimator为基模型#参数n_features_to_select为选择的特征个数RFE(estimator=LogisticRegression(), n_features_to_select=2).fit_transform(iris.data, iris.target) Embedded基于惩罚项的特征选择法使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。使用feature_selection库的SelectFromModel类结合带L1惩罚项的逻辑回归模型，来选择特征的代码如下： 12345from sklearn.feature_selection import SelectFromModelfrom sklearn.linear_model import LogisticRegression#带L1惩罚项的逻辑回归作为基模型的特征选择SelectFromModel(LogisticRegression(penalty="l1", C=0.1)).fit_transform(iris.data, iris.target L1惩罚项降维的原理在于保留多个对目标值具有同等相关性的特征中的一个，所以没选到的特征不代表不重要。故，可结合L2惩罚项来优化。具体操作为：若一个特征在L1中的权值为1，选择在L2中权值差别不大且在L1中权值为0的特征构成同类集合，将这一集合中的特征平分L1中的权值，故需要构建一个新的逻辑回归模型： 12345678910111213141516171819202122232425262728293031323334353637383940414243from sklearn.linear_model import LogisticRegressionclass LR(LogisticRegression): def __init__(self, threshold=0.01, dual=False, tol=1e-4, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1): #权值相近的阈值 self.threshold = threshold LogisticRegression.__init__(self, penalty='l1', dual=dual, tol=tol, C=C, fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, class_weight=class_weight, random_state=random_state, solver=solver, max_iter=max_iter, multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs) #使用同样的参数创建L2逻辑回归 self.l2 = LogisticRegression(penalty='l2', dual=dual, tol=tol, C=C, fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, class_weight = class_weight, random_state=random_state, solver=solver, max_iter=max_iter, multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs) def fit(self, X, y, sample_weight=None): #训练L1逻辑回归 super(LR, self).fit(X, y, sample_weight=sample_weight) self.coef_old_ = self.coef_.copy() #训练L2逻辑回归 self.l2.fit(X, y, sample_weight=sample_weight) cntOfRow, cntOfCol = self.coef_.shape #权值系数矩阵的行数对应目标值的种类数目 for i in range(cntOfRow): for j in range(cntOfCol): coef = self.coef_[i][j] #L1逻辑回归的权值系数不为0 if coef != 0: idx = [j] #对应在L2逻辑回归中的权值系数 coef1 = self.l2.coef_[i][j] for k in range(cntOfCol): coef2 = self.l2.coef_[i][k] #在L2逻辑回归中，权值系数之差小于设定的阈值，且在L1中对应的权值为0 if abs(coef1-coef2) &lt; self.threshold and j != k and self.coef_[i][k] == 0: idx.append(k) #计算这一类特征的权值系数均值 mean = coef / len(idx) self.coef_[i][idx] = mean return self 使用feature_selection库的SelectFromModel类结合带L1以及L2惩罚项的逻辑回归模型，来选择特征的代码如下： 12345from sklearn.feature_selection import SelectFromModel #带L1和L2惩罚项的逻辑回归作为基模型的特征选择#参数threshold为权值系数之差的阈值SelectFromModel(LR(threshold=0.5, C=0.1)).fit_transform(iris.data, iris.target) 基于树模型的特征选择法树模型中GBDT也可用来作为基模型进行特征选择，使用feature_selection库的SelectFromModel类结合GBDT模型，来选择特征的代码如下： 12345from sklearn.feature_selection import SelectFromModelfrom sklearn.ensemble import GradientBoostingClassifier #GBDT作为基模型的特征选择SelectFromModel(GradientBoostingClassifier()).fit_transform(iris.data, iris.target) 回顾 类 所属方式 说明 VarianceThreshold Filter 方差选择法 SelectKBest Filter 可选关联系数、卡方校验、最大信息系数作为得分计算的方法 RFE Wrapper 递归地训练基模型，将权值系数较小的特征从特征集合中消除 SelectFromModel Embedded 训练基模型，选择权值系数较高的特征 降维当特征选择完成后，可以直接训练模型了，但是可能由于特征矩阵过大，导致计算量大，训练时间长的问题，因此降低特征矩阵维度也是必不可少的。常见的降维方法除了以上提到的基于L1惩罚项的模型以外，另外还有主成分分析法(PCA)和线性判别分析(LDA)，线性判别分析本身也是一个分类模型。PCA和LDA有很多的相似点，其本质是要将原始的样本映射到维度更低的样本空间中，但是PCA和LDA的映射目标不一样：PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能。所以说PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。 主成分分析法(PCA)使用decomposition库的PCA类选择特征的代码如下： 12345from sklearn.decomposition import PCA#主成分分析法，返回降维后的数据#参数n_components为主成分数目PCA(n_components=2).fit_transform(iris.data) 线性判别分析法(LDA)使用lda库的LDA类选择特征的代码如下： 12345from sklearn.lda import LDA#线性判别分析法，返回降维后的数据#参数n_components为降维后的维数LDA(n_components=2).fit_transform(iris.data, iris.target) 回顾 库 类 说明 decomposition PCA 主成分分析法 lda LDA 线性判别分析法 总结再让我们回归一下本文开始的特征工程的思维导图，我们可以使用sklearn完成几乎所有特征处理的工作，而且不管是数据预处理，还是特征选择，抑或降维，它们都是通过某个类的方法fit_transform完成的，fit_transform要不只带一个参数：特征矩阵，要不带两个参数：特征矩阵加目标向量。这些难道都是巧合吗？还是故意设计成这样？方法fit_transform中有fit这一单词，它和训练模型的fit方法有关联吗？接下来，我将在《使用sklearn优雅地进行数据挖掘》中阐述其中的奥妙！]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>数据挖掘</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[华为机试题之字符串分割]]></title>
    <url>%2F2018%2F11%2F14%2F%E5%8D%8E%E4%B8%BA%E6%9C%BA%E8%AF%95%E9%A2%98%E4%B9%8B%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%88%86%E5%89%B2%2F</url>
    <content type="text"><![CDATA[今天是开始刷题打怪的第二天啊，偶然在牛客网上看到了华为的机试题合集。众所周知，华为的机试题在业界是很容易的，通常都被大家拿来当做练手之用，于是我也做了几题。发现真的是，华为的机试题真的超级友好。要是所有互联网公司的编程题难度是这样的就好啦，哈哈。好了，这里说一个我觉得还是比较经典的题—字符串分割问题，好了，话不多说，让我们正式开始吧！ 字符串分割题目描述 连续输入字符串，请按长度为8拆分每个字符串后输出到新的字符串数组； 长度不是8整数倍的字符串请在后面补数字0，空字符串不处理。 输入描述: 首先输入数字n，表示要输入多少个字符串。连续输入字符串(输出次数为N,字符串长度小于100) 输出描述: 按长度为8拆分每个字符串后输出到新的字符串数组，长度不是8整数倍的字符串请在后面补数字0，空字符串不处理 示例1：输入 2abc123456789 输出 abc000001234567890000000 Python解法思路：借助一个while循环和Python对字符串的切片操作，可以很容易得出答案，以下给出参考代码：123456789101112while True: try: a= int(input()) for i in range(a): s=input() while len(s)&gt;8: print(s[:8]) s=s[8:] print(s+(8-len(s))*'0') except: break]]></content>
      <categories>
        <category>华为机试题</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>算法</tag>
        <tag>华为机试题</tag>
        <tag>字符串</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode刷题之两数之和]]></title>
    <url>%2F2018%2F11%2F13%2FLeetcode%E5%88%B7%E9%A2%98%E4%B9%8B%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C%2F</url>
    <content type="text"><![CDATA[很久之前就有要写记录自己刷题的日记(博客)，奈何之前一直忙着没有太多时间，现在闲下来了，就赶紧把自己刷的题记录下来，哈哈。这里记录的是基于Python语言实现的Leetcode上面的算法题。好了，话不多说，让我们开始吧！ 两数之和(难度: Easy)题目描述给定一个整数数组和一个目标值，找出数组中和为目标值的两个数。 你可以假设每个输入只对应一种答案，且同样的元素不能被重复利用。 123456示例:给定 nums = [2, 7, 11, 15], target = 9因为 nums[0] + nums[1] = 2 + 7 = 9所以返回 [0, 1] 解题方案 思路一：- 时间复杂度: O(N^2) - 空间复杂度: O(1)暴力解法，两轮遍历beats：27.6% 123456789101112class Solution(object): def twoSum(self, nums, target): """ :type nums: List[int] :type target: int :rtype: List[int] """ for i in range(len(nums)): for j in range(i+1, len(nums)): if nums[i] + nums[j] == target: return [i, j] 思路二：- 时间复杂度: O(N) - 空间复杂度: O(N)上面的思路一太慢了，我们可以牺牲空间换取时间。 1234 2 7 11 15 不存在 存在之中lookup &#123;2:0&#125; [0，1] 建立字典lookup存放第一个数字，并存放该数字的index 判断 lookup种是否存在： target - 当前数字， 则表面当前值和lookup中的值加和为target 如果存在，则返回：target - 当前数字的index和当前值的index beats 100% 1234567891011121314class Solution(object): def twoSum(self, nums, target): """ :type nums: List[int] :type target: int :rtype: List[int] """ lookup = &#123;&#125; for i, num in enumerate(nums): if target - num in lookup: return [lookup[target-num], i] else: lookup[num] = i]]></content>
      <categories>
        <category>Leetcode刷题</category>
      </categories>
      <tags>
        <tag>Leetcode</tag>
        <tag>Python</tag>
        <tag>算法</tag>
        <tag>数组</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python高级特性之装饰器]]></title>
    <url>%2F2018%2F11%2F04%2FPython%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7%E4%B9%8B%E8%A3%85%E9%A5%B0%E5%99%A8%2F</url>
    <content type="text"><![CDATA[在代码运行期间动态增加功能的方式，称之为“装饰器”(Decorator)。装饰器是程序开发中经常会用到的一个功能，用好了装饰器，开发效率如虎添翼，所以这也是Python面试中必问的问题，但是对于好多初次接触这个知识的人来讲，这个功能有点绕，自学时直接绕过去了，然后面试问到了就挂了。因为装饰器是程序开发的基础知识，这个都不会，别跟人家说你会Python, 看了下面的文章，保证你学会装饰器。 先明白这段代码1234567891011121314151617#### 第一波 ####def foo(): print('foo')foo # 表示是函数foo() # 表示执行foo函数#### 第二波 ####def foo(): print('foo')foo = lambda x: x + 1foo() # 执行lambda表达式，而不再是原来的foo函数，因为foo这个名字被重新指向了另外一个匿名函数 函数名仅仅是个变量，只不过指向了定义的函数而已，所以才能通过函数名()调用，如果函数名=xxx被修改了，那么当在执行函数名()时，调用的就不是之前的那个函数了。 需求来了初创公司有N个业务部门，基础平台部门负责提供底层的功能，如：数据库操作、redis调用、监控API等功能。业务部门使用基础功能时，只需调用基础平台提供的功能即可。如下： 12345678910111213141516171819202122232425262728############### 基础平台提供的功能如下 ###############def f1(): print('f1')def f2(): print('f2')def f3(): print('f3')def f4(): print('f4')############### 业务部门A 调用基础平台提供的功能 ###############f1()f2()f3()f4()############### 业务部门B 调用基础平台提供的功能 ###############f1()f2()f3()f4() 目前公司有条不紊的进行着，但是，以前基础平台的开发人员在写代码时候没有关注验证相关的问题，即：基础平台的提供的功能可以被任何人使用。现在需要对基础平台的所有功能进行重构，为平台提供的所有功能添加验证机制，即：执行功能前，先进行验证。 老大把工作交给Low B，他是这么做的： 跟每个业务部门交涉，每个业务部门自己写代码，调用基础平台的功能之前先验证。诶，这样一来基础平台就不需要做任何修改了。太棒了，有充足的时间泡妹子… 当天Low B 被开除了… 老大把工作交给Low BB，他是这么做的： 1234567891011121314151617181920212223242526272829303132333435363738394041############### 基础平台提供的功能如下 ############### def f1(): # 验证1 # 验证2 # 验证3 print('f1')def f2(): # 验证1 # 验证2 # 验证3 print('f2')def f3(): # 验证1 # 验证2 # 验证3 print('f3')def f4(): # 验证1 # 验证2 # 验证3 print('f4')############### 业务部门不变 ############### ### 业务部门A 调用基础平台提供的功能### f1()f2()f3()f4()### 业务部门B 调用基础平台提供的功能 ### f1()f2()f3()f4() 过了一周 Low BB 被开除了… 老大把工作交给Low BBB，他是这么做的： 只对基础平台的代码进行重构，其他业务部门无需做任何修改 123456789101112131415161718192021222324252627282930313233############### 基础平台提供的功能如下 ############### def check_login(): # 验证1 # 验证2 # 验证3 passdef f1(): check_login() print('f1')def f2(): check_login() print('f2')def f3(): check_login() print('f3')def f4(): check_login() print('f4') 老大看了下Low BBB的实现，嘴角漏出了一丝的欣慰的笑，语重心长的跟Low BBB聊了个天： 老大说： 写代码要遵循开放封闭原则，虽然在这个原则是用的面向对象开发，但是也适用于函数式编程，简单来说，它规定已经实现的功能代码不允许被修改，但可以被扩展，即： 封闭：已实现的功能代码块 开放：对扩展开发 如果将开放封闭原则应用在上述需求中，那么就不允许在函数f1, f2, f3, f4的内部进行修改代码，老板就给了Low BBB一个实现方案： 123456789101112131415161718192021def w1(func): def inner(): # 验证1 # 验证2 # 验证3 func() return inner@w1def f1(): print('f1')@w1def f2(): print('f2')@w1def f3(): print('f3')@w1def f4(): print('f4') 对于上述代码，也是仅仅对基础平台的代码进行修改，就可以实现在其他人调用函数f1, f2, f3, f4之前都进行【验证】操作，并且其他业务部门无需做任何操作。 Low BBB心惊胆战的问了下，这段代码的内部执行原理是什么呢？ 老大正要生气，突然Low BBB的手机掉到地上，恰巧屏保就是Low BBB的女友照片，老大一看一紧一抖，喜笑颜开，决定和Low BBB交个好朋友。 详细的开始讲解了： 单独以f1为例： 123456789101112def w1(func): def inner(): # 验证1 # 验证2 # 验证3 func() return inner@w1def f1(): print('f1') python解释器就会从上到下解释代码，步骤如下： def w1(func): ==&gt;将w1函数加载到内存 @w1 没错，从表面上看解释器仅仅会解释这两句代码，因为函数在没有被调用之前其内部代码不会被执行。 从表面上看解释器着实会执行这两句，但是@w1这一句代码里却有大文章，@函数名是python的一种语法糖。 上例@w1内部会执行一下操作: 执行w1函数 执行w1函数, 并将@w1下面的函数作为w1函数的参数，即：@w1等价于w1(f1)所以，内部就会去执行： 1234567&gt; def inner(): &gt; #验证 1&gt; #验证 2&gt; #验证 3&gt; f1() # func是参数，此时func等于f1 &gt; return inner # 返回的inner，inner代表的是函数，非执行函数 ,其实就是将原来的 f1 函数塞进另外一个函数中&gt; w1的返回值 将执行完的w1函数返回值 赋值给@w1下面的函数的函数名f1即将w1的返回值再重新赋值给f1，即：1234567&gt; 新f1 = def inner(): &gt; #验证 1&gt; #验证 2&gt; #验证 3&gt; 原来f1()&gt; return inner&gt; 所以，以后业务部门想要执行f1函数时，就会执行新f1函数，在新f1函数内部先执行验证，再执行原来的f1函数，然后将原来f1函数的返回值返回给了业务调用者。 如此一来， 即执行了验证的功能，又执行了原来f1函数的内容，并将原f1函数返回值 返回给业务调用者 Low BBB你明白了吗？要是没明白的话，我晚上去你家帮你解决吧！！！ 再议装饰器12345678910111213141516171819202122232425262728293031# 定义函数：完成包裹数据def makeBold(fn): def wrapped(): return "&lt;b&gt;" + fn() + "&lt;/b&gt;" return wrapped# 定义函数：完成包裹数据def makeItalic(fn): def wrapped(): return "&lt;i&gt;" + fn() + "&lt;/i&gt;" return wrapped@makeBolddef test1(): return "hello world-1"@makeItalicdef test2(): return "hello world-2"@makeBold@makeItalicdef test3(): return "hello world-3"print(test1())print(test2())print(test3()) 运行结果: 123&lt;b&gt;hello world-1&lt;/b&gt;&lt;i&gt;hello world-2&lt;/i&gt;&lt;b&gt;&lt;i&gt;hello world-3&lt;/i&gt;&lt;/b&gt; 装饰器(decorator)功能 引入日志 函数执行时间统计 执行函数前预备处理 执行函数后清理功能 权限校验等场景 缓存 装饰器示例例1:无参数的函数12345678910111213141516from time import ctime, sleepdef timefun(func): def wrapped_func(): print("%s called at %s" % (func.__name__, ctime())) func() return wrapped_func@timefundef foo(): print("I am foo")foo()sleep(2)foo() 上面代码理解装饰器执行行为可理解成: 12345678foo = timefun(foo)# foo先作为参数赋值给func后,foo接收指向timefun返回的wrapped_funcfoo()# 调用foo(),即等价调用wrapped_func()# 内部函数wrapped_func被引用，所以外部函数的func变量(自由变量)并没有释放# func里保存的是原foo函数对象 例2:被装饰的函数有参数1234567891011121314151617rom time import ctime, sleepdef timefun(func): def wrapped_func(a, b): print("%s called at %s" % (func.__name__, ctime())) print(a, b) func(a, b) return wrapped_func@timefundef foo(a, b): print(a+b)foo(3,5)sleep(2)foo(2,4) 例3:被装饰的函数有不定长参数12345678910111213141516from time import ctime, sleepdef timefun(func): def wrapped_func(*args, **kwargs): print("%s called at %s"%(func.__name__, ctime())) func(*args, **kwargs) return wrapped_func@timefundef foo(a, b, c): print(a+b+c)foo(3,5,7)sleep(2)foo(2,4,9) 例4:装饰器中的return1234567891011121314151617181920212223from time import ctime, sleepdef timefun(func): def wrapped_func(): print("%s called at %s" % (func.__name__, ctime())) func() return wrapped_func@timefundef foo(): print("I am foo")@timefundef getInfo(): return '----hahah---'foo()sleep(2)foo()print(getInfo()) 执行结果: 1234567foo called at Fri Nov 4 21:55:35 2016I am foofoo called at Fri Nov 4 21:55:37 2016I am foogetInfo called at Fri Nov 4 21:55:37 2016None 如果修改装饰器为return func()，则运行结果： 1234567foo called at Fri Nov 4 21:55:57 2016I am foofoo called at Fri Nov 4 21:55:59 2016I am foogetInfo called at Fri Nov 4 21:55:59 2016----hahah--- 总结： 一般情况下为了让装饰器更通用，可以有return 例5:装饰器带参数,在原有装饰器的基础上，设置外部变量123456789101112131415161718192021222324252627282930313233#decorator2.pyfrom time import ctime, sleepdef timefun_arg(pre="hello"): def timefun(func): def wrapped_func(): print("%s called at %s %s" % (func.__name__, ctime(), pre)) return func() return wrapped_func return timefun# 下面的装饰过程# 1. 调用timefun_arg("itcast")# 2. 将步骤1得到的返回值，即time_fun返回， 然后time_fun(foo)# 3. 将time_fun(foo)的结果返回，即wrapped_func# 4. 让foo = wrapped_fun，即foo现在指向wrapped_func@timefun_arg("itcast")def foo(): print("I am foo")@timefun_arg("python")def too(): print("I am too")foo()sleep(2)foo()too()sleep(2)too() 可以理解为: 1foo()==timefun_arg("itcast")(foo)() 例6：类装饰器（扩展，非重点）装饰器函数其实是这样一个接口约束，它必须接受一个callable对象作为参数，然后返回一个callable对象。在Python中一般callable对象都是函数，但也有例外。只要某个对象重写了__call__()方法，那么这个对象就是callable的。 1234567class Test(): def __call__(self): print('call me!')t = Test()t() # call me 类装饰器demo 123456789101112131415161718192021222324252627class Test(object): def __init__(self, func): print("---初始化---") print("func name is %s"%func.__name__) self.__func = func def __call__(self): print("---装饰器中的功能---") self.__func()#说明：#1. 当用Test来装作装饰器对test函数进行装饰的时候，首先会创建Test的实例对象# 并且会把test这个函数名当做参数传递到__init__方法中# 即在__init__方法中的属性__func指向了test指向的函数##2. test指向了用Test创建出来的实例对象##3. 当在使用test()进行调用时，就相当于让这个对象()，因此会调用这个对象的__call__方法##4. 为了能够在__call__方法中调用原来test指向的函数体，所以在__init__方法中就需要一个实例属性来保存这个函数体的引用# 所以才有了self.__func = func这句代码，从而在调用__call__方法中能够调用到test之前的函数体@Testdef test(): print("----test---")test()showpy() #如果把这句话注释，重新运行程序，依然会看到"--初始化--" 运行结果如下： 12345---初始化---func name is test---装饰器中的功能-------test---]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python高级特性之闭包]]></title>
    <url>%2F2018%2F11%2F04%2FPython%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7%E4%B9%8B%E9%97%AD%E5%8C%85%2F</url>
    <content type="text"><![CDATA[闭包(closure)是函数式编程的重要的语法结构。函数式编程是一种编程范式(而面向过程编程和面向对象编程也都是编程范式)。在面向过程编程中，我们见到过函数(function)；在面向对象编程中，我们见过对象(object)。函数和对象的根本目的是以某种逻辑方式组织代码，并提高代码的可重复使用性(reusability)。闭包也是一种组织代码的结构，它同样提高了代码的可重复使用性。 不同的语言实现闭包的方式不同。Python以函数对象为基础，为闭包这一语法结构提供支持的(我们在特殊方法与多范式中，已经多次看到Python使用对象来实现一些特殊的语法)。Python一切皆对象，函数这一语法结构也是一个对象。在函数对象中，我们像使用一个普通对象一样使用函数对象，比如更改函数对象的名字，或者将函数对象作为参数进行传递。 函数引用1234567891011121314def test1(): print("--- in test1 func----")# 调用函数test1()# 引用函数ret = test1print(id(ret))print(id(test1))#通过引用调用函数ret() 运行结果： 1234--- in test1 func----140212571149040140212571149040--- in test1 func---- 什么是闭包？12345678910111213141516171819# 定义一个函数def test(number): # 在函数内部再定义一个函数，并且这个函数用到了外边函数的变量，那么将这个函数以及用到的一些变量称之为闭包 def test_in(number_in): print("in test_in 函数, number_in is %d" % number_in) return number+number_in # 其实这里返回的就是闭包的结果 return test_in# 给test函数赋值，这个20就是给参数numberret = test(20)# 注意这里的100其实给参数number_inprint(ret(100))#注 意这里的200其实给参数number_inprint(ret(200)) 运行结果： 123456in test_in 函数, number_in is 100120in test_in 函数, number_in is 200220 看一个闭包的实际例子12345678910def line_conf(a, b): def line(x): return a*x + b return lineline1 = line_conf(1, 1)line2 = line_conf(4, 5)print(line1(5))print(line2(5)) 这个例子中，函数line与变量a,b构成闭包。在创建闭包的时候，我们通过line_conf的参数a,b说明了这两个变量的取值，这样，我们就确定了函数的最终形式(y = x + 1和y = 4x + 5)。我们只需要变换参数a,b，就可以获得不同的直线表达函数。由此，我们可以看到，闭包也具有提高代码可复用性的作用。 如果没有闭包，我们需要每次创建直线函数的时候同时说明a,b,x。这样，我们就需要更多的参数传递，也减少了代码的可移植性. 注意点: 由于闭包引用了外部函数的局部变量，则外部函数的局部变量没有及时释放，消耗内存 修改外部函数中的变量python3的方法123456789101112131415161718192021def counter(start=0): def incr(): nonlocal start start += 1 return start return incrc1 = counter(5)print(c1())print(c1())c2 = counter(50)print(c2())print(c2())print(c1())print(c1())print(c2())print(c2()) python2的方法1234567891011121314def counter(start=0): count=[start] def incr(): count[0] += 1 return count[0] return incrc1 = closeure.counter(5)print(c1()) # 6print(c1()) # 7c2 = closeure.counter(100)print(c2()) # 101print(c2()) # 102]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL高级]]></title>
    <url>%2F2018%2F11%2F04%2FMySQL%E9%AB%98%E7%BA%A7%2F</url>
    <content type="text"><![CDATA[上一篇博文我们讲到了MySQL与Python的交互，也就是如何用Python语言来操作MySQL数据库，主要也就是进行CURD(增删改查)的操作。今天这篇博文是MySQL系列的最后一篇博文了，主要讲的是关于MySQL的一些高级部分的知识，主要包括：视图、事务、索引以及账户管理。好了，话不多说，让我们一起来具体看看！ 视图问题对于复杂的查询，往往是有多个数据表进行关联查询而得到，如果数据库因为需求等原因发生了改变，为了保证查询出来的数据与之前相同，则需要在多个地方进行修改，维护起来非常麻烦。 解决办法：定义视图 视图是什么通俗的讲，视图就是一条SELECT语句执行后返回的结果集。所以我们在创建视图的时候，主要的工作就落在创建这条SQL查询语句上。 视图是对若干张基本表的引用，一张虚表，查询语句执行的结果，不存储具体的数据（基本表数据发生了改变，视图也会跟着改变）； 方便操作，特别是查询操作，减少复杂的SQL语句，增强可读性； 定义视图 建议以v_开头 1create view 视图名称 as select语句; 查看视图 查看表会将所有的视图也列出来 1show tables; 使用视图 视图的用途就是查询 1select * from v_stu_score; 删除视图123drop view 视图名称;例：drop view v_stu_sco; 视图demo 视图的作用 提高了重用性，就像一个函数 对数据库重构，却不影响程序的运行 提高了安全性能，可以对不同的用户 让数据更加清晰 事务为什么要有事务？事务广泛的运用于订单系统、银行系统等多种场景 例如： A用户和B用户是银行的储户，现在A要给B转账500元，那么需要做以下几件事：检查A的账户余额&gt;500元；A 账户中扣除500元;B 账户中增加500元; 正常的流程走下来，A账户扣了500，B账户加了500，皆大欢喜。 那如果A账户扣了钱之后，系统出故障了呢？A白白损失了500，而B也没有收到本该属于他的500。 以上的案例中，隐藏着一个前提条件：A扣钱和B加钱，要么同时成功，要么同时失败。事务的需求就在于此 所谓事务,它是一个操作序列，这些操作要么都执行，要么都不执行，它是一个不可分割的工作单位。 例如，银行转帐工作：从一个帐号扣款并使另一个帐号增款，这两个操作要么都执行，要么都不执行。所以，应该把他们看成一个事务。事务是数据库维护数据一致性的单位，在每个事务结束时，都能保持数据一致性。 事务四大特性(简称ACID) 原子性(Atomicity) 一致性(Consistency) 隔离性(Isolation) 持久性(Durability) 以下内容出自《高性能MySQL》第三版，了解事务的ACID及四种隔离级有助于我们更好的理解事务运作。 下面举一个银行应用是解释事务必要性的一个经典例子。假如一个银行的数据库有两张表：支票表（checking）和储蓄表（savings）。现在要从用户Jane的支票账户转移200美元到她的储蓄账户，那么至少需要三个步骤： 检查支票账户的余额高于或者等于200美元。 从支票账户余额中减去200美元。 在储蓄帐户余额中增加200美元。 上述三个步骤的操作必须打包在一个事务中，任何一个步骤失败，则必须回滚所有的步骤。 可以用START TRANSACTION语句开始一个事务，然后要么使用COMMIT提交将修改的数据持久保存，要么使用ROLLBACK撤销所有的修改。事务SQL的样本如下： start transaction; select balance from checking where customer_id = 10233276; update checking set balance = balance - 200.00 where customer_id = 10233276; update savings set balance = balance + 200.00 where customer_id = 10233276; commit; 一个很好的事务处理系统，必须具备这些标准特性： 原子性(Atomicity) 一个事务必须被视为一个不可分割的最小工作单元，整个事务中的所有操作要么全部提交成功，要么全部失败回滚，对于一个事务来说，不可能只执行其中的一部分操作，这就是事务的原子性 一致性(Consistency) 数据库总是从一个一致性的状态转换到另一个一致性的状态(在前面的例子中，一致性确保了，即使在执行第三、四条语句之间时系统崩溃，支票账户中也不会损失200美元，因为事务最终没有提交，所以事务中所做的修改也不会保存到数据库中) 隔离性(Isolation) 通常来说，一个事务所做的修改在最终提交以前，对其他事务是不可见的(在前面的例子中，当执行完第三条语句、第四条语句还未开始时，此时有另外的一个账户汇总程序开始运行，则其看到支票帐户的余额并没有被减去200美元) 持久性(Durability) 一旦事务提交，则其所做的修改会永久保存到数据库(此时即使系统崩溃，修改的数据也不会丢失) 事务命令表的引擎类型必须是innodb类型才可以使用事务，这是mysql表的默认引擎 查看表的创建语句，可以看到engine=innodb1234567-- 选择数据库use jing_dong;-- 查看goods表show create table goods; 开启事务，命令如下： 开启事务后执行修改命令，变更会维护到本地缓存中，而不维护到物理表中 12345begin;或者start transaction; 提交事务，命令如下: 将缓存中的数据变更维护到物理表中 1commit; 回滚事务，命令如下： 放弃缓存中变更的数据 1rollback; 注意: 修改数据的命令会自动的触发事务，包括insert、update、delete 而在SQL语句中有手动开启事务的原因是：可以进行多次数据的修改，如果成功一起成功，否则一起会滚到之前的数据 索引思考 在图书馆中是如何找到一本书的? 一般的应用系统对比数据库的读写比例在10:1左右(即有10次查询操作时有1次写的操作)， 而且插入操作和更新操作很少出现性能问题， 遇到最多、最容易出问题还是一些复杂的查询操作，所以查询语句的优化显然是重中之重 解决办法当数据库中数据量很大时，查找数据会变得很慢 优化方案：索引 索引是什么？索引是一种特殊的文件(InnoDB数据表上的索引是表空间的一个组成部分)，它们包含着对数据表里所有记录的引用指针。 更通俗的说，数据库索引好比是一本书前面的目录，能加快数据库的查询速度 索引目的索引的目的在于提高查询效率，可以类比字典，如果要查“mysql”这个单词，我们肯定需要定位到m字母，然后从下往下找到y字母，再找到剩下的sql。如果没有索引，那么你可能需要把所有单词看一遍才能找到你想要的，如果我想找到m开头的单词呢？或者ze开头的单词呢？是不是觉得如果没有索引，这个事情根本无法完成？ 索引原理除了词典，生活中随处可见索引的例子，如火车站的车次表、图书的目录等。它们的原理都是一样的，通过不断的缩小想要获得数据的范围来筛选出最终想要的结果，同时把随机的事件变成顺序的事件，也就是我们总是通过同一种查找方式来锁定数据。 数据库也是一样，但显然要复杂许多，因为不仅面临着等值查询，还有范围查询(&gt;、&lt;、between、in)、模糊查询(like)、并集查询(or)等等。数据库应该选择怎么样的方式来应对所有的问题呢？我们回想字典的例子，能不能把数据分成段，然后分段查询呢？最简单的如果1000条数据，1到100分成第一段，101到200分成第二段，201到300分成第三段……这样查第250条数据，只要找第三段就可以了，一下子去除了90%的无效数据。 索引的使用 查看索引 1show index from 表名; 创建索引 如果指定字段是字符串，需要指定长度，建议长度与定义字段时的长度一致 字段类型如果不是字符串，可以不填写长度部分 1create index 索引名称 on 表名(字段名称(长度)) 删除索引 1drop index 索引名称 on 表名; 索引demo创建测试表testindex1create table test_index(title varchar(10)); 使用python程序(ipython也可以)通过pymsql模块向表中加入十万条数据123456789101112131415from pymysql import connectdef main(): # 创建Connection连接 conn = connect(host='localhost',port=3306,database='jing_dong',user='root',password='mysql',charset='utf8') # 获得Cursor对象 cursor = conn.cursor() # 插入10万次数据 for i in range(100000): cursor.execute("insert into test_index values('ha-%d')" % i) # 提交数据 conn.commit()if __name__ == "__main__": main() 查询 开启运行时间监测 1set profiling=1; 查找第1万条数据ha-99999 1select * from test_index where title='ha-99999'; 查看执行的时间 1show profiles; 为表title_index的title列创建索引 1create index title_index on test_index(title(10)); 执行查询语句 1select * from test_index where title='ha-99999'; 再次查看执行的时间 1show profiles; 注意 要注意的是，建立太多的索引将会影响更新和插入的速度，因为它需要同样更新每个索引文件。对于一个经常需要更新和插入的表格，就没有必要为一个很少使用的where子句单独建立索引了，对于比较小的表，排序的开销不会很大，也没有必要建立另外的索引。 建立索引会占用磁盘空间 账户管理 在生产环境下操作数据库时，绝对不可以使用root账户连接，而是创建特定的账户，授予这个账户特定的操作权限，然后连接进行操作，主要的操作就是数据的CURD MySQL账户体系：根据账户所具有的权限的不同，MySQL的账户可以分为以下几种: 服务实例级账号：启动了一个mysqld，即为一个数据库实例；如果某用户如root,拥有服务实例级分配的权限，那么该账号就可以删除所有的数据库、连同这些库中的表 数据库级别账号：对特定数据库执行增删改查的所有操作 数据表级别账号：对特定表执行增删改查等所有操作 字段级别的权限：对某些表的特定字段进行操作 存储程序级别的账号：对存储程序进行增删改查的操作 账户的操作主要包括创建账户、删除账户、修改密码、授权权限等 注意： 进行账户操作时，需要使用root账户登录，这个账户拥有最高的实例级权限 通常都使用数据库级操作权限 授予权限需要使用实例级账户登录后操作，以root为例 主要操作包括： 查看所有用户 修改密码 删除用户 查看所有用户 所有用户及权限信息存储在mysql数据库的user表中 查看user表的结构 1desc user; 主要字段说明: Host表示允许访问的主机 User表示用户名 authentication_string表示密码，为加密后的值 查看所有用户 1select host,user,authentication_string from user; 结果 1234567891011mysql&gt; select host,user,authentication_string from user;+-----------+------------------+-------------------------------------------+| host | user | authentication_string |+-----------+------------------+-------------------------------------------+| localhost | root | *E74858DB86EBA20BC33D0AECAE8A8108C56B17FA || localhost | mysql.sys | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE || localhost | debian-sys-maint | *EFED9C764966EDB33BB7318E1CBD122C0DFE4827 |+-----------+------------------+-------------------------------------------+3 rows in set (0.00 sec) 创建账户、授权 需要使用实例级账户登录后操作，以root为例 常用权限主要包括：create、alter、drop、insert、update、delete、select 如果分配所有权限，可以使用all privileges 创建账户&amp;授权1grant 权限列表 on 数据库 to '用户名'@'访问主机' identified by '密码'; 示例1创建一个laowang的账号，密码为123456，只能通过本地访问, 并且只能对jing_dong数据库中的所有表进行读操作 使用root登录123mysql -uroot -p回车后写密码，然后回车 创建账户并授予所有权限1grant select on jing_dong.* to 'laowang'@'localhost' identified by '123456'; 说明: 可以操作python数据库的所有表，方式为:jing_dong.* 访问主机通常使用 百分号%表示此账户可以使用任何ip的主机登录访问此数据库 访问主机可以设置成localhost或具体的ip，表示只允许本机或特定主机访问 查看用户有哪些权限 1show grants for laowang@localhost; 退出root的登录1quit 使用laowang账户登录123mysql -ulaowang -p回车后写密码，然后回车 登录后效果如下图: 示例2创建一个laoli的账号，密码为12345678，可以任意电脑进行链接访问, 并且对jing_dong数据库中的所有表拥有所有权限 1grant all privileges on jing_dong.* to "laoli"@"%" identified by "12345678" 账户操作修改权限1grant 权限名称 on 数据库 to 账户@主机 with grant option; 修改密码使用root登录，修改mysql数据库的user表 使用password()函数进行密码加密 123update user set authentication_string=password('新密码') where user='用户名';例：update user set authentication_string=password('123') where user='laowang'; 注意修改完成后需要刷新权限 1刷新权限：flush privileges 远程登录(危险慎用)如果向在一个Ubuntu中使用msyql命令远程连接另外一台mysql服务器的话，通过以下方式即可完成，但是此方法仅仅了解就好了，不要在实际生产环境中使用 修改/etc/mysql/mysql.conf.d/mysqld.cnf文件 1vim /etc/mysql/mysql.conf.d/mysqld.cnf 然后重启msyql 1service mysql restart 在另外一台Ubuntu中进行连接测试 如果依然连不上，可能原因： 1) 网络不通 通过ping xxx.xxx.xx.xxx可以发现网络是否正常 2) 查看数据库是否配置了bind_address参数 本地登录数据库查看my.cnf文件和数据库当前参数show variables like ‘bind_address’; 如果设置了bind_address=127.0.0.1 那么只能本地登录 3) 查看数据库是否设置了skip_networking参数 如果设置了该参数，那么只能本地登录mysql数据库 4) 端口指定是否正确 删除账户 语法1：使用root登录 123drop user '用户名'@'主机';例：drop user 'laowang'@'%'; 语法2：使用root登录，删除mysql数据库的user表中数据 123456789delete from user where user='用户名';例：delete from user where user='laowang';-- 操作结束之后需要刷新权限flush privileges 推荐使用语法1删除用户, 如果使用语法1删除失败，采用语法2方式 忘记root账户密码怎么办? 一般也轮不到我们来管理root账户,所以别瞎卖白粉的心了 万一呢? 到时候再来查http://blog.csdn.net/lxpbs8851/article/details/10895085]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL与Python交互]]></title>
    <url>%2F2018%2F11%2F02%2FMySQL%E4%B8%8EPython%E4%BA%A4%E4%BA%92%2F</url>
    <content type="text"><![CDATA[在上一篇博文中我们详细的讲了一些MySQL的查询相关的知识，相信大家已经对MySQL的查询的相关操作已经熟稔于心。这篇博文我们来讲一些实战性的东西，也就是如何用Python语言来操作MySQL数据库。好了，话不多说，让我们开始吧！ 准备数据准备数据表12345678910111213141516171819-- 创建 "京东" 数据库create database jing_dong charset=utf8;-- 使用 "京东" 数据库use jing_dong;-- 创建一个商品goods数据表create table goods( id int unsigned primary key auto_increment not null, name varchar(150) not null, cate_name varchar(40) not null, brand_name varchar(40) not null, price decimal(10,3) not null default 0, is_show bit not null default 1, is_saleoff bit not null default 0); 插入数据12345678910111213141516171819202122232425262728293031323334353637383940414243-- 向goods表中插入数据insert into goods values(0,'r510vc 15.6英寸笔记本','笔记本','华硕','3399',default,default); insert into goods values(0,'y400n 14.0英寸笔记本电脑','笔记本','联想','4999',default,default);insert into goods values(0,'g150th 15.6英寸游戏本','游戏本','雷神','8499',default,default); insert into goods values(0,'x550cc 15.6英寸笔记本','笔记本','华硕','2799',default,default); insert into goods values(0,'x240 超极本','超级本','联想','4880',default,default); insert into goods values(0,'u330p 13.3英寸超极本','超级本','联想','4299',default,default); insert into goods values(0,'svp13226scb 触控超极本','超级本','索尼','7999',default,default); insert into goods values(0,'ipad mini 7.9英寸平板电脑','平板电脑','苹果','1998',default,default);insert into goods values(0,'ipad air 9.7英寸平板电脑','平板电脑','苹果','3388',default,default); insert into goods values(0,'ipad mini 配备 retina 显示屏','平板电脑','苹果','2788',default,default); insert into goods values(0,'ideacentre c340 20英寸一体电脑 ','台式机','联想','3499',default,default); insert into goods values(0,'vostro 3800-r1206 台式电脑','台式机','戴尔','2899',default,default); insert into goods values(0,'imac me086ch/a 21.5英寸一体电脑','台式机','苹果','9188',default,default); insert into goods values(0,'at7-7414lp 台式电脑 linux ）','台式机','宏碁','3699',default,default); insert into goods values(0,'z220sff f4f06pa工作站','服务器/工作站','惠普','4288',default,default); insert into goods values(0,'poweredge ii服务器','服务器/工作站','戴尔','5388',default,default); insert into goods values(0,'mac pro专业级台式电脑','服务器/工作站','苹果','28888',default,default); insert into goods values(0,'hmz-t3w 头戴显示设备','笔记本配件','索尼','6999',default,default); insert into goods values(0,'商务双肩背包','笔记本配件','索尼','99',default,default); insert into goods values(0,'x3250 m4机架式服务器','服务器/工作站','ibm','6888',default,default); insert into goods values(0,'商务双肩背包','笔记本配件','索尼','99',default,default); SQL演练SQL语句的强化 查询类型cate_name为’超极本’的商品名称、价格 1select name,price from goods where cate_name = '超级本'; 显示商品的种类 1select cate_name from goods group by cate_name; 求所有电脑产品的平均价格,并且保留两位小数 1select round(avg(price),2) as avg_price from goods; 显示每种商品的平均价格 1select cate_name,avg(price) from goods group by cate_name; 查询每种类型的商品中 最贵、最便宜、平均价、数量 1select cate_name,max(price),min(price),avg(price),count(*) from goods group by cate_name; 查询所有价格大于平均价格的商品，并且按价格降序排序 123select id,name,price from goods where price &gt; (select round(avg(price),2) as avg_price from goods) order by price desc; 查询每种类型中最贵的电脑信息 1234567891011select * from goodsinner join ( select cate_name, max(price) as max_price, min(price) as min_price, avg(price) as avg_price, count(*) from goods group by cate_name ) as goods_new_info on goods.cate_name=goods_new_info.cate_name and goods.price=goods_new_info.max_price; 创建 “商品分类”” 表12345-- 创建商品分类表create table if not exists goods_cates( id int unsigned primary key auto_increment, name varchar(40) not null); 查询goods表中商品的种类 1select cate_name from goods group by cate_name; 将分组结果写入到goods_cates数据表 1insert into goods_cates (name) select cate_name from goods group by cate_name; 同步表数据 通过goods_cates数据表来更新goods表 1update goods as g inner join goods_cates as c on g.cate_name=c.name set g.cate_name=c.id; 创建 “商品品牌表” 通过create...select来创建数据表并且同时写入记录,一步到位 12345678-- select brand_name from goods group by brand_name;-- 在创建数据表的时候一起插入数据-- 注意: 需要对brand_name 用as起别名，否则name字段就没有值create table goods_brands ( id int unsigned primary key auto_increment, name varchar(40) not null) select brand_name as name from goods group by brand_name; 同步数据 通过goods_brands数据表来更新goods数据表 1update goods as g inner join goods_brands as b on g.brand_name=b.name set g.brand_name=b.id; 修改表结构 查看goods的数据表结构,会发现cate_name和brand_name对应的类型为 varchar但是存储的都是数字 1desc goods; 通过alter table语句修改表结构 123alter table goods change cate_name cate_id int unsigned not null,change brand_name brand_id int unsigned not null; 外键 分别在goods_cates和goods_brands表中插入记录 12insert into goods_cates(name) values ('路由器'),('交换机'),('网卡');insert into goods_brands(name) values ('海尔'),('清华同方'),('神舟'); 在goods数据表中写入任意记录 12insert into goods (name,cate_id,brand_id,price)values('LaserJet Pro P1606dn 黑白激光打印机', 12, 4,'1849'); 查询所有商品的详细信息 (通过内连接) 123select g.id,g.name,c.name,b.name,g.price from goods as ginner join goods_cates as c on g.cate_id=c.idinner join goods_brands as b on g.brand_id=b.id; 查询所有商品的详细信息 (通过左连接) 123select g.id,g.name,c.name,b.name,g.price from goods as gleft join goods_cates as c on g.cate_id=c.idleft join goods_brands as b on g.brand_id=b.id; 如何防止无效信息的插入,就是可以在插入前判断类型或者品牌名称是否存在呢? 可以使用之前讲过的外键来解决 外键约束:对数据的有效性进行验证 关键字: foreign key,只有innodb数据库引擎支持外键约束 对于已经存在的数据表 如何更新外键约束 123456789-- 给brand_id 添加外键约束成功alter table goods add foreign key (brand_id) references goods_brands(id);-- 给cate_id 添加外键失败-- 会出现1452错误-- 错误原因:已经添加了一个不存在的cate_id值12,因此需要先删除alter table goods add foreign key (cate_id) references goods_cates(id); 如何在创建数据表的时候就设置外键约束呢? 注意: goods中的cate_id的类型一定要和goods_cates表中的id类型一致 1234567891011create table goods( id int primary key auto_increment not null, name varchar(40) default '', price decimal(5,2), cate_id int unsigned, brand_id int unsigned, is_show bit default 1, is_saleoff bit default 0, foreign key(cate_id) references goods_cates(id), foreign key(brand_id) references goods_brands(id)); 如何取消外键约束 1234567-- 需要先获取外键约束名称,该名称系统会自动生成,可以通过查看表创建语句来获取名称show create table goods;-- 获取名称之后就可以根据名称来删除外键约束alter table goods drop foreign key 外键名称; 在实际开发中,很少会使用到外键约束,会极大的降低表更新的效率 数据库的设计 创建 “商品分类” 表(之前已经创建,无需再次创建)1234create table goods_cates( id int unsigned primary key auto_increment not null, name varchar(40) not null); 创建 “商品品牌” 表(之前已经创建,无需再次创建)1234create table goods_brands ( id int unsigned primary key auto_increment not null, name varchar(40) not null); 创建 “商品” 表(之前已经创建,无需再次创建)1234567891011create table goods( id int unsigned primary key auto_increment not null, name varchar(40) default '', price decimal(5,2), cate_id int unsigned, brand_id int unsigned, is_show bit default 1, is_saleoff bit default 0, foreign key(cate_id) references goods_cates(id), foreign key(brand_id) references goods_brands(id)); 创建 “顾客” 表123456create table customer( id int unsigned auto_increment primary key not null, name varchar(30) not null, addr varchar(100), tel varchar(11) not null); 创建 “订单” 表123456create table orders( id int unsigned auto_increment primary key not null, order_date_time datetime not null, customer_id int unsigned, foreign key(customer_id) references customer(id)); 创建 “订单详情” 表12345678create table order_detail( id int unsigned auto_increment primary key not null, order_id int unsigned not null, goods_id int unsigned not null, quantity tinyint unsigned not null, foreign key(order_id) references orders(id), foreign key(goods_id) references goods(id)); 说明 以上创建表的顺序是有要求的,即如果goods表中的外键约束用的是goods_cates或者是goods_brands,那么就应该先创建这2个表,否则创建goods会失败 创建外键时,一定要注意类型要相同,否则失败 Python中操作MySQL步骤 引入模块 在python文件中引入pymysql模块(python3) 1from pymysql import * Connection对象 用于建立与数据库的连接 创建对象：调用connect()方法 1conn=connect(参数列表) 参数host：连接的mysql主机，如果本机是’localhost’ 参数port：连接的mysql主机的端口，默认是3306 参数database：数据库的名称 参数user：连接的用户名 参数password：连接的密码 参数charset：通信采用的编码方式，推荐使用utf8 对象的方法 close()关闭连接 commit()提交 cursor()返回Cursor对象，用于执行sql语句并获得结果 Cursor对象 用于执行sql语句，使用频度最高的语句为select、insert、update、delete 获取Cursor对象：调用Connection对象的cursor()方法 1cs1=conn.cursor() 对象的方法 close()关闭 execute(operation [, parameters ])执行语句，返回受影响的行数，主要用于执行insert、update、delete语句，也可以执行create、alter、drop等语句 fetchone()执行查询语句时，获取查询结果集的第一个行数据，返回一个元组 fetchall()执行查询时，获取结果集的所有行，一行构成一个元组，再将这些元组装入一个元组返回 对象的属性 rowcount只读属性，表示最近一次execute()执行后受影响的行数 connection获得当前连接对象 增删改查增删改12345678910111213141516171819202122232425262728293031from pymysql import *def main(): # 创建Connection连接 conn = connect(host='localhost',port=3306,database='jing_dong',user='root',password='mysql',charset='utf8') # 获得Cursor对象 cs1 = conn.cursor() # 执行insert语句，并返回受影响的行数：添加一条数据 # 增加 count = cs1.execute('insert into goods_cates(name) values("硬盘")') #打印受影响的行数 print(count) count = cs1.execute('insert into goods_cates(name) values("光盘")') print(count) # # 更新 # count = cs1.execute('update goods_cates set name="机械硬盘" where name="硬盘"') # # 删除 # count = cs1.execute('delete from goods_cates where id=6') # 提交之前的操作，如果之前已经之执行过多次的execute，那么就都进行提交 conn.commit() # 关闭Cursor对象 cs1.close() # 关闭Connection对象 conn.close()if __name__ == '__main__': main() 查询一行数据12345678910111213141516171819202122232425from pymysql import *def main(): # 创建Connection连接 conn = connect(host='localhost',port=3306,user='root',password='mysql',database='jing_dong',charset='utf8') # 获得Cursor对象 cs1 = conn.cursor() # 执行select语句，并返回受影响的行数：查询一条数据 count = cs1.execute('select id,name from goods where id&gt;=4') # 打印受影响的行数 print("查询到%d条数据:" % count) for i in range(count): # 获取查询的结果 result = cs1.fetchone() # 打印查询的结果 print(result) # 获取查询的结果 # 关闭Cursor对象 cs1.close() conn.close()if __name__ == '__main__': main() 查询多行数据12345678910111213141516171819202122232425262728from pymysql import *def main(): # 创建Connection连接 conn = connect(host='localhost',port=3306,user='root',password='mysql',database='jing_dong',charset='utf8') # 获得Cursor对象 cs1 = conn.cursor() # 执行select语句，并返回受影响的行数：查询一条数据 count = cs1.execute('select id,name from goods where id&gt;=4') # 打印受影响的行数 print("查询到%d条数据:" % count) # for i in range(count): # # 获取查询的结果 # result = cs1.fetchone() # # 打印查询的结果 # print(result) # # 获取查询的结果 result = cs1.fetchall() print(result) # 关闭Cursor对象 cs1.close() conn.close()if __name__ == '__main__': main() 参数化 sql语句的参数化，可以有效防止sql注入 注意：此处不同于python的字符串格式化，全部使用%s占位 123456789101112131415161718192021222324252627282930313233343536373839404142from pymysql import *def main(): find_name = input("请输入物品名称：") # 创建Connection连接 conn = connect(host='localhost',port=3306,user='root',password='mysql',database='jing_dong',charset='utf8') # 获得Cursor对象 cs1 = conn.cursor() # # 非安全的方式 # # 输入 " or 1=1 or " (双引号也要输入) # sql = 'select * from goods where name="%s"' % find_name # print("""sql===&gt;%s&lt;====""" % sql) # # 执行select语句，并返回受影响的行数：查询所有数据 # count = cs1.execute(sql) # 安全的方式 # 构造参数列表 params = [find_name] # 执行select语句，并返回受影响的行数：查询所有数据 count = cs1.execute('select * from goods where name=%s', params) # 注意： # 如果要是有多个参数，需要进行参数化 # 那么params = [数值1, 数值2....]，此时sql语句中有多个%s即可 # 打印受影响的行数 print(count) # 获取查询的结果 # result = cs1.fetchone() result = cs1.fetchall() # 打印查询的结果 print(result) # 关闭Cursor对象 cs1.close() # 关闭Connection对象 conn.close()if __name__ == '__main__': main()]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[详解MySQL查询]]></title>
    <url>%2F2018%2F10%2F31%2F%E8%AF%A6%E8%A7%A3MySQL%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[在上一篇博文中我们讲到了MySQL的一些命令行脚本，主要讲了数据库的操作、数据表的操作以及对数据的增删改查(CURD)操作。这几个操作里面我们最常见的就是CURD操作啦，在CURD操作中，我们最常见的就是查询(Select)操作，所以这篇博文我们就来详细的谈一谈MySQL的查询操作。 简单查询创建数据库、数据表12345678910111213141516171819202122-- 创建数据库create database python_test_1 charset=utf8;-- 使用数据库use python_test_1;-- students表create table students( id int unsigned primary key auto_increment not null, name varchar(20) default '', age tinyint unsigned default 0, height decimal(5,2), gender enum('男','女','中性','保密') default '保密', cls_id int unsigned default 0, is_delete bit default 0);-- classes表create table classes ( id int unsigned auto_increment primary key not null, name varchar(30) not null); 准备数据12345678910111213141516171819-- 向students表中插入数据insert into students values(0,'小明',18,180.00,2,1,0),(0,'小月月',18,180.00,2,2,1),(0,'彭于晏',29,185.00,1,1,0),(0,'刘德华',59,175.00,1,2,1),(0,'黄蓉',38,160.00,2,1,0),(0,'凤姐',28,150.00,4,2,1),(0,'王祖贤',18,172.00,2,1,1),(0,'周杰伦',36,NULL,1,1,0),(0,'程坤',27,181.00,1,2,0),(0,'刘亦菲',25,166.00,2,2,0),(0,'金星',33,162.00,3,3,1),(0,'静香',12,180.00,2,4,0),(0,'郭靖',12,170.00,1,4,0),(0,'周杰',34,176.00,2,5,0);-- 向classes表中插入数据insert into classes values (0, "python_01期"), (0, "python_02期"); 查询所有字段 123select * from 表名;例：select * from students; 查询指定字段 123select 列1,列2,... from 表名;例:select name from students; 使用as给字段取别名 1select id as 序号, name as 名字, gender as 性别 from students; 可以通过as给表取别名 12345678-- 如果是单表查询 可以省略表明select id, name, gender from students;-- 表名.字段名select students.id,students.name,students.gender from students;-- 可以通过 as 给表起别名 select s.id,s.name,s.gender from students as s; 消除重复行 在select后面列前使用distinct可以消除重复的行 123select distinct 列1,... from 表名;例：select distinct gender from students; 条件使用Where子句对表中的数据筛选，结果为True的行会出现在结果集中。 语法如下： 123select * from 表名 where 条件;例：select * from students where id=1; where后面支持多种运算符，进行条件的处理： 比较运算符 逻辑运算符 模糊查询 范围查询 空判断 比较运算符 等于: = 大于: &gt; 大于等于: &gt;= 小于: &lt; 小于等于: &lt;= 不等于: &lt;&gt;或者!= 例1：查询编号大于3的学生： 1select * from students where id &gt; 3; 例2：查询编号不大于4的学生： 1select * from students where id &lt;= 4; 例3：查询姓名不是“黄蓉”的学生： 1select * from students where name != '黄蓉'; 例4：查询没被删除的学生： 1select * from students where is_delete=0; 逻辑运算符 and or not 例4：查询编号大于3的女同学 1select * from students where id &gt; 3 and gender=0; 例5：查询编号小于4或没被删除的学生 1select * from students where id &lt; 4 or is_delete=0; 模糊查询 like %表示任意多个任意字符 _表示一个任意字符 rlike 正则匹配 例7：查询姓黄的学生： 1select * from students where name like '黄%'; 例8：查询姓黄并且并且”名”是一个字的学生： 1select * from students where name like '黄_'; 例9：查询姓黄或者叫靖的学生： 1select * from students where name like '黄%' or name like '%靖'; 例10：查询以’周’开头的学生(正则表达式)： 1select * from students where name rlike '^周.*' 例11：查询以’周’开头，以’伦’结尾的学生： 1select * from students where name rlike '^周.*伦$' 范围查询 in表示在一个非连续的范围内 例12：查询编号是1或3或8的学生： 1select * from students where id in(1,3,8); between ... and ...表示在一个连续的范围内： 例13：查询编号为3-8的学生： 1select * from students where id between 3 and 8; 例14：查询编号是3-8的男生： 1select * from students where (id between 3 and 8) and gender=1; 空判断 注意：null与’’是不同的 判空is null 例15：查询没有填写身高的学生： 1select * from students where height is null; 判非空is not null 例16：查询填写了身高的学生： 1select * from students where height is not null; 例17：查询填写了身高的男生： 1select * from students where height is not null and gender=1; 优先级 优先级从高到低的顺序为：小括号, not, 比较运算符, 逻辑运算符 and比or先运算，如果同时出现并希望先运算or，需要结合()使用 排序为了方便查看数据，可以对数据进行排序 语法： 1select * from 表名 order by 列1 asc|desc [,列2 asc|desc,...] 说明： 将行数据按照列1进行排序，如果某些行列1的值相同时，则按照列2排序，以此类推 默认按照列值从小到大排列(asc) asc从小到大排列，即升序 desc从大到小排列，即降序 例1：查询未删除男生信息，按学号降序排序： 1select * from students where gender=1 and is_delete=0 order by id desc; 例2：查询未删除学生信息，按照学号升序排序： 1select * from students where is_delete=0 order by name; 例3：显示所有的学生信息，先按照年龄从大–&gt;小排序，当年龄相同时 按照身高从高–&gt;矮排序 1select * from students order by age desc,height desc; 聚合函数为了快速得到统计数据，经常会用到如下5个聚合函数： 总数 count(*)表示计算总行数，括号中写星与列名，结果是相同的 例1：查询学生总数 1select count(*) from students; 最大值 max(列)表示求此列的最大值 例2：查询女生的编号的最大值 1select max(id) from students where gender=2; 最小值 min(列)表示求此列的最小值 例3：查询未删除的学生的最小编号： 1select min(id) from students where is_delete=0; 求和 sum(列)表示求此列的和 例4：查询男生的总年龄 1234select sum(age) from students where gender=1;-- 平均年龄select sum(age)/count(*) from students where gender=1; 平均值 avg(列)表示求此列的平均值 例5：查询未删除女生的编号的平均值 1234select avg(id) from students where is_delete=0 and gender=2;--计算平均值还可以用sum(列)/count(*)select sum(id)/count(*) from students where is_delete=0 and gender=2; 近似 round(计算结果, 保留小数的位数)表示求对应的结果的近似值 例6：计算所有人的平均年龄，并保留两位小数 1select round(avg(age),2) from students; 分组group by group by的含义，将查询结果按照1个或多个字段进行分组，字段值相同的为一组 group by可用于单个字段分组，也可用于多个字段分组 12345678910111213141516171819202122232425262728293031select * from students;+----+-----------+------+--------+--------+--------+-----------+| id | name | age | height | gender | cls_id | is_delete |+----+-----------+------+--------+--------+--------+-----------+| 1 | 小明 | 18 | 180.00 | 女 | 1 | || 2 | 小月月 | 18 | 180.00 | 女 | 2 |  || 3 | 彭于晏 | 29 | 185.00 | 男 | 1 | || 4 | 刘德华 | 59 | 175.00 | 男 | 2 |  || 5 | 黄蓉 | 38 | 160.00 | 女 | 1 | || 6 | 凤姐 | 28 | 150.00 | 保密 | 2 |  || 7 | 王祖贤 | 18 | 172.00 | 女 | 1 |  || 8 | 周杰伦 | 36 | NULL | 男 | 1 | || 9 | 程坤 | 27 | 181.00 | 男 | 2 | || 10 | 刘亦菲 | 25 | 166.00 | 女 | 2 | || 11 | 金星 | 33 | 162.00 | 中性 | 3 |  || 12 | 静香 | 12 | 180.00 | 女 | 4 | || 13 | 周杰 | 34 | 176.00 | 女 | 5 | || 14 | 郭靖 | 12 | 170.00 | 男 | 4 | |+----+-----------+------+--------+--------+--------+-----------+select gender from students group by gender;+--------+| gender |+--------+| 男 || 女 || 中性 || 保密 |+--------+ 根据gender字段来分组，gender字段的全部值有4个’男’, ‘女’, ‘中性’, ‘保密’，所以分为了4组。当group by单独使用时，只显示出每组的第一条记录, 所以group by单独使用时的实际意义不大 group by + group_concat() group_concat(字段名)可以作为一个输出字段来使用 表示分组之后，根据分组结果，使用group_concat()来放置每一组的某字段的值的集合 123456789101112131415161718192021222324252627282930313233343536select gender from students group by gender;+--------+| gender |+--------+| 男 || 女 || 中性 || 保密 |+--------+select gender,group_concat(name) from students group by gender;+--------+-----------------------------------------------------------+| gender | group_concat(name) |+--------+-----------------------------------------------------------+| 男 | 彭于晏,刘德华,周杰伦,程坤,郭靖 || 女 | 小明,小月月,黄蓉,王祖贤,刘亦菲,静香,周杰 || 中性 | 金星 || 保密 | 凤姐 |+--------+-----------------------------------------------------------+select gender,group_concat(id) from students group by gender;+--------+------------------+| gender | group_concat(id) |+--------+------------------+| 男 | 3,4,8,9,14 || 女 | 1,2,5,7,10,12,13 || 中性 | 11 || 保密 | 6 |+--------+------------------+ group by + 聚合函数 通过group_concat()的启发，我们既然可以统计出每个分组的某字段的值的集合，那么我们也可以通过集合函数来对这个值的集合做一些操作 12345678910111213141516171819202122232425262728293031323334353637select gender,group_concat(age) from students group by gender;+--------+----------------------+| gender | group_concat(age) |+--------+----------------------+| 男 | 29,59,36,27,12 || 女 | 18,18,38,18,25,12,34 || 中性 | 33 || 保密 | 28 |+--------+----------------------+分别统计性别为男/女的人年龄平均值select gender,avg(age) from students group by gender;+--------+----------+| gender | avg(age) |+--------+----------+| 男 | 32.6000 || 女 | 23.2857 || 中性 | 33.0000 || 保密 | 28.0000 |+--------+----------+分别统计性别为男/女的人的个数select gender,count(*) from students group by gender;+--------+----------+| gender | count(*) |+--------+----------+| 男 | 5 || 女 | 7 || 中性 | 1 || 保密 | 1 |+--------+----------+ group by + having having条件表达式：用来分组查询后指定一些条件来输出查询结果 having作用和where一样，但having只能用于group by 12345678select gender,count(*) from students group by gender having count(*)&gt;2;+--------+----------+| gender | count(*) |+--------+----------+| 男 | 5 || 女 | 7 |+--------+----------+ group by + with rollup with rollup的作用是：在最后新增一行，来记录当前列里所有记录的总和 123456789101112131415161718192021222324select gender,count(*) from students group by gender with rollup;+--------+----------+| gender | count(*) |+--------+----------+| 男 | 5 || 女 | 7 || 中性 | 1 || 保密 | 1 || NULL | 14 |+--------+----------+select gender,group_concat(age) from students group by gender with rollup;+--------+-------------------------------------------+| gender | group_concat(age) |+--------+-------------------------------------------+| 男 | 29,59,36,27,12 || 女 | 18,18,38,18,25,12,34 || 中性 | 33 || 保密 | 28 || NULL | 29,59,36,27,12,18,18,38,18,25,12,34,33,28 |+--------+-------------------------------------------+ 分页获取部分行当数据量过大时，在一页中查看数据是一件非常麻烦的事情 语法： 1select * from 表名 limit start,count 说明： 从start开始，获取count条数据 例1：查询前3行男生信息： 1select * from students where gender=1 limit 0,3; 示例： 已知：每页显示m条数据，当前显示第n页 求总页数：此段逻辑后面会在python中实现 查询总条数p1 使用p1除以m得到p2 如果整除则p2为总页数 如果不整除则p2+1为总页数 求第n页的数据 1select * from students where is_delete=0 limit (n-1)*m,m 连接查询当查询结果的列来源于多张表时，需要将多张表连接成一个大的数据集，再选择合适的列返回mysql支持三种类型的连接查询, 分别为： 内连接查询：查询的结果为两个表匹配到的数据： 右连接查询：查询的结果为两个表匹配到的数据，右表特有的数据，对于左表中不存在的数据使用null填充 左连接查询：查询的结果为两个表匹配到的数据，左表特有的数据，对于右表中不存在的数据使用null填充 语法： 1select * from 表1 inner或left或right join 表2 on 表1.列 = 表2.列 例1：使用内连接查询班级表与学生表: 1select * from students inner join classes on students.cls_id = classes.id; 例2：使用左连接查询班级表与学生表: 此处使用了as为表起别名，目的是编写简单 1select * from students as s left join classes as c on s.cls_id = c.id; 例3：使用右连接查询班级表与学生表： 1select * from students as s right join classes as c on s.cls_id = c.id; 例4：查询学生姓名及班级名称： 1select s.name,c.name from students as s inner join classes as c on s.cls_id = c.id; 例5：查询没有对应班级信息的学生 1select * from students as s left join classes as s on s.cls_id = c.id having c.id is null 自关联 设计省信息的表结构provinces id ptitle 设计市信息的表结构citys id ctitle proid citys表的proid表示城市所属的省，对应着provinces表的id值 问题: 能不能将两张表合成一张表呢? 思考: 观察两张表发现，citys表比provinces表多一个列proid，其它列的类型都是一样的 意义: 存储的都是地区信息，而且每种信息的数据量有限，没必要增加一个新表，或者将来还要存储区、乡镇信息，都增加新表的开销太大 答案： 定义表areas，结构如下： id atitle proid 说明： 因为省没有所属的省份，所以可以填写为null 城市所属的省份pid，填写省所对应的编号id 这就是自关联，表中的某一列，关联了这个表中的另外一列，但是它们的业务逻辑含义是不一样的，城市信息的pid引用的是省信息的id 在这个表中，结构不变，可以添加区县、乡镇街道、村社区等信息 创建areas表的语句如下： 12345create table areas( aid int primary key, atitle varchar(20), pid int); 从sql文件中导入数据 1source areas.sql; 查询一共有多少省 1select count(*) from areas where pid is null; 例1：查询省的名称为“山西省”的所有城市: 123select city.* from areas as cityinner join areas as province on city.pid=province.aidwhere province.atitle='山西省'; 例2：查询市的名称为“广州市”的所有区县: 123select dis.* from areas as disinner join areas as city on city.aid=dis.pidwhere city.atitle='广州市'; 子查询子查询 在一个select语句中,嵌入了另外一个select语句, 那么被嵌入的select语句称之为子查询语句 主查询 主要查询的对象,第一条select语句 主查询和子查询的关系 子查询是嵌入到主查询中 子查询是辅助主查询的，要么充当条件，要么充当数据源 子查询是可以独立存在的语句，是一条完整的select语句 子查询分类 标量子查询：子查询返回的的结果是一个数据(一行一列) 列子查询：返回的结果是一列(一列多行) 行子查询：返回的结果是一行(一行多列) 标量子查询 查询班级学生的平均年龄 查询大于平均年龄的学生 查询班级学生的平均身高： 1select * from students where age &gt; (select avg(age) from students); 列级子查询 查询还有学生在班的所有班级名字 找出学生表中所有的班级id 找出班级表中对应的名字 1select name from classes where id in (select cls_id from students); 行级子查询 需求：查找班级年龄最大,身高最高的学生 行元素：将多个字段合成一个行元素，在行级子查询中会使用到行元素 1select * from students where (height,age) = (select max(height),max(age) from students); 子查询中特定关键字使用 in范围 格式: 主查询 where 条件 in (列子查询) 总结查询的完整格式： 12345678SELECT select_expr [,select_expr,...] [ FROM tb_name [WHERE 条件判断] [GROUP BY &#123;col_name | postion&#125; [ASC | DESC], ...] [HAVING WHERE 条件判断] [ORDER BY &#123;col_name|expr|postion&#125; [ASC | DESC], ...] [ LIMIT &#123;[offset,]rowcount | row_count OFFSET offset&#125;]] 完整的select语句 123456select distinct *from 表名where ....group by ... having ...order by ...limit start,count 执行顺序为： from表名 where…… group by select distict * having…… order by…… limit start,count 实际使用中，只是语句中某些部分的组合，而不是全部]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL命令行脚本必知必会]]></title>
    <url>%2F2018%2F10%2F30%2FMySQL%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%84%9A%E6%9C%AC%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A%2F</url>
    <content type="text"><![CDATA[相信学过数据库这门课的人对SQL都不会陌生，SQL是结构化查询语言，是一种用来操作RDBMS的数据库语言，当前的主流关系型数据库都支持使用SQL语言进行操作, 也就是说可以通过SQL操作Oracle, SQL Server, MySQL, SQLite等所有主流的关系型的数据库。 MySQL是一个关系型数据库管理系统，由瑞典MySQL AB公司开发，后来被Sun公司收购，Sun公司后来又被Oracle公司收购，目前属于Oracle旗下产品。MySQL数据库开源、免费、不要钱、使用范围广, 跨平台支持性好, 提供了多种语言调用的API是学习数据库开发的首选。 好了讲到这里，就让我们开始MySQL命令行脚本的学习吧！ SQL简介上面也说了SQL是一门结构化查询语言，可以用来操作MySQL等主流关系型数据库。那么SQL语句可以分为哪几类呢？让我们一起来看看。 SQL语句主要分为: DQL：数据查询语言，用于对数据进行查询，如select DML：数据操作语言，对数据进行增加、修改、删除，如insert、udpate、delete TPL：事务处理语言，对事务进行处理，包括begin transaction、commit、rollback DCL：数据控制语言，进行授权与权限回收，如grant、revoke DDL：数据定义语言，进行数据库、表的管理等，如create、drop CCL：指针控制语言，通过控制指针完成表的操作，如declare cursor 对于Web开发的程序员来讲，重点是数据的CURD(增删改查)，必须熟练编写DQL、DML，能够编写DDL完成数据库、表的操作, 其它类型语句如TPL、DCL、CCL了解即可。 SQL是一门特殊的语言,专门用来操作关系数据库 不区分大小写 命令行连接这里以Linux的发行版Ubuntu 16.04操作系统为例演示命令行脚本，其他操作系统(如Windows)类似，都是大同小异的操作。 打开终端，运行命令： 123mysql -uroot -p# 回车后输入密码，当前设置的密码为mysql# 如果不想输入密码，可以直接将密码跟在该命令后面，即 mysql -uroot -pmysql Note：Windows系统安装完MySQL数据库后，直接在开始菜单里面找到MySQL 8.0 Command Line Client - Unicode程序, 点击打开，直接输入密码即可登录，无需再输入mysql -uroot -p命令 连接成功后如下图： 退出登录： 123quit 和 exit或ctrl+d 登陆成功后，输入以下命令查看效果： 12查看版本：select version();显示当前时间：select now(); 数据库操作 查看所有数据库 1show databases; 使用数据库 1use 数据库名; 查看当前使用的数据库 1select database(); 创建数据库 123create database 数据库名 charset=utf8;例：create database python charset=utf8; 查看数据库的创建语句 123show create database 数据库名;例：show create database Python; 删除数据库 123drop database 数据库名;例：drop database python; 数据表操作 查看当前数据库中所有表 1show tables; 查看表结构 1desc 表名; 创建表 auto_increment表示自动增长 12345678CREATE TABLE table_name( column1 datatype contrai, column2 datatype, column3 datatype, ..... columnN datatype, PRIMARY KEY(one or more columns)); 例：创建班级表 1234create table classes( id int unsigned auto_increment primary key not null, name varchar(10)); 创建学生表 12345678create table students( id int unsigned primary key auto_increment not null, name varchar(20) default '', age tinyint unsigned default 0, height decimal(5,2), gender enum('男','女','人妖','保密'), cls_id int unsigned default 0) 修改表-添加字段 123alter table 表名 add 列名 类型;例：alter table students add birthday datetime; 修改表-修改字段：重命名版 123alter table 表名 change 原名 新名 类型及约束;例：alter table students change birthday birth datetime not null; 修改表-修改字段：不重命名版 123alter table 表名 modify 列名 类型及约束;例：alter table students modify birth date not null; 修改表-删除字段 123alter table 表名 drop 列名;例：alter table students drop birthday; 删除表 123drop table 表名;例：drop table students; 查看表的创建语句 123show create table 表名;例：show create table classes; 数据的增删改查(CURD) CURD的解释: 代表创建(Create)、更新(Update)、读取(Retrieve)和删除(Delete) 查询 查询所有列 123select * from 表名;例：select * from classes; 查询指定列 可以使用as为列或表指定别名 123select 列1,列2,... from 表名;例：select id,name from classes; 增加 格式：INSERT [INTO] tb_name [(col_name,…)] {VALUES | VALUE} ({expr | DEFAULT},…),(…),… 说明：主键列是自动增长，但是在全列插入时需要占位，通常使用0或者default或者null来占位, 插入成功后以实际数据为准 全列插入：值的顺序与表中字段的顺序对应 123insert into 表名 values(...)例：insert into students values(0,’郭靖‘,1,'蒙古','2016-1-2'); 部分列插入：值的顺序与给出的列顺序对应 123insert into 表名(列1,...) values(值1,...)例：insert into students(name,hometown,birthday) values('黄蓉','桃花岛','2016-3-2'); 上面的语句一次可以向表中插入一行数据，还可以一次性插入多行数据，这样可以减少与数据库的通信 全列多行插入:值的顺序与给出的列顺序对应 123insert into 表名 values(...),(...)...;例：insert into classes values(0,'python1'),(0,'python2'); 部分列多行插入:值的顺序与给出的列顺序对应 123insert into 表名(列1,...) values(值1,...),(值1,...)...;例：insert into students(name) values('杨康'),('杨过'),('小龙女'); 修改 格式: UPDATE tbname SET col1={expr1|DEFAULT} [,col2={expr2|default}]…[where 条件判断] 123update 表名 set 列1=值1,列2=值2... where 条件例：update students set gender=0,hometown='北京' where id=5; 删除 DELETE FROM tbname [where 条件判断] 123delete from 表名 where 条件例：delete from students where id=5; 逻辑删除，本质就是修改操作 1update students set isdelete=1 where id=1; 备份 运行mysqldump命令 123mysqldump –uroot –p 数据库名 &gt; python.sql;# 按提示输入mysql的密码 恢复 连接mysql, 创建新的数据库， 退出连接，执行如下命令： 123mysql -uroot –p 新数据库名 &lt; python.sql# 根据提示输入mysql密码]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python之with与上下文管理器]]></title>
    <url>%2F2018%2F10%2F29%2FPython%E4%B9%8Bwith%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AE%A1%E7%90%86%E5%99%A8%2F</url>
    <content type="text"><![CDATA[如果你有阅读源码的习惯，可能会看到一些优秀的代码经常出现带有“with”关键字的语句，它通常用在什么场景呢？ 对于系统资源如文件、数据库连接、socket而言，应用程序打开这些资源并执行完业务逻辑之后，必须做的一件事就是要关闭（断开）该资源。 比如Python程序打开一个文件，往文件中写内容，写完之后，就要关闭该文件，否则会出现什么情况呢？极端情况下会出现&quot;Too many open files&quot;的错误，因为系统允许你打开的最大文件数量是有限的。 同样，对于数据库，如果连接数过多而没有及时关闭的话，就可能会出现&quot;Can not connect to MySQL server Too many connections&quot;，因为数据库连接是一种非常昂贵的资源，不可能无限制的被创建。 下面就让我们来看看如何正确关闭一个文件： 正确关闭一个文件普通版1234def m1(): f = open("output.txt", "w") f.write("python之禅") f.close() 这样写有一个潜在的问题，如果在调用write的过程中，出现了异常进而导致后续代码无法继续执行，close方法无法被正常调用，因此资源就会一直被该程序占用者释放。那么该如何改进代码呢？ 进阶版12345678def m2(): f = open("output.txt", "w") try: f.write("python之禅") except IOError: print("oops error") finally: f.close() 改良版本的程序是对可能发生异常的代码处进行try捕获，使用try/finally语句，该语句表示如果在try代码块中程序出现了异常，后续代码就不再执行，而直接跳转到except代码块。而无论如何，finally块的代码最终都会被执行。因此，只要把close放在finally代码中，文件就一定会关闭 高级版123def m3(): with open("output.txt", "r") as f: f.write("Python之禅") 一种更加简洁、优雅的方式就是用with关键字。open方法的返回值赋值给变量 f，当离开with代码块的时候，系统会自动调用f.close()方法，with的作用和使用try/finally语句是一样的。那么它的实现原理是什么？在讲with的原理前要涉及到另外一个概念，就是上下文管理器(Context Manager) 什么是上下文(context) 上下文在不同的地方表示不同的含义，要感性理解。context其实说白了，和文章的上下文是一个意思，在通俗一点，我觉得叫环境更好。…. 林冲大叫一声“啊也！”…. 问:这句话林冲的“啊也”表达了林冲怎样的心里？ 答:啊你妈个头啊！ 看，一篇文章，给你摘录一段，没前没后，你读不懂，因为有语境，就是语言环境存在，一段话说了什么，要通过上下文(文章的上下文)来推断。 app点击一个按钮进入一个新的界面，也要保存你是在哪个屏幕跳过来的等等信息，以便你点击返回的时候能正确跳回，如果不存肯定就无法正确跳回了。 看这些都是上下文的典型例子，理解成环境就可以，(而且上下文虽然叫上下文，但是程序里面一般都只有上文而已，只是叫的好听叫上下文。。进程中断在操作系统中是有上有下的，不过不这个高深的问题就不要深究了。。。) 上下文管理器任何实现了__enter__()和__exit__()方法的对象都可称之为上下文管理器，上下文管理器对象可以使用with关键字。显然，文件（file）对象也实现了上下文管理器。 那么文件对象是如何实现这两个方法的呢？我们可以模拟实现一个自己的文件类，让该类实现__enter__()和__exit__()方法 1234567891011121314class File(): def __init__(self, filename, mode): self.filename = filename self.mode = mode def __enter__(self): print("entering") self.f = open(self.filename, self.mode) return self.f def __exit__(self, *args): print("will exit") self.f.close() __enter__()方法返回资源对象，这里就是你将要打开的那个文件对象，__exit__()方法处理一些清除工作。 因为File类实现了上下文管理器，现在就可以使用with语句了 123with File('out.txt', 'w') as f: print("writing") f.write('hello, python') 这样，你就无需显示地调用close方法了，由系统自动去调用，哪怕中间遇到异常close方法也会被调用 实现上下文管理器的另外方式Python还提供了一个contextmanager的装饰器，更进一步简化了上下文管理器的实现方式。通过yield将函数分割成两部分，yield之前的语句在 __enter__方法中执行，yield之后的语句在__exit__方法中执行。紧跟在yield后面的值是函数的返回值 1234567from contextlib import contextmanager@contextmanagerdef my_open(path, mode): f = open(path, mode) yield f f.close() 调用 12with my_open('out.txt', 'w') as f: f.write("hello , the simplest context manager") 总结Python提供了with语法用于简化资源操作的后续清除操作，是try/finally 的替代方法，实现原理建立在上下文管理器之上。此外，Python还提供了一个contextmanager装饰器，更进一步简化上下管理器的实现方式。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python之魔法属性]]></title>
    <url>%2F2018%2F10%2F29%2FPython%E4%B9%8B%E9%AD%94%E6%B3%95%E5%B1%9E%E6%80%A7%2F</url>
    <content type="text"><![CDATA[无论是人或事物往往都有不按套路出牌的情况，Python的类属性也是如此，存在着一些具有特殊含义的属性。下面就要我们一起来看一下吧！ __doc__ 表示类的描述信息 1234567class Foo: """ 描述类信息，这是用于看片的神奇 """ def func(self): passprint(Foo.__doc__)#输出：类的描述信息 __module__和__class__ __module__ 表示当前操作的对象在那个模块 __class__ 表示当前操作的对象的类是什么 test.py 12345# -*- coding:utf-8 -*-class Person(object): def __init__(self): self.name = 'laowang' main.py 12345from test import Personobj = Person()print(obj.__module__) # 输出 test 即：输出模块print(obj.__class__) # 输出 test.Person 即：输出类 __init__ 初始化方法，通过类创建对象时，自动触发执行 1234567class Person: def __init__(self, name): self.name = name self.age = 18obj = Person('laowang') # 自动执行类中的 __init__ 方法 __del__ 当对象在内存中被释放时，自动触发执行 注：此方法一般无须定义，因为Python是一门高级语言，程序员在使用时无需关心内存的分配和释放，因为此工作都是交给Python解释器来执行，所以，__del__的调用是由解释器在进行垃圾回收时自动触发执行的。 123class Foo: def __del__(self): pass __call__ 对象后面加括号，触发执行 注：__init__方法的执行是由创建对象触发的，即：对象 = 类名() ；而对于__call__方法的执行是由对象后加括号触发的，即：对象()或者类()() 12345678910class Foo: def __init__(self): pass def __call__(self, *args, **kwargs): print('__call__')obj = Foo() # 执行 __init__obj() # 执行 __call__ __dict__ 类或对象中的所有属性 类的实例属性属于对象；类中的类属性和方法等属于类，即： 1234567891011121314151617181920212223class Province(object): country = 'China' def __init__(self, name, count): self.name = name self.count = count def func(self, *args, **kwargs): print('func')# 获取类的属性，即：类属性、方法、print(Province.__dict__)# 输出：&#123;'__dict__': &lt;attribute '__dict__' of 'Province' objects&gt;, '__module__': '__main__', 'country': 'China', '__doc__': None, '__weakref__': &lt;attribute '__weakref__' of 'Province' objects&gt;, 'func': &lt;function Province.func at 0x101897950&gt;, '__init__': &lt;function Province.__init__ at 0x1018978c8&gt;&#125;obj1 = Province('山东', 10000)print(obj1.__dict__)# 获取 对象obj1 的属性# 输出：&#123;'count': 10000, 'name': '山东'&#125;obj2 = Province('山西', 20000)print(obj2.__dict__)# 获取 对象obj1 的属性# 输出：&#123;'count': 20000, 'name': '山西'&#125; __str__ 如果一个类中定义了__str__方法，那么在打印对象时，默认输出该方法的返回值 12345678class Foo: def __str__(self): return 'laowang'obj = Foo()print(obj)# 输出：laowang __getitem__、__setitem__、__delitem__ 用于索引操作，如字典。以上分别表示获取、设置、删除数据: 12345678910111213141516171819# -*- coding:utf-8 -*-class Foo(object): def __getitem__(self, key): print('__getitem__', key) def __setitem__(self, key, value): print('__setitem__', key, value) def __delitem__(self, key): print('__delitem__', key)obj = Foo()result = obj['k1'] # 自动触发执行 __getitem__obj['k2'] = 'laowang' # 自动触发执行 __setitem__del obj['k1'] # 自动触发执行 __delitem__ __getslice__、__setslice__、__delslice__ 该三个方法用于切片操作，如：列表 123456789101112131415161718# -*- coding:utf-8 -*-class Foo(object): def __getslice__(self, i, j): print('__getslice__', i, j) def __setslice__(self, i, j, sequence): print('__setslice__', i, j) def __delslice__(self, i, j): print('__delslice__', i, j)obj = Foo()obj[-1:1] # 自动触发执行 __getslice__obj[0:1] = [11,22,33,44] # 自动触发执行 __setslice__del obj[0:2] # 自动触发执行 __delslice__]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python之静态方法与类方法]]></title>
    <url>%2F2018%2F10%2F29%2FPython%E4%B9%8B%E9%9D%99%E6%80%81%E6%96%B9%E6%B3%95%E4%B8%8E%E7%B1%BB%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[我们都知道Python语言是一门动态语言，根据类创建的实例可以任意绑定属性。给实例绑定属性的方法是通过实例变量，或者通过self变量。但是，如果是类本身需要绑定一个属性呢？可以直接在class中定义属性，这种属性是类属性，归对应的类所有。今天我们就来看一下Python中的类属性、实例属性以及实例方法、静态方法和类方法。 类属性、实例属性类属性、实例属性在定义和使用中有所区别，而最本质的区别是内存中保存的位置不同 实例属性属于对象 类属性属于类 123456789101112131415class Province(object): # 类属性 country = '中国' def __init__(self, name): # 实例属性 self.name = name# 创建一个实例对象obj = Province('山东省')# 直接访问实例属性print(obj.name)# 直接访问类属性Province.country 由上述代码可以看出【实例属性需要通过对象来访问】【类属性通过类访问】，在使用上可以看出实例属性和类属性的归属是不同的. 其在内存中的存储方式类似如下图： 由上图可以看出： 类属性在内存中只保存一份 实例属性在每个对象中都要保存一份 应用场景： 通过类创建实例对象时，如果每个对象需要具有相同名字的属性，那么就使用类属性，用一份既可 实例方法、静态方法和类方法方法包括：实例方法、静态方法和类方法，三种方法在内存中都归属于类，区别在于调用方式不同 实例方法：由对象调用；至少一个self参数；执行实例方法时，自动将调用该方法的对象赋值给self； 类方法：由类调用； 至少一个cls参数；执行类方法时，自动将调用该方法的类赋值给cls； 静态方法：由类调用；无默认参数 123456789101112131415161718192021222324252627282930class Foo(object): def __init__(self, name): self.name = name def ord_func(self): """ 定义实例方法，至少有一个self参数 """ # print(self.name) print('实例方法') @classmethod def class_func(cls): """ 定义类方法，至少有一个cls参数 """ print('类方法') @staticmethod def static_func(): """ 定义静态方法 ，无默认参数""" print('静态方法')f = Foo("中国")# 调用实例方法f.ord_func()# 调用类方法Foo.class_func()# 调用静态方法Foo.static_func() 对比： 相同点：对于所有的方法而言，均属于类，所以 在内存中也只保存一份 不同点：方法调用者不同、调用方法时自动传入的参数不同]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>OOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python之浅拷贝与深拷贝]]></title>
    <url>%2F2018%2F10%2F29%2FPython%E4%B9%8B%E6%B5%85%E6%8B%B7%E8%B4%9D%E4%B8%8E%E6%B7%B1%E6%8B%B7%E8%B4%9D%2F</url>
    <content type="text"><![CDATA[在Python中，对象的赋值，拷贝(深/浅拷贝)之间是有差异的，如果使用的时候不注意，就可能产生意外的结果。下面就让我们一起来看看它们之间具体存在的差异吧！ 浅拷贝 浅拷贝是对于一个对象的顶层拷贝 通俗的理解是：拷贝了引用，并没有拷贝内容 深拷贝 深拷贝是对于一个对象所有层次的拷贝(递归) 进一步理解深拷贝 拷贝的其他方式 分片表达式可以赋值一个序列 字典的copy方法可以拷贝一个字典 注意点浅拷贝对不可变类型和可变类型的copy不同 copy.copy对于可变类型，会进行浅拷贝* copy.copy对于不可变类型，不会拷贝，仅仅是指向 12345678910111213141516171819In [88]: a = [11,22,33]In [89]: b = copy.copy(a)In [90]: id(a)Out[90]: 59275144In [91]: id(b)Out[91]: 59525600In [92]: a.append(44)In [93]: aOut[93]: [11, 22, 33, 44]In [94]: bOut[94]: [11, 22, 33]In [95]: a = (11,22,33)In [96]: b = copy.copy(a)In [97]: id(a)Out[97]: 58890680In [98]: id(b)Out[98]: 58890680 copy.copy和copy.deepcopy的区别 copy.copy() copy.deepcopy()]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python高级特性之生成器]]></title>
    <url>%2F2018%2F10%2F28%2FPython%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7%E4%B9%8B%E7%94%9F%E6%88%90%E5%99%A8%2F</url>
    <content type="text"><![CDATA[利用迭代器，我们可以在每次迭代获取数据（通过next()方法）时按照特定的规律进行生成。但是我们在实现一个迭代器时，关于当前迭代到的状态需要我们自己记录，进而才能根据当前状态生成下一个数据。为了达到记录当前状态，并配合next()函数进行迭代使用，我们可以采用更简便的语法，即生成器(generator)。生成器是一类特殊的迭代器。下面就让我们一起来看看创建生成器的两种方法。 创建生成器方法1要创建一个生成器，有很多种方法。第一种方法很简单，只要把一个列表生成式的[]改成() 123456789In [15]: L = [ x*2 for x in range(5)]In [16]: LOut[16]: [0, 2, 4, 6, 8]In [17]: G = ( x*2 for x in range(5))In [18]: GOut[18]: &lt;generator object &lt;genexpr&gt; at 0x7f626c132db0&gt; 创建L和G的区别仅在于最外层的[]和()，L是一个列表，而G是一个生成器。我们可以直接打印出列表L的每一个元素，而对于生成器G，我们可以按照迭代器的使用方法来使用，即可以通过next()函数、for循环、list()等方法使用。 123456789101112131415161718192021222324In [19]: next(G)Out[19]: 0In [20]: next(G)Out[20]: 2In [21]: next(G)Out[21]: 4In [22]: next(G)Out[22]: 6In [23]: next(G)Out[23]: 8In [24]: next(G)---------------------------------------------------------------------------StopIteration Traceback (most recent call last)&lt;ipython-input-24-380e167d6934&gt; in &lt;module&gt;()----&gt; 1 next(G)StopIteration:In [25]: 123456789101112In [26]: G = ( x*2 for x in range(5))In [27]: for x in G: ....: print(x) ....: 02468In [28]: 创建生成器方法2generator非常强大。如果推算的算法比较复杂，用类似列表生成式的for循环无法实现的时候，还可以用函数来实现。 我们仍然用上篇博文提到的斐波那契数列来举例，回想我们在上一节用迭代器的实现方式： 123456789101112131415161718192021222324252627class FibIterator(object): """斐波那契数列迭代器""" def __init__(self, n): """ :param n: int, 指明生成数列的前n个数 """ self.n = n # current用来保存当前生成到数列中的第几个数了 self.current = 0 # num1用来保存前前一个数，初始值为数列中的第一个数0 self.num1 = 0 # num2用来保存前一个数，初始值为数列中的第二个数1 self.num2 = 1 def __next__(self): """被next()函数调用来获取下一个数""" if self.current &lt; self.n: num = self.num1 self.num1, self.num2 = self.num2, self.num1+self.num2 self.current += 1 return num else: raise StopIteration def __iter__(self): """迭代器的__iter__返回自身即可""" return self 注意，在用迭代器实现的方式中，我们要借助几个变量(n、current、num1、num2)来保存迭代的状态。现在我们用生成器来实现一下。 1234567891011121314151617181920212223242526272829303132333435In [30]: def fib(n): ....: current = 0 ....: num1, num2 = 0, 1 ....: while current &lt; n: ....: num = num1 ....: num1, num2 = num2, num1+num2 ....: current += 1 ....: yield num ....: return 'done' ....:In [31]: F = fib(5)In [32]: next(F)Out[32]: 1In [33]: next(F)Out[33]: 1In [34]: next(F)Out[34]: 2In [35]: next(F)Out[35]: 3In [36]: next(F)Out[36]: 5In [37]: next(F)---------------------------------------------------------------------------StopIteration Traceback (most recent call last)&lt;ipython-input-37-8c2b02b4361a&gt; in &lt;module&gt;()----&gt; 1 next(F)StopIteration: done 在使用生成器实现的方式中，我们将原本在迭代器__next__方法中实现的基本逻辑放到一个函数中来实现，但是将每次迭代返回数值的return换成了yield，此时新定义的函数便不再是函数，而是一个生成器了。简单来说：只要在def中有yield关键字的就称为生成器 此时按照调用函数的方式(案例中为F=fib(5))使用生成器就不再是执行函数体了，而是会返回一个生成器对象（案例中为F），然后就可以按照使用迭代器的方式来使用生成器了 12345678910In [38]: for n in fib(5): ....: print(n) ....: 11235In [39]: 但是用for循环调用generator时，发现拿不到generator的return语句的返回值。如果想要拿到返回值，必须捕获StopIteration错误，返回值包含在StopIteration的value中: 123456789101112131415161718In [39]: g = fib(5)In [40]: while True: ....: try: ....: x = next(g) ....: print("value:%d"%x) ....: except StopIteration as e: ....: print("生成器返回值:%s"%e.value) ....: break ....: value:1value:1value:2value:3value:5生成器返回值:doneIn [41]: 总结 使用了yield关键字的函数不再是函数，而是生成器。(使用了yield的函数就是生成器) yield关键字有两点作用: 保存当前运行状态（断点），然后暂停执行，即将生成器（函数）挂起 将yield关键字后面表达式的值作为返回值返回，此时可以理解为起到了return的作用 可以使用next()函数让生成器从断点处继续执行，即唤醒生成器（函数） Python3中的生成器可以使用return返回最终运行的返回值，而Python2中的生成器不允许使用return返回一个返回值（即可以使用return从生成器中退出，但return后不能有任何表达式）]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python高级特性之迭代器]]></title>
    <url>%2F2018%2F10%2F28%2FPython%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7%E4%B9%8B%E8%BF%AD%E4%BB%A3%E5%99%A8%2F</url>
    <content type="text"><![CDATA[迭代是访问集合元素的一种方式。迭代器是一个可以记住遍历的位置的对象。迭代器对象从集合的第一个元素开始访问，直到所有的元素被访问完结束。迭代器只能往前不会后退。 可迭代对象我们已经知道可以对list、tuple、str等类型的数据使用for ... in ...的循环语法从其中依次拿到数据进行使用，我们把这样的过程称为遍历，也叫迭代。 但是，是否所有的数据类型都可以放到for ... in ...的语句中，然后让for ... in ...每次从中取出一条数据供我们使用，即供我们迭代吗？ 12345678910111213141516171819202122232425262728&gt;&gt;&gt; for i in 100:... print(i)...Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: 'int' object is not iterable&gt;&gt;&gt;# int整型不是iterable，即int整型不是可以迭代的# 我们自定义一个容器MyList用来存放数据，可以通过add方法向其中添加数据&gt;&gt;&gt; class MyList(object):... def __init__(self):... self.container = []... def add(self, item):... self.container.append(item)...&gt;&gt;&gt; mylist = MyList()&gt;&gt;&gt; mylist.add(1)&gt;&gt;&gt; mylist.add(2)&gt;&gt;&gt; mylist.add(3)&gt;&gt;&gt; for num in mylist:... print(num)...Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: 'MyList' object is not iterable&gt;&gt;&gt;# MyList容器的对象也是不能迭代的 我们自定义了一个容器类型MyList，在将一个存放了多个数据的MyList对象放到for ... in ...的语句中，发现for ... in ...并不能从中依次取出一条数据返回给我们，也就说我们随便封装了一个可以存放多条数据的类型却并不能被迭代使用。 我们把可以通过for ... in ...这类语句迭代读取一条数据供我们使用的对象称之为可迭代对象(Iterable) 如何判断一个对象是否可迭代可以使用isinstance()判断一个对象是否是Iterable对象： 12345678910111213141516In [50]: from collections import IterableIn [51]: isinstance([], Iterable)Out[51]: TrueIn [52]: isinstance(&#123;&#125;, Iterable)Out[52]: TrueIn [53]: isinstance('abc', Iterable)Out[53]: TrueIn [54]: isinstance(mylist, Iterable)Out[54]: FalseIn [55]: isinstance(100, Iterable)Out[55]: False 可迭代对象的本质我们分析对可迭代对象进行迭代使用的过程，发现每迭代一次（即在for ... in ...中每循环一次）都会返回对象中的下一条数据，一直向后读取数据直到迭代了所有数据后结束。那么，在这个过程中就应该有一个“人”去记录每次访问到了第几条数据，以便每次迭代都可以返回下一条数据。我们把这个能帮助我们进行数据迭代的“人”称为迭代器(Iterator) 可迭代对象的本质就是可以向我们提供一个这样的中间“人”即迭代器帮助我们对其进行迭代遍历使用 可迭代对象通过__iter__方法向我们提供一个迭代器，我们在迭代一个可迭代对象的时候，实际上就是先获取该对象提供的一个迭代器，然后通过这个迭代器来依次获取对象中的每一个数据. 那么也就是说，一个具备了__iter__方法的对象，就是一个可迭代对象 12345678910111213141516&gt;&gt;&gt; class MyList(object):... def __init__(self):... self.container = []... def add(self, item):... self.container.append(item)... def __iter__(self):... """返回一个迭代器"""... # 我们暂时忽略如何构造一个迭代器对象... pass...&gt;&gt;&gt; mylist = MyList()&gt;&gt;&gt; from collections import Iterable&gt;&gt;&gt; isinstance(mylist, Iterable)True&gt;&gt;&gt;# 这回测试发现添加了__iter__方法的mylist对象已经是一个可迭代对象了 iter()函数与next()函数list、tuple等都是可迭代对象，我们可以通过iter()函数获取这些可迭代对象的迭代器。然后我们可以对获取到的迭代器不断使用next()函数来获取下一条数据。iter()函数实际上就是调用了可迭代对象的__iter__方法。 1234567891011121314151617&gt;&gt;&gt; li = [11, 22, 33, 44, 55]&gt;&gt;&gt; li_iter = iter(li)&gt;&gt;&gt; next(li_iter)11&gt;&gt;&gt; next(li_iter)22&gt;&gt;&gt; next(li_iter)33&gt;&gt;&gt; next(li_iter)44&gt;&gt;&gt; next(li_iter)55&gt;&gt;&gt; next(li_iter)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration&gt;&gt;&gt; 注意，当我们已经迭代完最后一个数据之后，再次调用next()函数会抛出StopIteration的异常，来告诉我们所有数据都已迭代完成，不用再执行next()函数了 如何判断一个对象是否是迭代器可以使用isinstance()判断一个对象是否是Iterator对象： 12345678910In [56]: from collections import IteratorIn [57]: isinstance([], Iterator)Out[57]: FalseIn [58]: isinstance(iter([]), Iterator)Out[58]: TrueIn [59]: isinstance(iter("abc"), Iterator)Out[59]: True 迭代器Iterator通过上面的分析，我们已经知道，迭代器是用来帮助我们记录每次迭代访问到的位置，当我们对迭代器使用next()函数的时候，迭代器会向我们返回它所记录位置的下一个位置的数据。实际上，在使用next()函数的时候，调用的就是迭代器对象的__next__方法（Python3中是对象的__next__方法，Python2中是对象的next()方法）。所以，我们要想构造一个迭代器，就要实现它的next方法。但这还不够，python要求迭代器本身也是可迭代的，所以我们还要为迭代器实现__iter__方法，而__iter__方法要返回一个迭代器，迭代器自身正是一个迭代器，所以迭代器的__iter__方法返回自身即可。 一个实现了iter方法和next方法的对象，就是迭代器。 1234567891011121314151617181920212223242526272829303132333435363738394041class MyList(object): """自定义的一个可迭代对象""" def __init__(self): self.items = [] def add(self, val): self.items.append(val) def __iter__(self): myiterator = MyIterator(self) return myiteratorclass MyIterator(object): """自定义的供上面可迭代对象使用的一个迭代器""" def __init__(self, mylist): self.mylist = mylist # current用来记录当前访问到的位置 self.current = 0 def __next__(self): if self.current &lt; len(self.mylist.items): item = self.mylist.items[self.current] self.current += 1 return item else: raise StopIteration def __iter__(self): return selfif __name__ == '__main__': mylist = MyList() mylist.add(1) mylist.add(2) mylist.add(3) mylist.add(4) mylist.add(5) for num in mylist: print(num) for … in … 的本质for item in Iterable循环的本质就是先通过iter()函数获取可迭代对象Iterable的迭代器，然后对获取到的迭代器不断调用next()方法来获取下一个值并将其赋值给item，当遇到StopIteration的异常后循环结束。 迭代器的应用场景我们发现迭代器最核心的功能就是可以通过next()函数的调用来返回下一个数据值。如果每次返回的数据值不是在一个已有的数据集合中读取的，而是通过程序按照一定的规律计算生成的，那么也就意味着可以不用再依赖一个已有的数据集合，也就是说不用再将所有要迭代的数据都一次性缓存下来供后续依次读取，这样可以节省大量的存储（内存）空间。 举个例子，比如，数学中有个著名的斐波拉契数列（Fibonacci），数列中第一个数为0，第二个数为1，其后的每一个数都可由前两个数相加得到： 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, … 现在我们想要通过for ... in ...循环来遍历迭代斐波那契数列中的前n个数。那么这个斐波那契数列我们就可以用迭代器来实现，每次迭代都通过数学计算来生成下一个数。 123456789101112131415161718192021222324252627282930313233class FibIterator(object): """斐波那契数列迭代器""" def __init__(self, n): """ :param n: int, 指明生成数列的前n个数 """ self.n = n # current用来保存当前生成到数列中的第几个数了 self.current = 0 # num1用来保存前前一个数，初始值为数列中的第一个数0 self.num1 = 0 # num2用来保存前一个数，初始值为数列中的第二个数1 self.num2 = 1 def __next__(self): """被next()函数调用来获取下一个数""" if self.current &lt; self.n: num = self.num1 self.num1, self.num2 = self.num2, self.num1+self.num2 self.current += 1 return num else: raise StopIteration def __iter__(self): """迭代器的__iter__返回自身即可""" return selfif __name__ == '__main__': fib = FibIterator(10) for num in fib: print(num, end=" ") 并不是只有for循环能接收可迭代对象除了for循环能接收可迭代对象，list、tuple等也能接收。 1234li = list(FibIterator(15))print(li)tp = tuple(FibIterator(6))print(tp)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python单例模式]]></title>
    <url>%2F2018%2F10%2F27%2FPython%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[目标 单例设计模式 __new__方法 Python中的单例 单例设计模式 设计模式 设计模式是前人工作的总结和提炼，通常，被人们广泛流传的设计模式都是针对某一特定问题的成熟的解决方案 使用设计模式是为了可重用代码、让代码更容易被他人理解、保证代码可靠性 单例设计模式 目的—- 让类创建的对象，在系统中只有唯一的一个实例 每一次执行类名()返回的对象，内存地址是相同的 单例设计模式的应用场景 音乐播放对象 回收站对象 打印机对象 …… __new__方法 使用类名()创建对象时，Python的解释器首先会调用__new__方法为对象分配空间 __new__是一个由object基类提供的内置的静态方法，主要作用有两个： 在内存中为对象分配空间 返回对象的引用 Python的解释器获得对象的引用后，将引用作为第一个参数，传递给__init__方法 重写__new__方法的代码非常固定 重写__new__方法一定要return super().__new__(cls) 否则Python的解释器得不到分配了空间的对象引用，就不会调用对象的初始化方法 注意：__new__是一个静态方法，在调用时需要主动传递cls参数 示例代码： 123456789101112class MusicPlayer(object): def __new__(cls, *args, **kwargs): # 如果不返回任何结果， return super().__new__(cls) def __init__(self): print("初始化音乐播放对象")player = MusicPlayer()print(player) Python中的单例 单例 — 让类创建的对象，在系统中只有唯一的一个实例 定义一个类属性，初始值是None, 用于记录单例对象的引用 重写__new__方法 如果类属性 is None, 调用父类方法分配空间，并在类属性中记录结果 返回类属性中记录的对象引用 12345678910111213class MusicPlayer(object): # 定义类属性记录单例对象引用 instance = None def __new__(cls, *args, **kwargs): # 1. 判断类属性是否已经被赋值 if cls.instance is None: cls.instance = super().__new__(cls) # 2. 返回类属性的单例引用 return cls.instance 只执行一次初始化操作 在每次使用类名()创建对象时，Python的解释器都会自动调用两个方法： __new__分配空间 __init__对象初始化 在上一小节对__new__方法改造之后，每次都会得到第一次被创建对象的引用 但是初始化方法还是会被再次调用 需求 让初始化动作只被执行一次 解决办法 定义一个类属性init_flag, 标记是否执行过初始化动作，初始值为False 在__init__方法中，判断init_flag, 如果为False就执行初始化动作 然后将init_flag设置为True 这样，再次自动调用__init__方法时，初始化动作就不会被再次执行了 12345678910111213141516171819202122232425262728293031class MusicPlayer(object): # 记录第一个被创建对象的引用 instance = None # 记录是否执行过初始化动作 init_flag = False def __new__(cls, *args, **kwargs): # 1. 判断类属性是否是空对象 if cls.instance is None: # 2. 调用父类的方法，为第一个对象分配空间 cls.instance = super().__new__(cls) # 3. 返回类属性保存的对象引用 return cls.instance def __init__(self): if not MusicPlayer.init_flag: print("初始化音乐播放器") MusicPlayer.init_flag = True# 创建多个对象player1 = MusicPlayer()print(player1)player2 = MusicPlayer()print(player2)]]></content>
      <categories>
        <category>Python</category>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Design Pattern</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令之其他]]></title>
    <url>%2F2018%2F10%2F21%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E4%B9%8B%E5%85%B6%E4%BB%96%2F</url>
    <content type="text"><![CDATA[在前面的几篇博文中，已经陆陆续续的讲了Linux操作系统的目录与文件、远程管理、用户权限、系统信息相关的基本终端命令，作为本系列的最后一篇博文，我们来讲一下其他的常用的Linux终端命令。 它们分别是查找文件命令find、软链接命令ln、打包和压缩命令tar以及软禁安装命令(此处特指Ubuntu系统)apt-get 下面我们就来具体看看这几个命令你给的用法。 查找文件 find命令功能非常强大，通常用来在特定的目录下搜索符合条件的文件 序号 命令 作用 01 find [路径] -name “*.py” 查找指定路径下扩展名是 .py 的文件，包括子目录 如果省略路径，表示在当前文件夹下查找 之前学习的通配符，在使用find命令一样可以使用 有关find的高级使用，后续在更新…… 12345find -name "*1*"# 搜索桌面目录下，文件名包含 1 的文件find -name "*.txt"# 搜索桌面目录下，所有以 .txt 为扩展名的文件 软链接 序号 命令 作用 01 ln -s 被链接的源文件 链接文件 建立文件的软链接，用通俗的方式讲类似于Windows下的快捷方式 注意： 没有-s选项建立的是一个硬链接文件 两个文件占用相同大小的硬盘空间，工作中几乎不会建立文件的硬链接 源文件要使用绝对路径，不能使用相对路径，这样可以方便移动链接文件后，仍然能够正常使用 硬链接简介(知道) 在使用ln创建链接时，如果没有-s选项，会创建一个硬链接，而不是软链接 文件软硬链接的示意图 在Linux中，文件名和文件的数据是分开存储的 提示： 在Linux中，只有文件的硬链接数 == 0才会被删除 使用ls -l可以查看一个文件的硬链接的数量(硬链接——有多少种方式可以访问文件或者目录) 在日常工作中，几乎不会建立文件的硬链接，知道即可 打包压缩 打包压缩是日常工作中备份文件的一种方式 在不同的操作系统中，常用的打包压缩方式是不同的： Windows常用rar Mac常用zip Linux常用tar.gz 打包 / 解包 tar是Linux中最常用的备份工具，此命令可以把一系列文件打包到一个大文件中，也可以把一个打包的大文件恢复成一系列文件 tar的命令格式如下： 12345# 打包文件tar -cvf 打包文件.tar 被打包的文件／路径...# 解包文件tar -xvf 打包文件.tar tar选项说明 选项 含义 c 生成档案文件，创建打包文件 v 解开档案文件 x 列出归档解档的详细过程，显示进度 f 指定档案文件名称，f后面一定是 .tar 文件，所以必须放选项最后 注意：f选项必须放在最后，其他选项顺序可以随意 压缩 / 解压缩gzip tar与gzip命令结合可以使用实现文件打包和压缩 tar只负责打包文件，但不压缩 用gzip压缩tar打包后的文件，其扩展名一般用xxx.tar.gz 在Linux中，最常见的压缩文件格式就是xxx.tar.gz 在tar命令中有一个选项-z可以调用gzip，从而可以方便的实现压缩和解压缩的功能 命令格式如下: 12345678# 压缩文件tar -zcvf 打包文件.tar.gz 被压缩的文件／路径...# 解压缩文件tar -zxvf 打包文件.tar.gz# 解压缩到指定路径tar -zxvf 打包文件.tar.gz -C 目标路径 选项 含义 -C 解压缩到指定目录，注意：要解压缩的目录必须存在 bzip2 tar与bzip2命令结合可以使用实现文件打包和压缩(用法和gzip一样) tar只负责打包文件，但不压缩 用bzip2压缩tar打包后的文件，其扩展名一般用xxx.tar.bz2 在tar命令中有一个选项-j可以调用bzip2，从而可以方便的实现压缩和解压缩的功能 命令格式如下: 12345# 压缩文件tar -jcvf 打包文件.tar.bz2 被压缩的文件／路径...# 解压缩文件tar -jxvf 打包文件.tar.bz2 软件安装通过 apt 安装／卸载软件 apt是 Advanced Packaging Tool，是Linux下的一款安装包管理工具 可以在终端中方便的安装／卸载／更新软件包 12345678# 1. 安装软件$ sudo apt install 软件包# 2. 卸载软件$ sudo apt remove 软件名# 3. 更新已安装的包$ sudo apt upgrade 配置软件源 如果希望在ubuntu中安装软件，更加快速，可以通过设置镜像源，选择一个访问网速更快的服务器，来提供软件下载／安装服务 提示：更换服务器之后，需要一个相对比较长时间的更新过程，需要耐心等待。更新完成后，再安装软件都会从新设置的服务器下载软件了 所谓镜像源，就是所有服务器的内容是相同的(镜像)，但是根据所在位置不同，国内服务器通常速度会更快一些！]]></content>
      <categories>
        <category>Linux操作系统</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令之系统信息相关]]></title>
    <url>%2F2018%2F10%2F21%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E4%B9%8B%E7%B3%BB%E7%BB%9F%E4%BF%A1%E6%81%AF%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[这篇博文主要是讲系统信息相关的命令。主要是为了通过远程终端维护服务器时，查看服务器上当前系统日期和时间 / 磁盘空间占用情况 / 程序执行情况 本文所提到的终端命令都是查询命令，通过这些命令对系统资源的使用情况有个了解 时间和日期 序号 命令 作用 01 date 查看系统时间 02 cal calendar查看日历，-y选项可以查看一年的日历 磁盘信息 序号 命令 作用 01 df -h disk free 显示磁盘剩余空间 02 du -h [目录名] disk usage 显示目录下的文件大小 选项说明： 参数 含义 -h 以人性化的方式显示文件大小 进程信息 所谓进程，通俗地说就是当前正在执行的一个程序 序号 命令 作用 01 ps aux process status 查看进程的详细状况 02 top 动态显示运行中的进程并且排序 03 kill [-9] 进程代号 终止指定代号的进程，-9表示强行终止 ps默认只会显示当前用户通过终端启动的应用程序 ps选项功能说明 选项 含义 a 显示终端上的所有进程，包括其他用户的进程 u 显示进程的详细状态 x 显示没有控制终端的进程 提示： 使用kill命令时，最好只终止由当前用户开启的进程，而不要终止root身份开启的进程，否则可能导致系统崩溃 要退出top可以直接输入q]]></content>
      <categories>
        <category>Linux操作系统</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令之用户权限]]></title>
    <url>%2F2018%2F10%2F20%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E4%B9%8B%E7%94%A8%E6%88%B7%E6%9D%83%E9%99%90%2F</url>
    <content type="text"><![CDATA[之前的一篇文章讲到了Linux操作系统的远程管理相关的终端命令，这篇文章我们来谈谈与用户权限有关的终端命令，毕竟我们也是经常与权限打交道。主要包含以下三块： 用户和权限的基本概念 用户管理终端命令 组管理终端命令 修改权限终端命令(有一篇单独的博文具体讲过了，这里再提一下) 用户和权限的基本概念基本概念 用户是Linux系统工作中重要的一环，用户管理包括用户与组管理 在Linux系统中，不论是由本机或是远程登录系统，每个系统都必须拥有一个账号，并且对于不同的系统资源拥有不同的使用权限 在Linux中，可以指定每一个用户针对不同的文件或者目录的不同权限 对文件/目录的权限包括： 序号 权限 英文 缩写 数字代号 01 读 read r 4 02 写 write w 2 03 执行 execute x 1 组 为了方便用户的管理，提出了组的概念，如下图所示： 在实际应用中，可以预先针对组设置好权限，然后将不同的用户添加到对应的组中，从而不用依次为每一个用户设置权限 ls -l扩展 ls -l可以查看文件夹下文件的详细信息，从左到右依次是: 权限，第 1 个字符如果是d表示目录,如果是-表示文件 硬链接数，通俗地讲，就是有多少种方式，可以访问到当前目录／文件 拥有者，家目录下文件／目录的拥有者通常都是当前用户 组，在Linux中，很多时候，会出现组名和用户名相同的情况，后续会讲 大小 时间 名称 chmod简单使用（重要） chmod可以修改用户／组对文件／目录的权限 命令格式如下：1chmod +/-rwx 文件名|目录名 超级用户 Linux系统中的root账号通常用于系统的维护和管理，对操作系统的所有资源具有所有访问权限 在大多数版本的Linux中，都不推荐直接使用root账号登录系统 在Linux安装的过程中，系统会自动创建一个用户账号，而这个默认的用户就称为“标准用户” sudo su是substitute user的缩写，表示使用另一个用户的身份 sudo命令用来以其他身份来执行命令，预设的身份为root 用户使用sudo时，必须先输入密码，之后有5分钟的有效期限，超过期限则必须重新输入密码 若其未经授权的用户企图使用sudo，则会发出警告邮件给管理员 组管理终端命令 提示：创建组/删除组的终端命令都需要通过sudo执行 序号 命令 作用 01 groupadd 组名 添加组 02 groupdel 组名 删除组 03 cat/etc/group 确认组信息 04 chgrp -R 组名 文件/目录名 递归修改文件/目录的所属组 提示： 组信息保存在/etc/group文件中 etc目录是专门用来保存系统配置信息的目录 在实际应用中，可以预先针对组设置好权限，然后将不同的用户添加到对应的组中，从而不用依次为每一个用户设置权限 用户管理终端命令 提示：创建用户/删除用户/修改其他用户密码的终端命令都需要通过sudo执行 创建用户/设置密码/删除用户 序号 命令 作用 说明 01 useradd -m -g 组 新建用户名 添加新用户 -m自动建立用户家目录-g指定用户所在的组，否则会建立一个和同名的组 02 passwd 用户名 设置用户密码 如果是普通用户，直接用passwd可以修改自己的账户密码 03 userdel -r 用户名 删除用户 -r选项会自动删除用户家目录 04 cat /etc/passwd 确认用户信息 新建用户后，用户信息会保存在 /etc/passwd文件中 提示： 创建用户时，如果忘记添加-m选项指定新用户的家目录—最简单的方法就是删除用户，重新创建 创建用户时，默认会创建一个和用户名同名的组名 用户信息保存在/etc/passwd文件中 查看用户信息 序号 命令 作用 01 id[用户名] 查看用户 UID 和 GID 信息 02 who 查看当前所有登录的用户列表 03 whoami 查看当前登录用户的账户名 password文件etc/passwd文件存放的是用户的信息，由6个分号组成的7个信息，分别是： 用户名 密码(x，表示加密的密码) UID(用户标识) GID(组标识) 用户全名或本地账号 家目录 登录使用的Shell，就是登录之后，使用的终端命令，ubuntu默认是dash usermod usermod可以用来设置用户的主组／附加组和登录 Shell，命令格式如下: 主组: 通常在新建用户时指定，在etc/passwd的第4列GID对应的组 附加组: 在etc/group中最后一列表示该组的用户列表，用于指定用户的附加权限 提示：设置了用户的附加组之后，需要更新登陆才能生效。 12345678# 修改用户的主组（passwd 中的 GID）usermod -g 组 用户名# 修改用户的附加组usermod -G 组 用户名# 修改用户登录 Shellusermod -s /bin/bash 用户名 注意：默认使用useradd添加的用户是没有权限使用sudo以root身份执行命令的，可以使用以下命令，将用户添加到sudo附加组中 1usermod -G sudo 用户名 which(重要) 提示： /etc/passwd是用于保存用户信息的文件 /usr/bin/passwd是用于修改用户密码的程序 which命令可以查看执行命令所在的位置，例如： 123456789which ls# 输出# /bin/lswhich useradd# 输出# /usr/sbin/useradd bin和sbin 在Linux中，绝大多数可执行文件都是保存在/bin、/sbin、/usr/bin、/usr/sbin /bin（binary）是二进制执行文件目录，主要用于具体应用 /sbin（system binary）是系统管理员专用的二进制代码存放目录，主要用于系统管理 /usr/bin（user commands for applications）后期安装的一些软件 /usr/sbin（super user commands for applications）超级用户的一些管理程序 提示： cd这个终端命令是内置在系统内核中的，没有独立的文件，因此用which无法找到 cd命令的位置 切换用户 序号 命令 作用 说明 01 su - 用户名 切换用户，并且切换目录 -可以切换到用户家目录，否则保持位置不变 02 exit 退出当前登录账户 su不接用户名，可以切换到root,但是不推荐使用，因为不安全 exit示意图如下： 修改文件权限 序号 命令 作用 01 chown 修改拥有者 02 chgrp 修改组 03 chmod 修改权限 命令格式如下： 12345678# 修改文件|目录的拥有者chown 用户名 文件名|目录名# 递归修改文件|目录的组chgrp -R 组名 文件名|目录名# 递归修改文件权限chmod -R 755 文件名|目录名 chmod在设置权限时，可以简单地使用三个数字分别对应拥有者/组和其他用户的权限 12# 直接修改文件|目录的 读|写|执行 权限，但是不能精确到 拥有者|组|其他chmod +/-rwx 文件名|目录名 常见的数字组合有(u表示用户／g表示组／o表示其他)： 777 ===&gt; u=rwx,g=rwx,o=rwx 755 ===&gt; u=rwx,g=rx,o=rx 644 ===&gt; u=rw,g=r,o=r]]></content>
      <categories>
        <category>Linux操作系统</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令之远程管理]]></title>
    <url>%2F2018%2F10%2F20%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E4%B9%8B%E8%BF%9C%E7%A8%8B%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[之前的一篇文章讲到了Linux操作系统的与目录和文件相关的终端命令，目录和文件相关的命令也是我们用的最多的命令。这篇文章我们来谈谈与远程管理有关的终端命令，主要包含以下三块： 关机/重启： shutdown 查看或者配置网卡信息： ifconfig ping 远程登录和复制文件： ssh scp 下面我们就来具体来看看这几个命令。 shutdownshutdown命令可以安全关闭或者重新启动系统。 常见的用法为： 1~/Documents$ shutdown 选项 时间 注意： 不指定选项和参数，默认表示1分钟之后关闭电脑。 远程维护服务器时，最好不要关闭系统，而应该重新启动系统。 12~/Documents$ shutdown -r# -r表示重新启动系统 常用命令示例： 1234567891011121314# 重新启动操作系统，其中 now 表示现在$ shutdown -r now# 立刻关机，其中 now 表示现在$ shutdown now# 系统在今天的 20:25 会关机$ shutdown 20:25# 系统再过十分钟后自动关机$ shutdown +10# 取消之前指定的关机计划$ shutdown -c ifconfigifconfig可以查看/配置计算机当前的网卡配置信息 12345# 查看网卡配置信息$ ifconfig# 查看网卡对应的 IP 地址$ ifconfig | grep inet 提示： 一台计算机中有可能会有一个物理网卡和多个虚拟网卡，在 Linux中物理网卡的名字通常以ensXX表示. 127.0.0.1被称为本地回环/环回地址，一般用来测试本机网卡是否正常 pingping一般用于检测当前计算机到目标计算机之间的网络是否通畅，数值越大，速度越慢 12345# 检测到目标主机是否连接正常$ ping IP地址# 检测本地网卡工作正常$ ping 127.0.0.1 Notes: ping的工作原理与潜水艇的声纳相似，ping这个命令就是取自声纳的声音 网络管理员之间也常将ping用作动词——ping一下计算机X，看他是否开着 ping的原理：网络上的机器都有唯一确定的IP地址，我们给目标IP地址**发送一个数据包，对方就要返回一个数据包，根据返回的数据包以及时间，我们可以确定目标主机的存在 ssh基础(重点)在Linux中SSH是非常常用的工具，通过SSH客户端我们可以连接到运行了SSH服务器的远程机器上. SSH客户端是一种使用Secure Shell(SSH)协议连接到远程计算机的软件程序 SSH是目前较可靠，专为远程登录会话和其他网络服务提供安全性的协议 利用SSH协议可以有效防止远程管理过程中的信息泄露 通过SSH协议可以对所有传输的数据进行加密，也能够防止DNS欺骗和IP欺骗 SSH的另一项优点是传输的数据可以是经过压缩的，所以可以加快传输的速度 1) 域名和端口号 域名: 由一串用点分隔的名字组成，例如：www.baidu.com 是IP地址的别名，方便用户记忆 端口号: IP地址: 通过IP地址找到网络上的计算机 端口号: 通过端口号可以找到计算机上运行的应用程序 SSH服务器的默认端口号是22,如果是默认端口号，在连接的时候，可以忽略 常见服务器端口号： 序号 服务 端口号 01 SSH服务器 22 02 Web服务器 80 03 HTTPS服务器 443 04 FTP服务器 21 1) SSH客户端的简单使用 1ssh [-p port] user@remote user是在远程机器上的用户名,如果不指定的话默认为当前用户 remote是远程机器的地址，可以是IP/域名，或者是后面会提到的别名 port是SSH Server监听的端口，如果不指定，就默认为22 使用exit退出当前用户的登录 注意： SSH这个终端命令只能在Linux或者UNIX系统下使用 如果在Windows系统中，可以安装PuTTY或者XShell客户端软件即可 scpscp就是secure copy，是一个在Linux下用来进行远程拷贝文件的命令它的地址格式与SSH基本相同，需要注意的是， 在指定端口时用的是大写的-P而不是小写的 12345678910111213# 把本地当前目录下的01.py文件复制到远程Home目录下的Desktop/01.py# 注意：`:` 后面的路径如果不是绝对路径，则以用户的家目录作为参照路径scp -P port 01.py user@remote:Desktop/01.py# 把远程Home目录下的Desktop/01.py文件复制到本地当前目录下的01.pyscp -P port user@remote:Desktop/01.py 01.py# 加上-r选项可以传送文件夹# 把当前目录下的demo文件夹复制到远程Home目录下的Desktopscp -r demo user@remote:Desktop# 把远程Home目录下的Desktop复制到当前目录下的demo文件夹scp -r user@remote:Desktop demo 选项 含义 -r 若给出的源文件是目录文件，则 scp 将递归复制该目录下的所有子目录和文件，目标文件必须为一个目录名 -P 若远程 SSH 服务器的端口不是 22，需要使用大写字母 -P 选项指定端口 注意： scp这个终端命令只能在Linux或者UNIX系统下使用 如果在Windows系统中，可以安装PuTTY, 使用pscp命令行工具或者安装 FileZilla使用FTP进行文件传输 FileZilla在传输文件时，使用的是FTP服务而不是SSH服务，因此端口号应该设置为 21 ssh高级(知道) 免密码登陆 配置别名 注意：有关SSH配置信息都保存在用户家目录下的.ssh目录下 1) 免密码登陆： 步骤： 配置公钥： 执行ssh-keygen即可生成SSH钥匙，一路回车即可 上传公钥到服务器： 执行ssh-copy-id -p port user@remote,可以让远程服务器记住我们的公钥 示意图： 非对称加密算法 使用公钥加密的数据，需要使用私钥解密 使用私钥加密的数据，需要使用公钥解密 2) 配置别名： 每次都输入ssh -p port user@remote，时间久了会觉得很麻烦，特别是当user, remote和port都得输入，而且还不好记忆 而配置别名可以让我们进一步偷懒，譬如用：ssh mac来替代上面这么一长串，那么就在 ~/.ssh/config里面追加以下内容：1234Host mac HostName ip地址 User itheima Port 22 保存之后，即可用ssh mac实现远程登录了，scp同样可以使用]]></content>
      <categories>
        <category>Linux操作系统</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Github如何同步更新一个你Fork的仓库？]]></title>
    <url>%2F2018%2F06%2F12%2FGithub-%E5%A6%82%E4%BD%95%E5%90%8C%E6%AD%A5%E6%9B%B4%E6%96%B0%E4%B8%80%E4%B8%AA%E4%BD%A0Fork%E7%9A%84%E4%BB%93%E5%BA%93%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[我们在进行Github协同开发的时候，往往会去fork一个仓库到自己的Github中，过一段时间以后，原仓库可能会有各种提交以及修改，很可惜，Github本身并没有自动进行同步的机制，这个需要我们手动去执行，现在我来演示一下如何进行自己的仓库和原仓库进行Gith同步的操作。重点在于：除了origin以外，增加一个upstream，来跟踪原仓库的更新。 先将你fork到自己的Github的远程仓库克隆到本地1$ git clone https://github.com/zmzhouXJTU/Problem-sets 查看远程仓库的信息123$ git remote -vorigin https://github.com/zmzhouXJTU/Problem-sets (fetch)origin https://github.com/zmzhouXJTU/Problem-sets (push) 配置原远程仓库(即你所fork的)的路径1$ git remote add upstream https://github.com/ACLoong/Problem-sets 再次查看远程仓库信息确定是否配置成功12345$ git remote -vorigin https://github.com/zmzhouXJTU/Problem-sets (fetch)origin https://github.com/zmzhouXJTU/Problem-sets (push)upstream https://github.com/ACLoong/Problem-sets (fetch)upstream https://github.com/ACLoong/Problem-sets (push) 抓取原仓库的更新文件，该文件会被存储在一个本地分支upstream/master上1234567$ git fetch upstreamremote: Counting objects: 21, done.remote: Compressing objects: 100% (15/15), done.remote: Total 21 (delta 4), reused 5 (delta 1), pack-reused 0Unpacking objects: 100% (21/21), done.From https://github.com/ACLoong/Problem-sets * [new branch] master -&gt; upstream/master 切换到本地主分支(如果当前不在的话)12$ git checkout master# Switched to branch 'master' 将原仓库的更新文件(即存储在本地的upstream/master分支上的文件)与本地仓库的当前分支合并123456$ git merge upstream/masterUpdating 60b537a..426148cFast-forward problem_set/first_week/refs/Process&amp;Thread.md | 29 +++++++++++++++++++++++++++ 1 file changed, 29 insertions(+) create mode 100644 problem_set/first_week/refs/Process&amp;Thread.md Note: 此时，你的本地仓库已经和原来仓库完全同步了。但是注意，此时只是你电脑上的本地仓库和原作者的远程github仓库同步了，你fork到自己的github远程仓库还没有同步。要想实现本地和自己的Github远程仓库的同步，只需要如下操作即可： 将其更新到自己的Github远程仓库，可以利用命令git push origin master来实现123456789$ git push origin masterCounting objects: 21, done.Delta compression using up to 4 threads.Compressing objects: 100% (16/16), done.Writing objects: 100% (21/21), 5.65 KiB | 1.88 MiB/s, done.Total 21 (delta 4), reused 0 (delta 0)remote: Resolving deltas: 100% (4/4), completed with 2 local objects.To https://github.com/zmzhouXJTU/Problem-sets 60b537a..426148c master -&gt; master 好了，至此任务就完成了。不知你是否看明白了呢？有任何问题，欢迎在此博客下方评论留言。]]></content>
      <categories>
        <category>版本控制之Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux文件权限]]></title>
    <url>%2F2018%2F04%2F26%2FLinux%E6%96%87%E4%BB%B6%E6%9D%83%E9%99%90%2F</url>
    <content type="text"><![CDATA[在 Linux 操作系统中，权限是一个非常重要的东西。 它无时无刻不在影响着你的各种操作。就像有的时候，你想去百度云下载一些别人分享的文件，可是你却发现虽然你能看到文件但是你却不能下载，这就是一种权限的典型例子。在 Linux 中, 这种权限随处可见。你可以设置，让别人不能越界。 不过像我这种人，也就是一台电脑， 一个用户(手动滑稽)，涉及不到多少有关权限方面的问题。 如果你也和我差不多，只有你自己在用 Linux 的电脑，主要用它来运行你的代码。我教你一招来修改权限，并且方便运行你的Python脚本的招数。 这里不会涉及过深的权限管理。我们了解一些基础，方便你拿你的Linux跑机器学习的代码就好。如果你想扩展学习的话, 网上会有很多教程。可以自行谷歌。 ls查看权限查看文件权限的方法很简单，其实在之前我的上一篇博文就讲了这个内容。 如果你还记得的话，我们在说 ls指令的时候, 提到过权限问题， 不过到了这节内容我们仔细的来说一说权限。如果你在 Terminal 中输入如下图片中的指令： 如上图所示，在 Terminal 中查看文件的权限的命令可以用ls -l 或者ls -lh，这个是看当前目录下面所有文件的权限，如果你只是想单独看看某一个具体的文件的权限，只需要在后面加上相应的文件名就可以了，比如ls -lh p1.py。 我们看到的上图中的-rw-rw-r--就是相关的权限啦。那么这些具体又是表达什么意思的呢？看下面的图就明白啦。 Type: 很多种 (最常见的是 - 为文件, d 为文件夹, 其他的还有l, n … 这种东西, 真正自己遇到了, 网上再搜就好, 一次性说太多记不住的)。 User: 后面跟着的三个空是使用 User 的身份能对这个做什么处理 (r 可读; w 可写; x 可执行; - 不能完成某个操作)。 Group: 一个 Group 里可能有一个或者多个 user, 这些权限的样式和 User 一样。 Others: 除了 User 和 Group 以外人的权限。 如果有人对 User, group, others 这三个没什么概念的话，我这里补充一下。User 一般就是指你，这个当前正在使用电脑的人。 Group 是一个 User 的集合，最开始创建新 User 的时候, 他也为这个 User 创建了一个和 User 一样名字的 Group, 这个新 Group 里只有这个 User。一般来说，像一个企业部门的电脑，都可以放在一个 Group 里, 分享了一些共享文件和权限。Others 就是除了上面提到的 User 和 Group 以外的人。 好了, 有了这些理解, 我们拿上面的 p1.py 来举例. 我们可以将 -rw-rw-r-- 拆成 - (这是文件), -rw(这个 User可以读,写), rw- (这个Group里可以读,写), r– (其他人只能读)。 chmod 修改权限好了, 我们知道了这些权限的问题, 那我们如何来改写权限呢? chmod (change mode)命令就是来干这个的。 通常的修改形式是： 1~$ chmod [谁] [怎么修改] [哪个文件] 举个最简单的例子, 现在的 p1.py 的权限是 -rw-rw-r--, 如果我们想让user和group拥有执行的能力。按照下面这样来改就行了。相应的结果如下图所示： 这里的 ug+x 很形象, User,Group + execute , 给 p1.py 这个文件进行修改。所以我们的修改形式就能总结出下面这样： [谁] u: 对于 User 修改 g: 对于 Group 修改 o: 对于 Others 修改 a: (all) 对于所有人修改 [怎么修改] +, -, =: 作用的形式, 加上, 减掉, 等于某些权限 r, w, x 或者多个权限的组合, 比如 rx [哪个文件] 施加操作的文件, 可以为多个文件 除了上面这些修改形式, 还有一些简化版的形式, 就是用数字来表示相应的权限，此时一组权限的 3 个位当做二进制数字的位，从左到右每个位的权值为 4、2、1，即每个权限对应的数字权值为 r : 4、w : 2、x : 1。 文件默认权限 文件默认权限：文件默认没有可执行权限，因此为666，也就是 -rw-rw-rw- 。 目录默认权限：目录必须要能够进入，也就是必须拥有可执行权限，因此为777 ，也就是 drwxrwxrwx。 一个使用Python的技巧我不怎么用权限这东西, 但是我却发现给 python 文件添加权限x 还算有用的. 为什么这么说? 因为通常, 如果一个 .py 没有x 权限, 在 terminal 中你就需要这样执行: 1~/Documents/Folder1$ python3 p1.py 如果你有了 x (可执行权限), 你运行这个文件可以直接这样打: 1~/Documents/Folder1$ ./p1.py 很酷炫有木有？但是上面这个脚本能够成功运行有一个前提，那就是你在这个Python脚本p1.py的开头加上一句话，如下所示： 12#!/usr/bin/python3print("Hello World") 好了，加上上面这句话的话以后运行这个脚本就可以直接用那本很酷炫的方式而不用每次都python3 xx.py啦。]]></content>
      <categories>
        <category>Linux操作系统</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令之目录和文件]]></title>
    <url>%2F2018%2F04%2F26%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E4%B9%8B%E7%9B%AE%E5%BD%95%E5%92%8C%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[作为一名程序员，相信大家对Linux操作系统都不会陌生。但是Linux系统那些繁杂的操作命令通常让我们很头疼，我也在网上查阅了很多资料，做了一些总结。这里将常用的一些Linux操作命令(主要是与文件相关的操作命令)整理如下： sudo这个SuperUserDo(简写成”sudo”)是Linux新手要使用的最重要的命令。需要根权限的每一个命令都需要这个sudo命令。你可以在需要根权限的每个命令之前使用sudo 1$ sudo su pwd12~/Documents$ pwd# 获取当前所在的路径(绝对路径) ls(list)借助ls(list)命令，终端就会显示你正在处理的那个目录里面的所有文件和文件夹。假设我在Home目录(也就是”~”)里面，想查看当前目录下的文件夹和文件。相应的命令如下图所示： 12~$ ls# 显示当前目录的所有文件和文件夹 上面是ls命令最基本的使用方式，下面我们再看看ls命令其他的使用方式。 12~$ ls -l# 输出详细信息 -l (long 的简写). 这个指令会打印出文件的权限 (-rw-rw-r-- 之后我们在细说这个), 用户名, 文件大小, 修改日期, 文件名 12~$ ls -a# -a (all 的简写) 显示所有文件 . 这里还会显示隐藏的文件 (以 . 开头的) 12~$ ls -lh# -lh (human), 直接 -l 不方便人看, 这个指令是为了方便给人观看的. 注意这里的文件大小使用了 K, MB, GB 之类概括 12~$ ls --help# 还有很多其他的功能, 我们可以通过 --help 来查看 cd更改目录(cd)是始终在终端中使用的主要命令。它是最基本的Linux命令之一。使用这个命令很简单。只要输入你想要从当前目录进入到的那个文件夹的名称(如果是多层，中间用”/“进行分隔)。如果想要返回上一级，只要将双圆点(..)作为参数。 假设我在Home目录(也就是”~”)中，想进入到在Home目录里面的Documents子目录。下面是我可以使用cd命令的方法： 12~$ cd Documents/# 跳转到对应的Documents目录 若果要想返回到上一级目录，只用将双圆点(..)作为相应的参数即可。如下图所示： 12~/Documents$ cd ..# 返回上一级目录 除了上面两个基本的命令之外，还有一些有趣的，我们来看看。 12~$ cd Documents/Folder1/# 去往子文件夹Folder1 12~/Documents/Folder1$ cd -# 返回你刚刚所在的目录(也就是你的上一条命令所在的目录) 12~/Documents/Folder1$ cd ../../# 向上返回两次(再往上返回可以依次类推) 12~/Documents/Folder1$ cd ~# 去往当前用户的主目录 touchtouch 命令的意思的新建，它的使用很简单。我们先去往 Documents 的文件夹, 里面已经有了 folder1 和 file1, 如果我们想新建一个 file2 使用下面的语句就好(可以指定文件的扩展名)。一个空文件就这样建立好了。(注意：如果创建的文件不存在，则会创建一个空白文件；如果文件已经存在，可以修改文件的末次修改日期) 12~/Documents$ touch file2.txt# 建立一个文件名为file2的空的文本文档 如果你想同时建立多个文件，输入多个文件的名字，以空格分开。 12~/Documents$ touch file3.txt file4.txt file5.txt# 同时建立三个空的文本文档(也可以不指定文件的扩展名) treetree 命令的意思的新建，可以以树状图列出文件目录结构. 12~/Documents$ tree [目录名]# 查看当前目录下的文件目录结构 12~/Documents$ tree -d [目录名]# 只会显示当前目录下的目录结构而不显示文件结构 cp拷贝粘贴是我们为了组织整理文件而需要完成的重要任务。使用cp将帮助你从终端拷贝粘贴文件。首先，你确定想要拷贝的那个文件，然后输入目的地位置，即可粘贴文件。cp (copy) 是复制文件或者文件夹的指令, 常用的方式是复制 “源文件” 到 “目标文件”。即： 1~$ cp 源文件 目标文件 注意: 如果你将文件拷贝到任何新文件都需要根权限的目录，那么你就需要使用sudo命令。 下面是cp命令的一些常见操作。 12~/Documents$ cp file1 file1copy# 将file1复制成file1copy 123~/Documents$ cp -i file1 file1copy# -i (interactive) 注意: 如果 file1copy 已经存在, 它将会直接覆盖已存在的 file1copy, 如果要避免直接覆盖, 我们在 cp 后面加一个选项。# 在这句问句后面打上 “Yes”, “Y”, 或者任何大小写形式的 “y” 和 “yes”, 它将进行覆盖操作. 直接回车或者打其他字母, 就会放弃复制这项操作。 12~/Documents$ cp file1 Folder1/# 将file1复制到文件夹Folder1 12~/Documents$ cp -R Folder1/ Folder2/# 复制文件夹, 需要加上 -R (recursive) 12~/Documents$ cp file* Folder1/# 复制多个文件。复制名字部分相同的多个文件到某个文件夹, * 是说"你就找文件名前面是 file 的文件, 后面是什么名字无所谓" 12~/Documents$ cp file2.txt file5.txt Folder1/# 或者你可以单独选定几个文件, cp 会默认最后一个选项是要复制去的文件夹. 比如把 file2.txt 和 file5.txt 复制去 Folder1/ mv知道了 cp, mv就好理解多了, 基本是一样的。mv是剪切(移动)的命令。mv命令可以用来移动文件或目录，也可以给文件或目录重命名。下面是mv命令的一些常见操作。 12~/Documents$ mv file1 Folder1/# 将file1移动到文件夹Folder1 12~/Documents$ mv -i file1 file1rename# 重命名文件file1为file1rename。因为移动文件到原始的地点, 但是以不同的文件名。所以这种做法不就是在重命名嘛!(加上`-i`覆盖文件前提示) 12~/Documents$ mv -f file1 Folder2/# 将文件file1强制移动到Folder2文件夹下。如果目标文件已经存在，不会询问而会直接覆盖。 最后还是想要提一句, 如果想要查看使用说明, 直接在指令后面打上 --help就能查看. mkdir仅仅会更改目录还不全面。有时候，你想要创建一个新的文件夹或子文件夹。可以使用mkdir命令来做到这一点。只要在终端中将你的文件夹名称放在mkdir命令的后面即可。mkdir (make directory) 就是创建一个文件夹的意思, 使用起来很简单。(注意：新建目录的名称不能与当前目录中已有的目录或文件同名) 以下是mkdir命令的一些常见操作。 12~/Documents$ mkdir Folder2/# 在Documents文件夹下创建一个文件夹Folder2 12~/Documents$ mkdir Folder2/folder# 在文件夹Folder2里面再创建一个子文件夹folder 12~/Documents$ mkdir -p A/B/C/D# 可以递归的创建目录，此处创建了4个目录 rmdirrmdir(remove directory) 也就是字面的意思，即移除文件夹。不过这有一个前提条件. 这些要移除的文件夹必须是空的，不然会失败。所以如果想刚刚建立的那个 Folder2 就不能被移除, 因为里面有个 folder 文件夹。那么怎么移除有文件的文件夹呢？这里需要用到我们后面讲的rm命令。 要移除个空文件夹, 比如我在新建一个 Folder3, 然后移除。 12~/Documents$ rmdir Folder3# 删除文件夹Folder3(rmdir只能移除空文件夹！！！) rmrm这个命令可以移除你的文件，甚至移除你的目录。如果文件需要根权限才能移除，可以使用-f(强制删除)。你还可以使用-r来进行递归移除，从而移除你的文件夹。 注意: 执行了 rm以后是不能进行返回操作的, 请确保别执行像这样的操作 rm /(或者在根目录下进行rm -rf *)，这会清空你的电脑。 12~/Documents$ rm file1# 删除单个文件filer1 12345~/Documents$ rm -i file2# -i 或 -I 有提示地移除文件 (为了避免误删)# -i 会每个要移除的文件都进行提示~/Documents$ rm -I file2 fil3 file4 file5# -I 超过3个文件才进行提示 12~/Documents$ rm -f file1# 强制删除，忽略不存在的文件，无需提示 123~/Documents$ rm -r Folder1/# -r 或 -R (recursively) 用来删文件夹(递归的删除目录下的内容)# 和 rmdir 不同, rm -r 可以在文件夹中有文件的情况下删除这个文件夹. 比如我的 Folder1 里有 file1 和 file2 两个文件. rm命令其他的特点和cp命令差不多，比如用带相应的前缀或者后缀(或者不带，这样会清空当前文件夹下所有文件)加’*’来一次性删除多个文件。 nanonano是 linux 的一款文字编辑工具. 我们可以拿它来做最基本的 terminal 端的文本编辑, 甚至可以写代码。下面我们用 touch 创建一个 Python 脚本。如果大家不懂Python 也没关系，你就知道我们可以拿 nano来编辑文字或者脚本就好了。 12~/Documents$ touch p1.py# 在Documents目录下创建一个python文件 然后用 nano 执行这个 p1.py 文件，如下图所示： 12~/Documents$ nano p1.py# 用nano命令执行这个文件 它就会变成一个文本编辑器, 你在里面可以写上一些脚本。然后按 “Ctrl + x” 来保存和退出。如果提示你保存, 你就按一下 “y” 键, 然后回车, 你的文件就被保存下来了。 接着如果你在 terminal 中输入相关的执行python文件的命令，你就能看到 terminal 执行了你的 python 文件。 cat作为用户，你常常需要查看来自脚本的一些文档或代码。同样，其中一个Linux基本命令是cat命令。它会为你显示文件里面的文本。 cat(catenate) 可以用来显示文件内容,创建文件,文件合并, 或者是将某个文件里的内容写入到其他文件里。cat会一次性的显示所有的内容,适合查看内容较少的文本文件。详细的操作见下面。 1234~/Documents$ cat -b p1.py ~/Documents$ cat -n p1.py # -b表示对文件的非空输出行编号# -n表示对文件的所有行输出行编号 123~/Documents$ cat p1.py &gt; p2.py~/Documents$ cat p2.py# &gt; 将文件的内容放到另一个文件里，这里我们将p1.py的内容写入到p2.py里面。 1234~/Documents$ cat p1.py p2.py &gt; p3.py~/Documents$ cat p3.py# &gt; 将多个文件的内容打包一起放入另一个文件，这里我们将p1.py的内容和p1.py的内容一同写入到p2.py里面。# 在显示的时候，两个文件的内容会分开显示 123~/Documents$ cat p2.py &gt;&gt; p3.py~/Documents$ cat p3.py# &gt;&gt; 将内容添加在一个文件末尾，这里我们将p2.py的内容添加到p3.py的末尾。 more和上面的cat命令类似,more命令可以用于分屏显示文件内容，每次只显示一页内容。适合于查看内容较多的文本文件。 123~/Documents$ more p1.py# 分屏查看p1.py文件的内容。如果要查看后续的内容，可以按住空格键(显示手册页的下一屏)或者回车键(一次滚动手册页的一行)# b:回滚一屏 f:前滚一屏 q:退出 head查看文件的前几行(默认查看前10行) 123~/Documents$ head -n 5 p2.py# head [-n number] filename# -n ：后面接数字，代表显示几行的意思 tail查看文件的后几行(默认查看倒数后10行) 123~/Documents$ tail -n 6 p3.py# tail [-n number] filename# -n ：后面接数字，代表显示几行的意思 grep你需要找到一个文件，但是又记不得它的确切位置或路径。grep可以帮助你解决这个问题。你可以使用grep命令，根据给定的关键字帮助找到文件。 grep命令是Linux系统中一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。 常用的两种正则表达式查找： ^a 行首，搜寻以 a 开头的行ke$ 行尾，搜寻以 ke 结束的行 1234567$ grep [-acinv] [--color=auto] 搜寻字符串 filename# -a ： 将 binary 文件以 text 文件的方式进行搜寻# -c ： 计算找到个数# -i ： 忽略大小写# -n ： 输出行号# -v ： 反向选择，亦即显示出 没有搜寻字符串内容 的那一行# --color=auto ：找到的关键字加颜色显示 12~/Documents$ grep hello p1.python# 在p1.py文件中搜索"hello"这个单词 范例：把含有 the 字符串的行提取出来(注意默认会有 –color=auto 选项，因此以下内容在 Linux 中有颜色显示 the 字符串) 1234567$ grep -n 'the' regular_express.txt# 以下为相应的显示结果# 8:I can't finish the test.# 12:the symbol '*' is represented as start.# 15:You are the best is mean you are the no. 1.# 16:The world Happy is the same with "glad".# 18:google is the best tools for search keyword 因为 {和 }在 shell 是有特殊意义的，因此必须要使用转义字符进行转义。 1$ grep -n 'go\&#123;2,5\&#125;g' regular_express.txt grep还有很多更强大的用法，后续会继续更新。当然，具体的用法可以通过grep --help进行查看。 echoecho会在终端中显示参数指定的文字,通常会和重定向联合使用。 重定向&gt;和&gt;&gt;: Linux 允许将命令执行结果重定向到一个文件 将本应显示在终端上的内容 输出／追加到指定文件中 其中： &gt;表示输出，会覆盖文件原有的内容。 &gt;&gt;表示追加，会将内容追加到已有文件的末尾。 1234~/Documents$ echo Hello World &gt; 1.txt~/Documents$ echo Python &gt;&gt; 1.txt# 将'Hello World'写入到1.txt中# 将'Python'追加到1.txt文件的末尾 管道 | Linux允许将一个命令的输出可以通过管道做为另一个命令的输入 可以理解现实生活中的管子，管子的一头塞东西进去，另一头取出来，这里|的左右分为两端，左端塞东西（写），右端取东西（读） 常用的管道命令有： more: 分屏显示内容 grep: 在命令执行结果的基础上查询指定的文本 1~/Documents$ ls -lha | more apt-get就不同的发行版而言，这个命令各不相同。在基于Debian的Linux发行版中，想安装、移除和升级任何软件包，我们可以使用高级包装工具(APT)软件包管理器。apt-get命令可帮助你安装需要在Linux中运行的软件。这是个功能强大的命令行工具，可以执行安装、升级、甚至移除软件这类任务。 在其他发行版(比如Fedora和Centos)中，有不同的软件包管理器。Fedora过去有yum，但现在它有dnf。 1~ sudo apt-get update 1~ sudo dnf update 12~ sudo apt-get install &lt;package name&gt;# 安装相应的软件或工具 poweroff有时候，你需要直接从终端来进行关机。这个命令就能完成这项任务，别忘了在命令的开头添加sudo，因为它需要根权限才能执行poweroff。 1~$ sudo poweroff 结束语好了，基本的Linux命令大概就这么多。它会帮助你在这个早期阶段开始使用Linux，借助这些基本的Linux命令，开始使用Linux，并且定个目标：每天学会使用1个至3个命令。后续我也会继续更新~]]></content>
      <categories>
        <category>Linux操作系统</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git常用命令整理]]></title>
    <url>%2F2018%2F04%2F24%2FGit%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[前言碎碎念自从使用Git作为版本控制工具以来，通过教程学习、手册查阅方式了解了Git的相关原理和Git的命令，能够顺利使用。但由于还不熟练，实践经验也还不够丰富，每次遇到问题都需要重新搜索，多次下来十分麻烦。另一方面，查阅手册往往是不够的，因为手册只会告诉你什么命令做什么用，不会根据不同场景告诉你应该用什么命令。 所以在这篇文章中，我将常用的Git命令根据不同的使用场景做一个整理，加深印象的同时也方便自己日后进行查阅。 四个概念这里借用阮一峰老师的文章《常用Git命令清单》中的图。 几个专有名词的译名如下： Workspace: 工作区，也就是正在编辑的文件目录 Index / Stage: 暂存区 Repository: 本地仓库，.git文件夹管理的版本库 Remote: 远程仓库，也就是github.com上面的仓库 例如，在最常用的命令串中： 12345678$ git add &lt;file&gt;#添加工作区指定文件的改动到暂存区，"&lt;file&gt;"为"."时添加全部文件$ git commit -m "XXXX"#提交暂存区的所有内容到本地仓库的当前分支$ git push -u origin master#上传本地仓库到已关联的远程仓库 建立工程在工作目录中建立与远程仓库关联的Git工程主要有两种情况：第一种是由本地上传到远程仓库；第二种是从远程仓库克隆到本地。 本地上传在这种情况下，远程仓库应该是没有工程的。在本地工程文件夹下： 123456789101112$ git init#初始化一个Git仓库，此时当前目录会增加一个.git文件夹(此文件夹默认是隐藏的)，当前文件夹受到Git的管理，并默认创建master分支$ git add &lt;file&gt;#添加指定文件到暂存区，"&lt;file&gt;"为"."时表明添加当前目录的所有文件到暂存区$ git commit -m "XXXX"#提交暂存区的所有内容到本地仓库的当前分支$ git remote add origin &lt;url&gt;#为当前项目添加远程主机。#其中origin为自定义的远程主机名，url为远程主机的地址（推荐采用ssh协议） 此时已经建立了本地仓库与远程仓库的关联，可以通过git push推送上传。第一次推送采用： 12$ git push -u origin master#将本地master分支推送到远程同名分支（若不存在则新建），同时-u指定origin为默认主机名，之后若要上传到origin可省略它。 远程克隆这种情况下，远程仓库已经有工程，只需要在本地工程文件夹下用git clone命令克隆： 1$ git clone &lt;url&gt; 此时本地仓库已经与对应远程仓库建立关联，为主机名origin的地址。 克隆其他分支git clone命令默认克隆远程项目的master分支及其历史，若还需克隆别的分支，可通过以下方式进行（以克隆dev分支为例）： 12$ git checkout -b dev origin/dev#检出origin下的dev分支到本地新建的dev分支，并建立本地分支与远程分支的追踪关系 或者： 12345678$ git checkout -b dev#新建并切换到本地分支dev$ git branch --set-upstream-to=origin/dev dev#建立origin/dev远程分支和dev本地分支的追踪关系$ git pull#拉取本地分支dev对应的远程分支的最新状态 托管到新的远程仓库在克隆需要的内容后，有时会希望托管到新的远程仓库。此时可以增加新的远程主机名: 1$ git remote add &lt;new_remote_name&gt; &lt;url&gt; 或者干脆更改原来origin的地址： 1$ git remote origin set-url &lt;url&gt; 分支管理查看分支12345678$ git branch#查看本地分支$ git branch -r#查看远程分支$ git branch -a#查看所有本地分支和远程分支 新建本地分支新建分支（不切换）： 1$ git branch &lt;new_branch&gt; 新建分支并切换到新分支： 1234$ git checkout -b &lt;new_branch&gt;#相当于：$ git branch &lt;new_branch&gt;$ git checkout &lt;new_branch&gt; 删除本地分支12345$ git branch -d &lt;branch&gt;#删除分支前检查该分支是否有未提交或者未合并的内容$ git branch -D &lt;branch&gt;#强制删除该分支 新建远程分支相当于把远程未添加的本地分支push到远程： 12$ git push origin &lt;local_branch&gt;:&lt;remote_branch&gt;#建议远程与本地分支同名，同名时可省略远程分支名 删除远程分支相当于push一个本地的空分支： 12$ git push origin :&lt;remote_branch&gt;#本地分支为空 或者用--delete： 1$ git push origin --delete &lt;remote_branch&gt; 合并分支12345$ git merge &lt;branch&gt;#快进合并（指针指向改变），合并&lt;branch&gt;分支到当前分支$ git merge --no-ff &lt;branch&gt;#合并&lt;branch&gt;到当前分支，在当前分支生成新节点，保证每个分支的独立演变史 rebase从前面的分析我们可以看出，多人在同一个分支上协作时，很容易出现冲突。即使没有冲突，后push的童鞋不得不先pull，在本地合并，然后才能push成功 这样每次合并再push以后，分支看上去会很乱。有强迫症的童鞋会问：为什么Git的提交历史不能是一条干净的直线？ 其实是可以做到的！Git有一种称为rebase的操作，有人把它翻译成“变基”rebase操作的特点：把分叉的提交历史“整理”成一条直线，看上去更直观。缺点是本地的分叉提交已经被修改过了 123$ git rebase# rebase操作可以把本地未push的分叉提交历史整理成直线；# rebase的目的是使得我们在查看历史提交的变化时更容易，因为分叉的提交需要三方对比 撤销与版本回退撤销工作区修改有时修改工作区后，发现修改错误，希望回到原来未修改时（上一次提交或暂存）的状态。可以采用git checkout命令： 1234567$ git diff#查看工作区未提交（或为暂存）的文件的具体修改$ git checkout -- &lt;file&gt;#恢复工作区指定文件到上一次提交（或暂存）状态$ git checkout .#撤销所有工作区修改 撤销暂存区修改12345$ git reset HEAD &lt;file&gt;# 将指定文件撤出暂存区$ git checkout -- &lt;file&gt;# 将文件在工作区的修改也撤销 版本回退希望将版本库回退到之前的提交时，采用git reset命令： 1234567891011$ git log#查看之前的版本提交记录$ git reflog#记录你的每一条命令，用于查看命令历史$ git reset --hard HEAD^#回退到上一个提交版本，^^代表上两个版本，以此类推。（也可以用~2等代替）或$ git reset --hard &lt;commitID&gt;#commitID可由git log查看得到 有必要整理一下git reset命令的三个参数： 123456789$ git reset --soft HEAD^#重置版本库头指针，且将这次提交之后的所有变更移动到暂存区$ git reset --mixed HEAD^#默认参数，等同于 git reset HEAD^#重置版本库头指针和暂存区，即这次提交之后的所有更改都留在工作区$ git reset --hard HEAD^#重置版本库头指针、暂存区和工作区，即这次提交之后的所有更改都不在存在于当前状态 在没有将之后的提交推送到远程仓库的情况下，git reset --hard是个很危险的操作。若是已经推送到远程仓库，使用git pull可以重新获得之后的版本提交。若是在没有远程备份时使用--hard进行版本回退，又想恢复到之后的版本，在一定时间内（一般为30天）可以通过git reflog查看操作id，再使用git reset --hard &lt;ID&gt;恢复。 删除文件在Git中，删除也是一个修改操作，来让我们实战一下 12345678$ rm &lt;file&gt;# 在工作区删除该文件$ git rm &lt;file&gt;# git rm用于删除一个文件。如果一个文件已经被提交到版本库，那么你永远不用担心误删，但是要小心，你只能恢复文件到最新版本，你会丢失最近一次提交后你修改的内容$ git commit -m "rm xxx"# 向版本库提交信息 stash储藏有时手头的工作进行到一半，需要切换分支做一些其他事情，可以采用git stash命令将当前的工作区储藏起来。 1234567891011121314$ git stash#储藏当前工作区$ git stash list#查看当前的stash储藏栈$ git stash apply#应用栈顶的储藏内容，恢复工作区到之前的储藏状态$ git stash apply stash@&#123;2&#125;#应用指定储藏内容$ git stash pop#与apply类似，但从栈中删除该储藏内容 多人协作推送分支推送分支，就是把该分支上的所有本地提交推送到远程库。推送时，要指定本地分支，这样，Git就会把该分支推送到远程库对应的远程分支上： 12345#默认推送的为master主分支$ git push origin master#如果要推送其他分支，将该分支对应的名字来替换master即可$ git push origin dev 抓取分支多人协作时，大家都会往master和dev分支上推送各自的修改。当你的小伙伴从远程库clone时，默认情况下，你的小伙伴只能看到本地的master分支。不信可以用git branch命令看看： 12$ git branch* master 现在，你的小伙伴要在dev分支上开发，就必须创建远程origin的dev分支到本地，于是他用这个命令创建本地dev分支： 12#注意：本地分支名称最好与远程分支保持一致。$ git checkout -b dev origin/dev 现在，他就可以在dev上继续修改，然后，时不时地把dev分支push到远程。 你的小伙伴已经向origin/dev分支推送了他的提交，而碰巧你对同样的文件也做了修改，并且试图推送。 这个时候会推送失败,因为你的小伙伴的最新提交和你试图推送的提交有冲突，解决办法也很简单: 先用git pull把最新的提交从origin/dev抓下来，然后，在本地合并，解决冲突，再推送。 1$ git pull 如果git pull提示“no tracking information”，则说明本地分支和远程分支的链接关系没有创建，用命令 1$ git branch --set-upstream branch-name origin/branch-name 最后再将你改好的解决冲突之后的分支push到远程仓库。这就是多人协作的工作模式，一旦熟悉了，就非常简单。 其他状态查看1$ git status 任何情况下都可以使用git status命令查看当前的版本控制状态（包括工作区、暂存区、仓库区），并给出当前状态下可能会用到的命令提示。经常使用该命令是好习惯。 查看历史信息12$ git log [--pretty=oneline]# git log命令显示从最近到最远的提交日志 查看远程库信息1$ git remote -v 配置git用户123$ git config user.name "your name"$ git config user.email "email@example.com"# 配置当前目录的git用户，加上--config参数时配置这台机器的所有git仓库 协议更改有时版本克隆是采用的是https协议，以至于每一次提交都需要输入用户名密码，很麻烦。而使用ssh协议就会方便很多，需要将当前的仓库协议进行更换。事实上，重置远程仓库名为ssh协议地址就可以了。 1$ git remote origin set-url git@example.com....]]></content>
      <categories>
        <category>版本控制之Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>Bash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何更改Github上面托管的项目的默认显示语言]]></title>
    <url>%2F2018%2F04%2F22%2F%E5%A6%82%E4%BD%95%E6%9B%B4%E6%94%B9Github%E4%B8%8A%E9%9D%A2%E6%89%98%E7%AE%A1%E7%9A%84%E9%A1%B9%E7%9B%AE%E7%9A%84%E9%BB%98%E8%AE%A4%E6%98%BE%E7%A4%BA%E8%AF%AD%E8%A8%80%2F</url>
    <content type="text"><![CDATA[最近将自己一门课的课程大作业，也算是一个小项目demo，放到了GitHub上面，由于项目里面包含许多下载的html文件，导致html代码远远的超过自己写的python代码，于是GitHub默认也就显示是html。 这种对于我这种有强迫症的人来说，简直就是不能忍受的。于是我就各种上网找解决方案。终于，在不懈的努力之后，让我给找到了，在这里分享给大家。 导致这种情况的主要原因是GitHub是使用Linguist来检测你的项目所使用的语言，Linguist具体的作用我个人猜测应该就是：统计你这个项目里面哪一种语言的代码量最多，就把这种语言作为当前这个项目的主语言,也就是默认显示语言。这种做法显然是很不合理的，你比如像python这种支持函数式编程而且崇尚语法简洁优雅的解释型脚本语言，它的代码量远远比不过其他的编译型语言(比如C/C++/Java)。 那么到底如何来解决这个问题呢？解决办法如下： 使用.gitattributes配置文件 具体做法是这样的：在你的GitHub代码仓库的根目录界面新建一个.gitattributes配置文件，如下图所示： 然后打开这个文件：把默认的显示语言（也就是统计的代码量最多的语言）全部改成你这个项目本来的语言。例如： 123*.js linguist-language=Python *.css linguist-language=Python *.html linguist-language=Python 这几行代码的意思就是说：将以.js , .css , .html 为扩展名的文件都按照Python语言来统计。就是这么简单。这么直接！！！ 好了，改完这些之后保存，再回到GitHub主界面就会看到默认的展示语言已经变成python了。主要的步骤就是这样，希望能够帮助到大家。]]></content>
      <categories>
        <category>版本控制之Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于惠普暗影精灵2 pro用一段时间之后，插上电源充不了电的问题]]></title>
    <url>%2F2018%2F04%2F22%2F%E5%85%B3%E4%BA%8E%E6%83%A0%E6%99%AE%E6%9A%97%E5%BD%B1%E7%B2%BE%E7%81%B52-pro%E7%94%A8%E4%B8%80%E6%AE%B5%E6%97%B6%E9%97%B4%E4%B9%8B%E5%90%8E%EF%BC%8C%E6%8F%92%E4%B8%8A%E7%94%B5%E6%BA%90%E5%85%85%E4%B8%8D%E4%BA%86%E7%94%B5%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[去年5月份换的新电脑，用到现在大半年了。近几天来突然发现电脑插上电源充不了电，关机重启啥的都没有效果，于是尝试着上网找各种解决方案，终于，在试了N种解决方案之后，终于让我成功地找到了解决问题的办法。现在将我的解决方案分享出来，希望对大家有所帮助。 具体的操作步骤（仅针对惠普电脑，其他品牌的电脑不敢保证，不过个人觉得应该差不多）如下： 先将电脑关机； 在关机状态下按住键盘上的windows和V键； 再按住上面的两个键的同时，不松开，再按住电源键，三个键同时按住不放2-3秒钟； 松开全部的按键； 按电源键开机，看是否会进入一个Bios设置界面，即是否会提示502，如果提示的话则表示操作成功； 再重新启动电脑一次，问题即可解决。]]></content>
      <categories>
        <category>电脑技巧</category>
      </categories>
      <tags>
        <tag>Computer Tricks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F04%2F21%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
